{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "# tensorboard in util\n",
    "import sys\n",
    "sys.path.append(\"/home/yasuhiko.iwase/jupyter/util\")\n",
    "import tensorboard as tb \n",
    "\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "# モデル保存用\n",
    "MODEL_DIR = os.path.join('./model')\n",
    "if os.path.exists(MODEL_DIR) is False:\n",
    "    os.mkdir(MODEL_DIR)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(x, y, n_batch, is_training,\n",
    "              input_digits=None,\n",
    "              output_digits=None,\n",
    "              n_hidden=None,\n",
    "              n_out=None):\n",
    "    def weight_variable(shape):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "        return tf.Variable(initial, name='w')\n",
    "\n",
    "    def bias_variable(shape):\n",
    "        initial = tf.zeros(shape, dtype=tf.float32)\n",
    "        return tf.Variable(initial, name='b')\n",
    "\n",
    "    # Encode\n",
    "    encoder = rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "    encoder = rnn.AttentionCellWrapper(encoder,\n",
    "                                       input_digits,\n",
    "                                       state_is_tuple=True)\n",
    "    state = encoder.zero_state(n_batch, tf.float32)\n",
    "    encoder_outputs = []\n",
    "    encoder_states = []\n",
    "\n",
    "    with tf.variable_scope('Encoder'):\n",
    "        for t in range(input_digits):\n",
    "            if t > 0:\n",
    "                tf.get_variable_scope().reuse_variables()\n",
    "            (output, state) = encoder(x[:, t, :], state)\n",
    "            encoder_outputs.append(output)\n",
    "            encoder_states.append(state)\n",
    "\n",
    "    # Decode\n",
    "    decoder = rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "    decoder = rnn.AttentionCellWrapper(decoder,\n",
    "                                       input_digits,\n",
    "                                       state_is_tuple=True)\n",
    "    state = encoder_states[-1]\n",
    "    decoder_outputs = [encoder_outputs[-1]]\n",
    "\n",
    "    # 出力層の重みとバイアスを事前に定義\n",
    "    V = weight_variable([n_hidden, n_out])\n",
    "    c = bias_variable([n_out])\n",
    "    outputs = []\n",
    "\n",
    "    with tf.variable_scope('Decoder'):\n",
    "        for t in range(1, output_digits):\n",
    "            if t > 1:\n",
    "                tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "            if is_training is True:\n",
    "                (output, state) = decoder(y[:, t-1, :], state)\n",
    "            else:\n",
    "                # 直前の出力を求める\n",
    "                linear = tf.matmul(decoder_outputs[-1], V) + c\n",
    "                out = tf.nn.softmax(linear)\n",
    "                outputs.append(out)\n",
    "                out = tf.one_hot(tf.argmax(out, -1), depth=output_digits)\n",
    "\n",
    "                (output, state) = decoder(out, state)\n",
    "\n",
    "            decoder_outputs.append(output)\n",
    "\n",
    "    if is_training is True:\n",
    "        output = tf.reshape(tf.concat(decoder_outputs, axis=1),\n",
    "                            [-1, output_digits, n_hidden])\n",
    "\n",
    "        linear = tf.einsum('ijk,kl->ijl', output, V) + c\n",
    "        return tf.nn.softmax(linear)\n",
    "    else:\n",
    "        # 最後の出力を求める\n",
    "        linear = tf.matmul(decoder_outputs[-1], V) + c\n",
    "        out = tf.nn.softmax(linear)\n",
    "        outputs.append(out)\n",
    "\n",
    "        output = tf.reshape(tf.concat(outputs, axis=1),\n",
    "                            [-1, output_digits, n_out])\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def loss(y, t):\n",
    "    cross_entropy = \\\n",
    "        tf.reduce_mean(-tf.reduce_sum(\n",
    "                       t * tf.log(tf.clip_by_value(y, 1e-10, 1.0)),\n",
    "                       reduction_indices=[1]))\n",
    "    return cross_entropy\n",
    "\n",
    "\n",
    "def training(loss):\n",
    "    optimizer = \\\n",
    "        tf.train.AdamOptimizer(learning_rate=0.001, beta1=0.9, beta2=0.999)\n",
    "    train_step = optimizer.minimize(loss)\n",
    "    return train_step\n",
    "\n",
    "\n",
    "def accuracy(y, t):\n",
    "    correct_prediction = tf.equal(tf.argmax(y, -1), tf.argmax(t, -1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def n(digits=3):\n",
    "    number = ''\n",
    "    for i in range(np.random.randint(1, digits + 1)):\n",
    "        number += np.random.choice(list('0123456789'))\n",
    "    return int(number)\n",
    "\n",
    "def padding(chars, maxlen):\n",
    "    return chars + ' ' * (maxlen - len(chars))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "データの生成\n",
    "'''\n",
    "N = 20000\n",
    "N_train = int(N * 0.9)\n",
    "N_validation = N - N_train\n",
    "\n",
    "digits = 4  # 最大の桁数\n",
    "input_digits = digits * 2 + 1  # 例： 1234+5678\n",
    "output_digits = digits + 1  # 5000+5000 = 10000 以上で５桁になる\n",
    "\n",
    "added = set()\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "chars = '0123456789+ '\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "while len(questions) < N:\n",
    "    a, b = n(digits), n(digits)  # 適当な数を２つ生成\n",
    "\n",
    "    pair = tuple(sorted((a, b)))\n",
    "    if pair in added:\n",
    "        continue\n",
    "\n",
    "    question = '{}+{}'.format(a, b)\n",
    "    question = padding(question, input_digits)  # 足りない桁を穴埋め\n",
    "    answer = str(a + b)\n",
    "    answer = padding(answer, output_digits)  # 足りない桁を穴埋め\n",
    "\n",
    "    added.add(pair)\n",
    "    questions.append(question)\n",
    "    answers.append(answer)\n",
    "\n",
    "X = np.zeros((len(questions), input_digits, len(chars)), dtype=np.integer)\n",
    "Y = np.zeros((len(questions), digits + 1, len(chars)), dtype=np.integer)\n",
    "\n",
    "for i in range(N):\n",
    "    for t, char in enumerate(questions[i]):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    for t, char in enumerate(answers[i]):\n",
    "        Y[i, t, char_indices[char]] = 1\n",
    "\n",
    "X_train, X_validation, Y_train, Y_validation = \\\n",
    "    train_test_split(X, Y, train_size=N_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "モデル設定\n",
    "'''\n",
    "n_in = len(chars)\n",
    "n_hidden = 128\n",
    "n_out = len(chars)\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, input_digits, n_in])\n",
    "t = tf.placeholder(tf.float32, shape=[None, output_digits, n_out])\n",
    "n_batch = tf.placeholder(tf.int32, shape=[])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "y = inference(x, t, n_batch, is_training,\n",
    "              input_digits=input_digits,\n",
    "              output_digits=output_digits,\n",
    "              n_hidden=n_hidden, n_out=n_out)\n",
    "loss = loss(y, t)\n",
    "train_step = training(loss)\n",
    "\n",
    "acc = accuracy(y, t)\n",
    "\n",
    "history = {\n",
    "    'val_loss': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# モデルの初期化（初回のみ）\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "保存済データの利用\n",
    "'''\n",
    "\n",
    "MODEL_DIR = os.path.join('./model')\n",
    "saver = tf.train.Saver()  # モデル保存用\n",
    "\n",
    "saver.restore(sess, MODEL_DIR + '/02_attention_tensorflow_model.ckpt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017/07/27 16:52:55\n",
      "Epoch: 1\n",
      "==========\n",
      "validation loss: 0.0407438  validation acc:  0.9707\n",
      "Q:   17+8986   A:   9903  T/F: FALSE\n",
      "Q:   50+867    A:   917   T/F: TRUE!!!!!\n",
      "Q:   820+27    A:   847   T/F: TRUE!!!!!\n",
      "Q:   27+72     A:   99    T/F: TRUE!!!!!\n",
      "Q:   6265+12   A:   6277  T/F: TRUE!!!!!\n",
      "Q:   812+28    A:   840   T/F: TRUE!!!!!\n",
      "Q:   51+89     A:   140   T/F: TRUE!!!!!\n",
      "Q:   9846+1063 A:   10909 T/F: TRUE!!!!!\n",
      "Q:   3+8011    A:   8014  T/F: TRUE!!!!!\n",
      "Q:   233+69    A:   302   T/F: TRUE!!!!!\n",
      "----------\n",
      "2017/07/27 16:54:11\n",
      "Epoch: 2\n",
      "==========\n",
      "validation loss: 0.0404406  validation acc:  0.9719\n",
      "Q:   2688+94   A:   2782  T/F: TRUE!!!!!\n",
      "Q:   25+1427   A:   1452  T/F: TRUE!!!!!\n",
      "Q:   545+184   A:   729   T/F: TRUE!!!!!\n",
      "Q:   7153+1    A:   7154  T/F: TRUE!!!!!\n",
      "Q:   666+9238  A:   9904  T/F: TRUE!!!!!\n",
      "Q:   131+6424  A:   6555  T/F: TRUE!!!!!\n",
      "Q:   0+5371    A:   5371  T/F: TRUE!!!!!\n",
      "Q:   1577+3365 A:   4942  T/F: TRUE!!!!!\n",
      "Q:   135+1     A:   136   T/F: TRUE!!!!!\n",
      "Q:   682+398   A:   1080  T/F: TRUE!!!!!\n",
      "----------\n",
      "2017/07/27 16:55:28\n",
      "Epoch: 3\n",
      "==========\n",
      "validation loss: 0.0419216  validation acc:  0.971\n",
      "Q:   8342+135  A:   8477  T/F: TRUE!!!!!\n",
      "Q:   97+98     A:   196   T/F: FALSE\n",
      "Q:   0+2843    A:   2843  T/F: TRUE!!!!!\n",
      "Q:   3923+162  A:   4085  T/F: TRUE!!!!!\n",
      "Q:   4+2266    A:   2270  T/F: TRUE!!!!!\n",
      "Q:   4017+3    A:   4020  T/F: TRUE!!!!!\n",
      "Q:   29+51     A:   80    T/F: TRUE!!!!!\n",
      "Q:   62+68     A:   130   T/F: TRUE!!!!!\n",
      "Q:   8661+821  A:   9482  T/F: TRUE!!!!!\n",
      "Q:   30+398    A:   428   T/F: TRUE!!!!!\n",
      "----------\n",
      "2017/07/27 16:56:46\n",
      "Epoch: 4\n",
      "==========\n",
      "validation loss: 0.0447528  validation acc:  0.971\n",
      "Q:   3108+62   A:   3160  T/F: FALSE\n",
      "Q:   227+2     A:   229   T/F: TRUE!!!!!\n",
      "Q:   9480+830  A:   10311 T/F: FALSE\n",
      "Q:   341+914   A:   1255  T/F: TRUE!!!!!\n",
      "Q:   2939+8335 A:   11283 T/F: FALSE\n",
      "Q:   8286+3    A:   8289  T/F: TRUE!!!!!\n",
      "Q:   48+5747   A:   5895  T/F: FALSE\n",
      "Q:   3753+4    A:   3757  T/F: TRUE!!!!!\n",
      "Q:   472+5     A:   477   T/F: TRUE!!!!!\n",
      "Q:   0+256     A:   256   T/F: TRUE!!!!!\n",
      "----------\n",
      "2017/07/27 17:03:10\n",
      "Epoch: 9\n",
      "==========\n",
      "validation loss: 0.0457762  validation acc:  0.9704\n",
      "Q:   415+5297  A:   5712  T/F: TRUE!!!!!\n",
      "Q:   93+70     A:   163   T/F: TRUE!!!!!\n",
      "Q:   4740+8669 A:   13419 T/F: FALSE\n",
      "Q:   559+6     A:   565   T/F: TRUE!!!!!\n",
      "Q:   5682+8    A:   5690  T/F: TRUE!!!!!\n",
      "Q:   852+2867  A:   3719  T/F: TRUE!!!!!\n",
      "Q:   812+50    A:   862   T/F: TRUE!!!!!\n",
      "Q:   3+8011    A:   8014  T/F: TRUE!!!!!\n",
      "Q:   70+2959   A:   3029  T/F: TRUE!!!!!\n",
      "Q:   3385+2106 A:   5491  T/F: TRUE!!!!!\n",
      "----------\n",
      "2017/07/27 17:04:26\n",
      "Epoch: 10\n",
      "==========\n",
      "validation loss: 0.0456396  validation acc:  0.971\n",
      "Q:   242+46    A:   288   T/F: TRUE!!!!!\n",
      "Q:   28+2413   A:   2441  T/F: TRUE!!!!!\n",
      "Q:   592+14    A:   606   T/F: TRUE!!!!!\n",
      "Q:   2+485     A:   487   T/F: TRUE!!!!!\n",
      "Q:   1979+8284 A:   10254 T/F: FALSE\n",
      "Q:   8715+3    A:   8718  T/F: TRUE!!!!!\n",
      "Q:   0+256     A:   256   T/F: TRUE!!!!!\n",
      "Q:   7351+8    A:   7359  T/F: TRUE!!!!!\n",
      "Q:   8999+2    A:   8001  T/F: FALSE\n",
      "Q:   4402+6    A:   4408  T/F: TRUE!!!!!\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "モデル学習\n",
    "'''\n",
    "epochs = 10\n",
    "batch_size = 200\n",
    "\n",
    "n_batches = N_train // batch_size # 切り捨て除算\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(datetime.datetime.now().strftime(\"%Y/%m/%d %H:%M:%S\"))\n",
    "    print('Epoch:', epoch+1)\n",
    "    print('=' * 10)\n",
    "\n",
    "    X_, Y_ = shuffle(X_train, Y_train)\n",
    "\n",
    "    for i in range(n_batches):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "\n",
    "        sess.run(train_step, feed_dict={\n",
    "            x: X_[start:end],\n",
    "            t: Y_[start:end],\n",
    "            n_batch: batch_size,\n",
    "            is_training: True\n",
    "        })\n",
    "\n",
    "    # 検証データを用いた評価\n",
    "    val_loss = loss.eval(session=sess, feed_dict={\n",
    "        x: X_validation,\n",
    "        t: Y_validation,\n",
    "        n_batch: N_validation,\n",
    "        is_training: False\n",
    "    })\n",
    "    val_acc = acc.eval(session=sess, feed_dict={\n",
    "        x: X_validation,\n",
    "        t: Y_validation,\n",
    "        n_batch: N_validation,\n",
    "        is_training: False\n",
    "    })\n",
    "\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "\n",
    "    print('validation loss:', val_loss,' validation acc: ', val_acc)\n",
    "\n",
    "\n",
    "    # 検証データからランダムに問題を選んで答え合わせ\n",
    "    for i in range(10):\n",
    "        index = np.random.randint(0, N_validation)\n",
    "        question = X_validation[np.array([index])]\n",
    "        answer = Y_validation[np.array([index])]\n",
    "        prediction = y.eval(session=sess, feed_dict={\n",
    "            x: question,\n",
    "            # t: answer,\n",
    "            n_batch: 1,\n",
    "            is_training: False\n",
    "        })\n",
    "        question = question.argmax(axis=-1)\n",
    "        answer = answer.argmax(axis=-1)\n",
    "        prediction = np.argmax(prediction, -1)\n",
    "\n",
    "        q = ''.join(indices_char[i] for i in question[0])\n",
    "        a = ''.join(indices_char[i] for i in answer[0])\n",
    "        p = ''.join(indices_char[i] for i in prediction[0])\n",
    "\n",
    "        print('Q:  ', q, 'A:  ', p, 'T/F:', end=' ')\n",
    "        if a == p:\n",
    "            print('TRUE!!!!!')\n",
    "        else:\n",
    "            print('FALSE')\n",
    "        \n",
    "    print('-' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: ./model/02_attention_tensorflow_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "モデルファイル保存\n",
    "'''\n",
    "saver = tf.train.Saver()  # モデル保存用\n",
    "model_path = saver.save(sess, MODEL_DIR + '/02_attention_tensorflow_model.ckpt')\n",
    "print('Model saved to:', model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:   6643+197  A:   6830  T/F: FALSE\n",
      "Q:   586+395   A:   980   T/F: FALSE\n",
      "Q:   27+182    A:   219   T/F: FALSE\n",
      "Q:   14+8955   A:   8979  T/F: FALSE\n",
      "Q:   3098+3027 A:   6215  T/F: FALSE\n",
      "Q:   169+9519  A:   9698  T/F: FALSE\n",
      "Q:   27+182    A:   219   T/F: FALSE\n",
      "Q:   5223+106  A:   5339  T/F: FALSE\n",
      "success_count:  92 / 100\n",
      "success_rate:  0.92\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "予測精度の評価\n",
    "'''\n",
    "\n",
    "questions_count = 100\n",
    "success_count = 0\n",
    "\n",
    "# 検証データからランダムに問題を選んで答え合わせ\n",
    "for i in range(questions_count):\n",
    "    index = np.random.randint(0, N_validation)\n",
    "    question = X_validation[np.array([index])]\n",
    "    answer = Y_validation[np.array([index])]\n",
    "    prediction = y.eval(session=sess, feed_dict={\n",
    "        x: question,\n",
    "        # t: answer,\n",
    "        n_batch: 1,\n",
    "        is_training: False\n",
    "    })\n",
    "    question = question.argmax(axis=-1)\n",
    "    answer = answer.argmax(axis=-1)\n",
    "    prediction = np.argmax(prediction, -1)\n",
    "\n",
    "    q = ''.join(indices_char[i] for i in question[0])\n",
    "    a = ''.join(indices_char[i] for i in answer[0])\n",
    "    p = ''.join(indices_char[i] for i in prediction[0])\n",
    "\n",
    "    if a == p:\n",
    "        success_count += 1 \n",
    "    else:\n",
    "        print('Q:  ', q, 'A:  ', p, 'T/F:', end=' ')\n",
    "        print('FALSE')\n",
    "        \n",
    "success_rate = success_count / questions_count\n",
    "print(\"success_count: \",success_count, \"/\",questions_count)\n",
    "print(\"success_rate: \",success_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.5488135039273248&quot;).pbtxt = 'node {\\n  name: &quot;Placeholder&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Placeholder_1&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Placeholder_2&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Placeholder_3&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_BOOL\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;stack/1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 128\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;stack&quot;\\n  op: &quot;Pack&quot;\\n  input: &quot;Placeholder_2&quot;\\n  input: &quot;stack/1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;axis&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;stack&quot;\\n  input: &quot;zeros/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;stack_1/1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 128\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;stack_1&quot;\\n  op: &quot;Pack&quot;\\n  input: &quot;Placeholder_2&quot;\\n  input: &quot;stack_1/1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;axis&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros_1/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros_1&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;stack_1&quot;\\n  input: &quot;zeros_1/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;stack_2/1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 128\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;stack_2&quot;\\n  op: &quot;Pack&quot;\\n  input: &quot;Placeholder_2&quot;\\n  input: &quot;stack_2/1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;axis&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros_2/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros_2&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;stack_2&quot;\\n  input: &quot;zeros_2/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;stack_3/1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1152\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;stack_3&quot;\\n  op: &quot;Pack&quot;\\n  input: &quot;Placeholder_2&quot;\\n  input: &quot;stack_3/1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;axis&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros_3/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros_3&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;stack_3&quot;\\n  input: &quot;zeros_3/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/strided_slice/stack&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/strided_slice/stack_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/strided_slice/stack_2&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/strided_slice&quot;\\n  op: &quot;StridedSlice&quot;\\n  input: &quot;Placeholder&quot;\\n  input: &quot;Encoder/strided_slice/stack&quot;\\n  input: &quot;Encoder/strided_slice/stack_1&quot;\\n  input: &quot;Encoder/strided_slice/stack_2&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;begin_mask&quot;\\n    value {\\n      i: 5\\n    }\\n  }\\n  attr {\\n    key: &quot;ellipsis_mask&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;end_mask&quot;\\n    value {\\n      i: 5\\n    }\\n  }\\n  attr {\\n    key: &quot;new_axis_mask&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;shrink_axis_mask&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\t\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;zeros_3&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/weights/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\214\\\\000\\\\000\\\\000\\\\014\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/weights/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.1986798495054245\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/weights/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.1986798495054245\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/weights/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/weights/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 1234\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 29\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/weights/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/weights/Initializer/random_uniform/max&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/weights/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/weights/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/weights/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/weights/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/weights/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/weights/Initializer/random_uniform/mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/weights/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/weights&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 140\\n        }\\n        dim {\\n          size: 12\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/weights/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/weights&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/weights/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/weights/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/weights&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention_cell_wrapper/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention_cell_wrapper/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Encoder/strided_slice&quot;\\n  input: &quot;zeros_2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention_cell_wrapper/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention_cell_wrapper/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention_cell_wrapper/concat&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/biases/Initializer/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 12\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/biases&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 12\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/biases/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/biases&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/biases/Initializer/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/biases/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/biases&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/biases&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention_cell_wrapper/MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\214\\\\000\\\\000\\\\000\\\\000\\\\002\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.09592942148447037\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.09592942148447037\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 1234\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 47\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/Initializer/random_uniform/max&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/Initializer/random_uniform/mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 140\\n        }\\n        dim {\\n          size: 512\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/BiasAdd&quot;\\n  input: &quot;zeros_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/biases/Initializer/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/basic_lstm_cell/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 512\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/biases&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/basic_lstm_cell/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 512\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/biases/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/biases&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/biases/Initializer/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/basic_lstm_cell/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/biases/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/biases&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/basic_lstm_cell/biases&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/split/split_dim&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/split&quot;\\n  op: &quot;Split&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/split/split_dim&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;num_split&quot;\\n    value {\\n      i: 4\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/add/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/split:2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/add/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/Sigmoid&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;zeros&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/Sigmoid&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/Sigmoid_1&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/split&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/Tanh&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/split:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/Sigmoid_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/add_1&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/mul_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/Tanh_1&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/Sigmoid_2&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/split:3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/mul_2&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/Tanh_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/Sigmoid_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/add_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/mul_2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/attn_w/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/attn_w&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/attn_w/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/attn_w&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.1530931144952774\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/attn_w/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/attn_w&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.1530931144952774\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/attn_w/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_w/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/attn_w&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 1234\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 80\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/attn_w/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_w/Initializer/random_uniform/max&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_w/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/attn_w&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/attn_w/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_w/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_w/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/attn_w&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/attn_w/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_w/Initializer/random_uniform/mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_w/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/attn_w&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/attn_w&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/attn_w&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 1\\n        }\\n        dim {\\n          size: 1\\n        }\\n        dim {\\n          size: 128\\n        }\\n        dim {\\n          size: 128\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/attn_w/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_w&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_w/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/attn_w&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/attn_w/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_w&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/attn_w&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/attn_v/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/attn_v&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 128\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/attn_v/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/attn_v&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.1530931144952774\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/attn_v/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/attn_v&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.1530931144952774\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/attn_v/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_v/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/attn_v&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 1234\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 90\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/attn_v/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_v/Initializer/random_uniform/max&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_v/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/attn_v&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/attn_v/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_v/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_v/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/attn_v&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/attn_v/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_v/Initializer/random_uniform/mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_v/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/attn_v&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/attn_v&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/attn_v&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 128\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/attn_v/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_v&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_v/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/attn_v&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/attn_v/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_v&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/attn_v&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\t\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/Conv2D&quot;\\n  op: &quot;Conv2D&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_w/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;SAME&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/weights/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\001\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/weights/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.125\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/weights/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.125\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/weights/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/weights/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 1234\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 103\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/weights/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/weights/Initializer/random_uniform/max&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/weights/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/weights/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/weights/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/weights/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/weights/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/weights/Initializer/random_uniform/mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/weights/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/weights&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 256\\n        }\\n        dim {\\n          size: 128\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/weights/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/weights&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/weights/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/weights/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/weights&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/attention/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/concat&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/biases/Initializer/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 128\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/biases&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 128\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/biases/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/biases&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/biases/Initializer/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/biases/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/biases&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/biases&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attention/MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/Reshape_1/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/Reshape_1/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/Conv2D&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/Reshape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/Tanh&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_v/read&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/Sum/reduction_indices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\002\\\\000\\\\000\\\\000\\\\003\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/Sum/reduction_indices&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/Softmax&quot;\\n  op: &quot;Softmax&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/Sum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/Reshape_2/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\t\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/Reshape_2&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/Softmax&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/Reshape_2/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/Reshape_2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/Sum_1/reduction_indices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/mul_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/Sum_1/reduction_indices&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/Reshape_3/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/Reshape_3&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/Sum_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/Reshape_3/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/Slice/begin&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/Slice/size&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/Slice/begin&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/Slice/size&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\001\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.125\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.125\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 1234\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 137\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/Initializer/random_uniform/max&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/Initializer/random_uniform/mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 256\\n        }\\n        dim {\\n          size: 128\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/mul_2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/Reshape_3&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attn_output_projection/biases/Initializer/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attn_output_projection/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 128\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attn_output_projection/biases&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attn_output_projection/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 128\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attn_output_projection/biases/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/biases&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/biases/Initializer/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attn_output_projection/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attn_output_projection/biases/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/biases&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attn_output_projection/biases&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attn_output_projection/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/ExpandDims/dim&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/ExpandDims&quot;\\n  op: &quot;ExpandDims&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/ExpandDims/dim&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tdim&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/concat_1/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/concat_1&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/Slice&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/ExpandDims&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/concat_1/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/Reshape_1/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\200\\\\004\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/concat_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/Reshape_1/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/strided_slice_1/stack&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/strided_slice_1/stack_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\002\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/strided_slice_1/stack_2&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/strided_slice_1&quot;\\n  op: &quot;StridedSlice&quot;\\n  input: &quot;Placeholder&quot;\\n  input: &quot;Encoder/strided_slice_1/stack&quot;\\n  input: &quot;Encoder/strided_slice_1/stack_1&quot;\\n  input: &quot;Encoder/strided_slice_1/stack_2&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;begin_mask&quot;\\n    value {\\n      i: 5\\n    }\\n  }\\n  attr {\\n    key: &quot;ellipsis_mask&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;end_mask&quot;\\n    value {\\n      i: 5\\n    }\\n  }\\n  attr {\\n    key: &quot;new_axis_mask&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;shrink_axis_mask&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\t\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/Reshape_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/attention_cell_wrapper/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/attention_cell_wrapper/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Encoder/strided_slice_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/Reshape_3&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention_cell_wrapper/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/attention_cell_wrapper/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention_cell_wrapper/concat&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention_cell_wrapper/MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/mul_2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/split/split_dim&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/split&quot;\\n  op: &quot;Split&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/split/split_dim&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;num_split&quot;\\n    value {\\n      i: 4\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/add/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/split:2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/add/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/Sigmoid&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/add_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/Sigmoid&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/Sigmoid_1&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/split&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/Tanh&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/split:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/Sigmoid_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_1&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/Tanh_1&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/Sigmoid_2&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/split:3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/Tanh_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/Sigmoid_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/attention/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\t\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/attention/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/attention/Conv2D&quot;\\n  op: &quot;Conv2D&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_w/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;SAME&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/attention/attention/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/concat&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/attention/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/attention/MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/attention/Reshape_1/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/attention/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/Reshape_1/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/attention/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/Conv2D&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/Reshape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/attention/Tanh&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/attention/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_v/read&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/attention/Sum/reduction_indices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\002\\\\000\\\\000\\\\000\\\\003\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/attention/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/Sum/reduction_indices&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/attention/Softmax&quot;\\n  op: &quot;Softmax&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/Sum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/attention/Reshape_2/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\t\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/attention/Reshape_2&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/Softmax&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/Reshape_2/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/attention/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/Reshape_2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/attention/Sum_1/reduction_indices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/attention/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/mul_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/Sum_1/reduction_indices&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/attention/Reshape_3/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/attention/Reshape_3&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/Sum_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/Reshape_3/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/attention/Slice/begin&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/attention/Slice/size&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/attention/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/Slice/begin&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/Slice/size&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/Reshape_3&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/attn_output_projection/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/ExpandDims/dim&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/ExpandDims&quot;\\n  op: &quot;ExpandDims&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attn_output_projection/BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/ExpandDims/dim&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tdim&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/concat_1/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/concat_1&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/Slice&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/ExpandDims&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/concat_1/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/Reshape_1/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\200\\\\004\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_1/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/concat_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/Reshape_1/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/strided_slice_2/stack&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\002\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/strided_slice_2/stack_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\003\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/strided_slice_2/stack_2&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/strided_slice_2&quot;\\n  op: &quot;StridedSlice&quot;\\n  input: &quot;Placeholder&quot;\\n  input: &quot;Encoder/strided_slice_2/stack&quot;\\n  input: &quot;Encoder/strided_slice_2/stack_1&quot;\\n  input: &quot;Encoder/strided_slice_2/stack_2&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;begin_mask&quot;\\n    value {\\n      i: 5\\n    }\\n  }\\n  attr {\\n    key: &quot;ellipsis_mask&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;end_mask&quot;\\n    value {\\n      i: 5\\n    }\\n  }\\n  attr {\\n    key: &quot;new_axis_mask&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;shrink_axis_mask&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\t\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/Reshape_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/attention_cell_wrapper/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/attention_cell_wrapper/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Encoder/strided_slice_2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/Reshape_3&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention_cell_wrapper/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/attention_cell_wrapper/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention_cell_wrapper/concat&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention_cell_wrapper/MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/split/split_dim&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/split&quot;\\n  op: &quot;Split&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/split/split_dim&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;num_split&quot;\\n    value {\\n      i: 4\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/add/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/split:2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/add/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/Sigmoid&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/Sigmoid&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/Sigmoid_1&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/split&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/Tanh&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/split:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/Sigmoid_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_1&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/Tanh_1&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/Sigmoid_2&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/split:3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/Tanh_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/Sigmoid_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/attention/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\t\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/attention/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/attention/Conv2D&quot;\\n  op: &quot;Conv2D&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_w/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;SAME&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/attention/attention/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/concat&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/attention/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/attention/MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/attention/Reshape_1/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/attention/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/Reshape_1/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/attention/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/Conv2D&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/Reshape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/attention/Tanh&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/attention/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_v/read&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/attention/Sum/reduction_indices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\002\\\\000\\\\000\\\\000\\\\003\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/attention/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/Sum/reduction_indices&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/attention/Softmax&quot;\\n  op: &quot;Softmax&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/Sum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/attention/Reshape_2/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\t\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/attention/Reshape_2&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/Softmax&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/Reshape_2/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/attention/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/Reshape_2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/attention/Sum_1/reduction_indices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/attention/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/mul_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/Sum_1/reduction_indices&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/attention/Reshape_3/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/attention/Reshape_3&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/Sum_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/Reshape_3/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/attention/Slice/begin&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/attention/Slice/size&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/attention/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/Slice/begin&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/Slice/size&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/Reshape_3&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/attn_output_projection/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/ExpandDims/dim&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/ExpandDims&quot;\\n  op: &quot;ExpandDims&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attn_output_projection/BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/ExpandDims/dim&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tdim&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/concat_1/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/concat_1&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/Slice&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/ExpandDims&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/concat_1/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/Reshape_1/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\200\\\\004\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_2/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/concat_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/Reshape_1/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/strided_slice_3/stack&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\003\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/strided_slice_3/stack_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\004\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/strided_slice_3/stack_2&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/strided_slice_3&quot;\\n  op: &quot;StridedSlice&quot;\\n  input: &quot;Placeholder&quot;\\n  input: &quot;Encoder/strided_slice_3/stack&quot;\\n  input: &quot;Encoder/strided_slice_3/stack_1&quot;\\n  input: &quot;Encoder/strided_slice_3/stack_2&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;begin_mask&quot;\\n    value {\\n      i: 5\\n    }\\n  }\\n  attr {\\n    key: &quot;ellipsis_mask&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;end_mask&quot;\\n    value {\\n      i: 5\\n    }\\n  }\\n  attr {\\n    key: &quot;new_axis_mask&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;shrink_axis_mask&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\t\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/Reshape_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/attention_cell_wrapper/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/attention_cell_wrapper/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Encoder/strided_slice_3&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/Reshape_3&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention_cell_wrapper/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/attention_cell_wrapper/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention_cell_wrapper/concat&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention_cell_wrapper/MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/split/split_dim&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/split&quot;\\n  op: &quot;Split&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/split/split_dim&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;num_split&quot;\\n    value {\\n      i: 4\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/add/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/split:2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/add/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/Sigmoid&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/Sigmoid&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/Sigmoid_1&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/split&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/Tanh&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/split:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/Sigmoid_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_1&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/Tanh_1&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/Sigmoid_2&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/split:3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/Tanh_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/Sigmoid_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/attention/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\t\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/attention/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/attention/Conv2D&quot;\\n  op: &quot;Conv2D&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_w/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;SAME&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/attention/attention/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/concat&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/attention/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/attention/MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/attention/Reshape_1/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/attention/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/Reshape_1/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/attention/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/Conv2D&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/Reshape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/attention/Tanh&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/attention/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_v/read&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/attention/Sum/reduction_indices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\002\\\\000\\\\000\\\\000\\\\003\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/attention/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/Sum/reduction_indices&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/attention/Softmax&quot;\\n  op: &quot;Softmax&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/Sum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/attention/Reshape_2/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\t\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/attention/Reshape_2&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/Softmax&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/Reshape_2/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/attention/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/Reshape_2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/attention/Sum_1/reduction_indices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/attention/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/mul_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/Sum_1/reduction_indices&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/attention/Reshape_3/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/attention/Reshape_3&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/Sum_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/Reshape_3/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/attention/Slice/begin&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/attention/Slice/size&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/attention/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/Slice/begin&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/Slice/size&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/Reshape_3&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/attn_output_projection/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/ExpandDims/dim&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/ExpandDims&quot;\\n  op: &quot;ExpandDims&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attn_output_projection/BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/ExpandDims/dim&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tdim&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/concat_1/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/concat_1&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/Slice&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/ExpandDims&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/concat_1/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/Reshape_1/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\200\\\\004\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_3/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/concat_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/Reshape_1/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/strided_slice_4/stack&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\004\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/strided_slice_4/stack_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\005\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/strided_slice_4/stack_2&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/strided_slice_4&quot;\\n  op: &quot;StridedSlice&quot;\\n  input: &quot;Placeholder&quot;\\n  input: &quot;Encoder/strided_slice_4/stack&quot;\\n  input: &quot;Encoder/strided_slice_4/stack_1&quot;\\n  input: &quot;Encoder/strided_slice_4/stack_2&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;begin_mask&quot;\\n    value {\\n      i: 5\\n    }\\n  }\\n  attr {\\n    key: &quot;ellipsis_mask&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;end_mask&quot;\\n    value {\\n      i: 5\\n    }\\n  }\\n  attr {\\n    key: &quot;new_axis_mask&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;shrink_axis_mask&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\t\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/Reshape_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/attention_cell_wrapper/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/attention_cell_wrapper/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Encoder/strided_slice_4&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/Reshape_3&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention_cell_wrapper/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/attention_cell_wrapper/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention_cell_wrapper/concat&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention_cell_wrapper/MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/concat&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/split/split_dim&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/split&quot;\\n  op: &quot;Split&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/split/split_dim&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;num_split&quot;\\n    value {\\n      i: 4\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/add/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/split:2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/add/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/Sigmoid&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/Sigmoid&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/Sigmoid_1&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/split&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/Tanh&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/split:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/Sigmoid_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_1&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/Tanh_1&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/Sigmoid_2&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/split:3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_2&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/Tanh_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/Sigmoid_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/attention/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\t\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/attention/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/attention/Conv2D&quot;\\n  op: &quot;Conv2D&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_w/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;SAME&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/attention/attention/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/concat&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/attention/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/attention/MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/attention/Reshape_1/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/attention/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/Reshape_1/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/attention/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/Conv2D&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/Reshape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/attention/Tanh&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/attention/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_v/read&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/attention/Sum/reduction_indices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\002\\\\000\\\\000\\\\000\\\\003\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/attention/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/Sum/reduction_indices&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/attention/Softmax&quot;\\n  op: &quot;Softmax&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/Sum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/attention/Reshape_2/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\t\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/attention/Reshape_2&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/Softmax&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/Reshape_2/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/attention/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/Reshape_2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/attention/Sum_1/reduction_indices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/attention/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/mul_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/Sum_1/reduction_indices&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/attention/Reshape_3/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/attention/Reshape_3&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/Sum_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/Reshape_3/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/attention/Slice/begin&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/attention/Slice/size&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/attention/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/Slice/begin&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/Slice/size&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/Reshape_3&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/concat&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/attn_output_projection/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/ExpandDims/dim&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/ExpandDims&quot;\\n  op: &quot;ExpandDims&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attn_output_projection/BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/ExpandDims/dim&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tdim&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/concat_1/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/concat_1&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/Slice&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/ExpandDims&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/concat_1/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/Reshape_1/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\200\\\\004\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_4/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/concat_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/Reshape_1/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/strided_slice_5/stack&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\005\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/strided_slice_5/stack_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\006\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/strided_slice_5/stack_2&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/strided_slice_5&quot;\\n  op: &quot;StridedSlice&quot;\\n  input: &quot;Placeholder&quot;\\n  input: &quot;Encoder/strided_slice_5/stack&quot;\\n  input: &quot;Encoder/strided_slice_5/stack_1&quot;\\n  input: &quot;Encoder/strided_slice_5/stack_2&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;begin_mask&quot;\\n    value {\\n      i: 5\\n    }\\n  }\\n  attr {\\n    key: &quot;ellipsis_mask&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;end_mask&quot;\\n    value {\\n      i: 5\\n    }\\n  }\\n  attr {\\n    key: &quot;new_axis_mask&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;shrink_axis_mask&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\t\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/Reshape_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/attention_cell_wrapper/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/attention_cell_wrapper/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Encoder/strided_slice_5&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/Reshape_3&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention_cell_wrapper/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/attention_cell_wrapper/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention_cell_wrapper/concat&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention_cell_wrapper/MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/concat&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/split/split_dim&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/split&quot;\\n  op: &quot;Split&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/split/split_dim&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;num_split&quot;\\n    value {\\n      i: 4\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/add/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/split:2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/add/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/Sigmoid&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/Sigmoid&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/Sigmoid_1&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/split&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/Tanh&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/split:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/Sigmoid_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_1&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/Tanh_1&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/Sigmoid_2&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/split:3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_2&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/Tanh_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/Sigmoid_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/attention/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\t\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/attention/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/attention/Conv2D&quot;\\n  op: &quot;Conv2D&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_w/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;SAME&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/attention/attention/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/concat&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/attention/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/attention/MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/attention/Reshape_1/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/attention/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/Reshape_1/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/attention/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/Conv2D&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/Reshape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/attention/Tanh&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/attention/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_v/read&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/attention/Sum/reduction_indices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\002\\\\000\\\\000\\\\000\\\\003\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/attention/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/Sum/reduction_indices&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/attention/Softmax&quot;\\n  op: &quot;Softmax&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/Sum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/attention/Reshape_2/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\t\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/attention/Reshape_2&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/Softmax&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/Reshape_2/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/attention/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/Reshape_2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/attention/Sum_1/reduction_indices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/attention/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/mul_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/Sum_1/reduction_indices&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/attention/Reshape_3/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/attention/Reshape_3&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/Sum_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/Reshape_3/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/attention/Slice/begin&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/attention/Slice/size&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/attention/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/Slice/begin&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/Slice/size&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/Reshape_3&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/concat&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/attn_output_projection/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/ExpandDims/dim&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/ExpandDims&quot;\\n  op: &quot;ExpandDims&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attn_output_projection/BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/ExpandDims/dim&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tdim&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/concat_1/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/concat_1&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/Slice&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/ExpandDims&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/concat_1/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/Reshape_1/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\200\\\\004\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_5/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/concat_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/Reshape_1/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/strided_slice_6/stack&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\006\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/strided_slice_6/stack_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\007\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/strided_slice_6/stack_2&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/strided_slice_6&quot;\\n  op: &quot;StridedSlice&quot;\\n  input: &quot;Placeholder&quot;\\n  input: &quot;Encoder/strided_slice_6/stack&quot;\\n  input: &quot;Encoder/strided_slice_6/stack_1&quot;\\n  input: &quot;Encoder/strided_slice_6/stack_2&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;begin_mask&quot;\\n    value {\\n      i: 5\\n    }\\n  }\\n  attr {\\n    key: &quot;ellipsis_mask&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;end_mask&quot;\\n    value {\\n      i: 5\\n    }\\n  }\\n  attr {\\n    key: &quot;new_axis_mask&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;shrink_axis_mask&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\t\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/Reshape_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/attention_cell_wrapper/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/attention_cell_wrapper/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Encoder/strided_slice_6&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/Reshape_3&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention_cell_wrapper/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/attention_cell_wrapper/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention_cell_wrapper/concat&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention_cell_wrapper/MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/concat&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/split/split_dim&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/split&quot;\\n  op: &quot;Split&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/split/split_dim&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;num_split&quot;\\n    value {\\n      i: 4\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/add/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/split:2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/add/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/Sigmoid&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/Sigmoid&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/Sigmoid_1&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/split&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/Tanh&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/split:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/Sigmoid_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_1&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/Tanh_1&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/Sigmoid_2&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/split:3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_2&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/Tanh_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/Sigmoid_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/attention/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\t\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/attention/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/attention/Conv2D&quot;\\n  op: &quot;Conv2D&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_w/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;SAME&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/attention/attention/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/concat&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/attention/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/attention/MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/attention/Reshape_1/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/attention/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/Reshape_1/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/attention/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/Conv2D&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/Reshape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/attention/Tanh&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/attention/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_v/read&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/attention/Sum/reduction_indices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\002\\\\000\\\\000\\\\000\\\\003\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/attention/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/Sum/reduction_indices&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/attention/Softmax&quot;\\n  op: &quot;Softmax&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/Sum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/attention/Reshape_2/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\t\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/attention/Reshape_2&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/Softmax&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/Reshape_2/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/attention/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/Reshape_2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/attention/Sum_1/reduction_indices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/attention/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/mul_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/Sum_1/reduction_indices&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/attention/Reshape_3/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/attention/Reshape_3&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/Sum_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/Reshape_3/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/attention/Slice/begin&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/attention/Slice/size&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/attention/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/Slice/begin&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/Slice/size&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/Reshape_3&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/concat&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/attn_output_projection/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/ExpandDims/dim&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/ExpandDims&quot;\\n  op: &quot;ExpandDims&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attn_output_projection/BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/ExpandDims/dim&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tdim&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/concat_1/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/concat_1&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/Slice&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/ExpandDims&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/concat_1/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/Reshape_1/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\200\\\\004\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_6/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/concat_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/Reshape_1/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/strided_slice_7/stack&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\007\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/strided_slice_7/stack_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\010\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/strided_slice_7/stack_2&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/strided_slice_7&quot;\\n  op: &quot;StridedSlice&quot;\\n  input: &quot;Placeholder&quot;\\n  input: &quot;Encoder/strided_slice_7/stack&quot;\\n  input: &quot;Encoder/strided_slice_7/stack_1&quot;\\n  input: &quot;Encoder/strided_slice_7/stack_2&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;begin_mask&quot;\\n    value {\\n      i: 5\\n    }\\n  }\\n  attr {\\n    key: &quot;ellipsis_mask&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;end_mask&quot;\\n    value {\\n      i: 5\\n    }\\n  }\\n  attr {\\n    key: &quot;new_axis_mask&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;shrink_axis_mask&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\t\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/Reshape_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/attention_cell_wrapper/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/attention_cell_wrapper/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Encoder/strided_slice_7&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/Reshape_3&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention_cell_wrapper/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/attention_cell_wrapper/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention_cell_wrapper/concat&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention_cell_wrapper/MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/concat&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/split/split_dim&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/split&quot;\\n  op: &quot;Split&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/split/split_dim&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;num_split&quot;\\n    value {\\n      i: 4\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/add/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/split:2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/add/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/Sigmoid&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/Sigmoid&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/Sigmoid_1&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/split&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/Tanh&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/split:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/Sigmoid_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_1&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/Tanh_1&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/Sigmoid_2&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/split:3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_2&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/Tanh_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/Sigmoid_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/attention/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\t\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/attention/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/attention/Conv2D&quot;\\n  op: &quot;Conv2D&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_w/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;SAME&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/attention/attention/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/concat&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/attention/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/attention/MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/attention/Reshape_1/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/attention/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/Reshape_1/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/attention/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/Conv2D&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/Reshape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/attention/Tanh&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/attention/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_v/read&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/attention/Sum/reduction_indices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\002\\\\000\\\\000\\\\000\\\\003\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/attention/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/Sum/reduction_indices&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/attention/Softmax&quot;\\n  op: &quot;Softmax&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/Sum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/attention/Reshape_2/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\t\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/attention/Reshape_2&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/Softmax&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/Reshape_2/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/attention/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/Reshape_2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/attention/Sum_1/reduction_indices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/attention/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/mul_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/Sum_1/reduction_indices&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/attention/Reshape_3/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/attention/Reshape_3&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/Sum_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/Reshape_3/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/attention/Slice/begin&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/attention/Slice/size&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/attention/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/Slice/begin&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/Slice/size&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/Reshape_3&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/concat&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/attn_output_projection/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/ExpandDims/dim&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/ExpandDims&quot;\\n  op: &quot;ExpandDims&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attn_output_projection/BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/ExpandDims/dim&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tdim&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/concat_1/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/concat_1&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/Slice&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/ExpandDims&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/concat_1/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/Reshape_1/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\200\\\\004\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_7/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/concat_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/Reshape_1/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/strided_slice_8/stack&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\010\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/strided_slice_8/stack_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\t\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/strided_slice_8/stack_2&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/strided_slice_8&quot;\\n  op: &quot;StridedSlice&quot;\\n  input: &quot;Placeholder&quot;\\n  input: &quot;Encoder/strided_slice_8/stack&quot;\\n  input: &quot;Encoder/strided_slice_8/stack_1&quot;\\n  input: &quot;Encoder/strided_slice_8/stack_2&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;begin_mask&quot;\\n    value {\\n      i: 5\\n    }\\n  }\\n  attr {\\n    key: &quot;ellipsis_mask&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;end_mask&quot;\\n    value {\\n      i: 5\\n    }\\n  }\\n  attr {\\n    key: &quot;new_axis_mask&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;shrink_axis_mask&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\t\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/Reshape_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/attention_cell_wrapper/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/attention_cell_wrapper/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Encoder/strided_slice_8&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/Reshape_3&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention_cell_wrapper/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/attention_cell_wrapper/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention_cell_wrapper/concat&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention_cell_wrapper/MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/concat&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/split/split_dim&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/split&quot;\\n  op: &quot;Split&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/split/split_dim&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;num_split&quot;\\n    value {\\n      i: 4\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/add/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/split:2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/add/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/Sigmoid&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/Sigmoid&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/Sigmoid_1&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/split&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/Tanh&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/split:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/Sigmoid_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_1&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/Tanh_1&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/Sigmoid_2&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/split:3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_2&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/Tanh_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/Sigmoid_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/attention/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\t\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/attention/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/attention/Conv2D&quot;\\n  op: &quot;Conv2D&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_w/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;SAME&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/attention/attention/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/concat&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/attention/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/attention/MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/attention/Reshape_1/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/attention/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/Reshape_1/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/attention/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/Conv2D&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/Reshape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/attention/Tanh&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/attention/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_v/read&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/attention/Sum/reduction_indices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\002\\\\000\\\\000\\\\000\\\\003\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/attention/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/Sum/reduction_indices&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/attention/Softmax&quot;\\n  op: &quot;Softmax&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/Sum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/attention/Reshape_2/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\t\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/attention/Reshape_2&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/Softmax&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/Reshape_2/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/attention/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/Reshape_2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/attention/Sum_1/reduction_indices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/attention/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/mul_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/Sum_1/reduction_indices&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/attention/Reshape_3/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/attention/Reshape_3&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/Sum_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/Reshape_3/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/attention/Slice/begin&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/attention/Slice/size&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/attention/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/Slice/begin&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/Slice/size&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/Reshape_3&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/concat&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/attn_output_projection/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/ExpandDims/dim&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/ExpandDims&quot;\\n  op: &quot;ExpandDims&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attn_output_projection/BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/ExpandDims/dim&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tdim&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/concat_1/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/concat_1&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/Slice&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/ExpandDims&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/concat_1/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/Reshape_1/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\200\\\\004\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper_8/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/concat_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/Reshape_1/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;truncated_normal/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\200\\\\000\\\\000\\\\000\\\\014\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;truncated_normal/mean&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;truncated_normal/stddev&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.009999999776482582\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;truncated_normal/TruncatedNormal&quot;\\n  op: &quot;TruncatedNormal&quot;\\n  input: &quot;truncated_normal/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 1234\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 657\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;truncated_normal/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;truncated_normal/TruncatedNormal&quot;\\n  input: &quot;truncated_normal/stddev&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;truncated_normal&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;truncated_normal/mul&quot;\\n  input: &quot;truncated_normal/mean&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 128\\n        }\\n        dim {\\n          size: 12\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Variable&quot;\\n  input: &quot;truncated_normal&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Variable&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Variable&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Variable&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros_4&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 12\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable_1&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 12\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable_1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Variable_1&quot;\\n  input: &quot;zeros_4&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Variable_1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable_1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Variable_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Variable_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attn_output_projection/BiasAdd&quot;\\n  input: &quot;Variable/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Decoder/MatMul&quot;\\n  input: &quot;Variable_1/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/Softmax&quot;\\n  op: &quot;Softmax&quot;\\n  input: &quot;Decoder/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/ArgMax/dimension&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: -1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/ArgMax&quot;\\n  op: &quot;ArgMax&quot;\\n  input: &quot;Decoder/Softmax&quot;\\n  input: &quot;Decoder/ArgMax/dimension&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/one_hot/on_value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/one_hot/off_value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/one_hot/depth&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 5\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/one_hot&quot;\\n  op: &quot;OneHot&quot;\\n  input: &quot;Decoder/ArgMax&quot;\\n  input: &quot;Decoder/one_hot/depth&quot;\\n  input: &quot;Decoder/one_hot/on_value&quot;\\n  input: &quot;Decoder/one_hot/off_value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;TI&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n  attr {\\n    key: &quot;axis&quot;\\n    value {\\n      i: -1\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\t\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/Reshape_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/weights/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\205\\\\000\\\\000\\\\000\\\\005\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/weights/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.20851440727710724\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/weights/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.20851440727710724\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/weights/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/weights/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 1234\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 681\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/weights/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/weights/Initializer/random_uniform/max&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/weights/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/weights/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/weights/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/weights/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/weights/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/weights/Initializer/random_uniform/mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/weights/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/weights&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 133\\n        }\\n        dim {\\n          size: 5\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/weights/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/weights&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/weights/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/weights/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/weights&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention_cell_wrapper/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention_cell_wrapper/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Decoder/one_hot&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/Reshape_3&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention_cell_wrapper/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention_cell_wrapper/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention_cell_wrapper/concat&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/biases/Initializer/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 5\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/biases&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 5\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/biases/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/biases&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/biases/Initializer/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/biases/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/biases&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/biases&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention_cell_wrapper/MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\205\\\\000\\\\000\\\\000\\\\000\\\\002\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.09644856303930283\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.09644856303930283\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 1234\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 699\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights/Initializer/random_uniform/max&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights/Initializer/random_uniform/mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 133\\n        }\\n        dim {\\n          size: 512\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_2&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/biases/Initializer/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/basic_lstm_cell/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 512\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/biases&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/basic_lstm_cell/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 512\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/biases/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/biases&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/biases/Initializer/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/basic_lstm_cell/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/biases/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/biases&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/basic_lstm_cell/biases&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/split/split_dim&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/split&quot;\\n  op: &quot;Split&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/split/split_dim&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;num_split&quot;\\n    value {\\n      i: 4\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/add/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/split:2&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/add/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/Sigmoid&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/Sigmoid&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/Sigmoid_1&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/split&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/Tanh&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/split:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/Sigmoid_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/add_1&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/mul_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/Tanh_1&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/Sigmoid_2&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/split:3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/mul_2&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/Tanh_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/Sigmoid_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/add_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/mul_2&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/attn_w/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/attn_w&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/attn_w/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/attn_w&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.1530931144952774\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/attn_w/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/attn_w&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.1530931144952774\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/attn_w/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_w/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/attn_w&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 1234\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 732\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/attn_w/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_w/Initializer/random_uniform/max&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_w/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/attn_w&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/attn_w/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_w/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_w/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/attn_w&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/attn_w/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_w/Initializer/random_uniform/mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_w/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/attn_w&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/attn_w&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/attn_w&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 1\\n        }\\n        dim {\\n          size: 1\\n        }\\n        dim {\\n          size: 128\\n        }\\n        dim {\\n          size: 128\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/attn_w/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_w&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_w/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/attn_w&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/attn_w/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_w&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/attn_w&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/attn_v/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/attn_v&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 128\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/attn_v/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/attn_v&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.1530931144952774\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/attn_v/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/attn_v&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.1530931144952774\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/attn_v/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_v/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/attn_v&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 1234\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 742\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/attn_v/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_v/Initializer/random_uniform/max&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_v/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/attn_v&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/attn_v/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_v/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_v/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/attn_v&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/attn_v/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_v/Initializer/random_uniform/mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_v/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/attn_v&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/attn_v&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/attn_v&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 128\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/attn_v/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_v&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_v/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/attn_v&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/attn_v/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_v&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/attn_v&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\t\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/Reshape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/Conv2D&quot;\\n  op: &quot;Conv2D&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/Reshape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_w/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;SAME&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/weights/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\001\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/weights/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.125\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/weights/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.125\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/weights/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/weights/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 1234\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 755\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/weights/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/weights/Initializer/random_uniform/max&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/weights/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/weights/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/weights/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/weights/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/weights/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/weights/Initializer/random_uniform/mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/weights/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/weights&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 256\\n        }\\n        dim {\\n          size: 128\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/weights/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/weights&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/weights/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/weights/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/weights&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/attention/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/concat&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/biases/Initializer/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 128\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/biases&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 128\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/biases/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/biases&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/biases/Initializer/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/biases/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/biases&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/biases&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attention/MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/Reshape_1/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/BiasAdd&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/Reshape_1/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/Conv2D&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/Reshape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/Tanh&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_v/read&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/Sum/reduction_indices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\002\\\\000\\\\000\\\\000\\\\003\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/Sum/reduction_indices&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/Softmax&quot;\\n  op: &quot;Softmax&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/Sum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/Reshape_2/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\t\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/Reshape_2&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/Softmax&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/Reshape_2/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/Reshape_2&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/Sum_1/reduction_indices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/mul_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/Sum_1/reduction_indices&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/Reshape_3/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/Reshape_3&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/Sum_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/Reshape_3/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/Slice/begin&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/Slice/size&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/Reshape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/Slice/begin&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/Slice/size&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\001\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.125\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.125\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 1234\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 789\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights/Initializer/random_uniform/max&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights/Initializer/random_uniform/mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 256\\n        }\\n        dim {\\n          size: 128\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/mul_2&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/Reshape_3&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attn_output_projection/biases/Initializer/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attn_output_projection/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 128\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attn_output_projection/biases&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attn_output_projection/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 128\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attn_output_projection/biases/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/biases&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/biases/Initializer/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attn_output_projection/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attn_output_projection/biases/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/biases&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attn_output_projection/biases&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attn_output_projection/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/ExpandDims/dim&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/ExpandDims&quot;\\n  op: &quot;ExpandDims&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/BiasAdd&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/ExpandDims/dim&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tdim&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/concat_1/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/concat_1&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/Slice&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/ExpandDims&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/concat_1/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/Reshape_1/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\200\\\\004\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/concat_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/Reshape_1/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/BiasAdd&quot;\\n  input: &quot;Variable/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/add_1&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Decoder/MatMul_1&quot;\\n  input: &quot;Variable_1/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/Softmax_1&quot;\\n  op: &quot;Softmax&quot;\\n  input: &quot;Decoder/add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/ArgMax_1/dimension&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: -1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/ArgMax_1&quot;\\n  op: &quot;ArgMax&quot;\\n  input: &quot;Decoder/Softmax_1&quot;\\n  input: &quot;Decoder/ArgMax_1/dimension&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/one_hot_1/on_value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/one_hot_1/off_value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/one_hot_1/depth&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 5\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/one_hot_1&quot;\\n  op: &quot;OneHot&quot;\\n  input: &quot;Decoder/ArgMax_1&quot;\\n  input: &quot;Decoder/one_hot_1/depth&quot;\\n  input: &quot;Decoder/one_hot_1/on_value&quot;\\n  input: &quot;Decoder/one_hot_1/off_value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;TI&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n  attr {\\n    key: &quot;axis&quot;\\n    value {\\n      i: -1\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\t\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/Reshape_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/attention_cell_wrapper/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/attention_cell_wrapper/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Decoder/one_hot_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/Reshape_3&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention_cell_wrapper/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/attention_cell_wrapper/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention_cell_wrapper/concat&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention_cell_wrapper/MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/BiasAdd&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/mul_2&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/split/split_dim&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/split&quot;\\n  op: &quot;Split&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/split/split_dim&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;num_split&quot;\\n    value {\\n      i: 4\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/add/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/split:2&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/add/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/Sigmoid&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/add_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/Sigmoid&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/Sigmoid_1&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/split&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/Tanh&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/split:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/Sigmoid_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_1&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/Tanh_1&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/Sigmoid_2&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/split:3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/Tanh_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/Sigmoid_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/attention/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\t\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/attention/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/Reshape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/attention/Conv2D&quot;\\n  op: &quot;Conv2D&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/Reshape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_w/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;SAME&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/attention/attention/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/concat&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/attention/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/attention/MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/attention/Reshape_1/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/attention/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/BiasAdd&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/Reshape_1/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/attention/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/Conv2D&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/Reshape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/attention/Tanh&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/attention/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_v/read&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/attention/Sum/reduction_indices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\002\\\\000\\\\000\\\\000\\\\003\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/attention/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/Sum/reduction_indices&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/attention/Softmax&quot;\\n  op: &quot;Softmax&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/Sum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/attention/Reshape_2/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\t\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/attention/Reshape_2&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/Softmax&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/Reshape_2/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/attention/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/Reshape_2&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/attention/Sum_1/reduction_indices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/attention/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/mul_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/Sum_1/reduction_indices&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/attention/Reshape_3/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/attention/Reshape_3&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/Sum_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/Reshape_3/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/attention/Slice/begin&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/attention/Slice/size&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/attention/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/Reshape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/Slice/begin&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/Slice/size&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/Reshape_3&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/attn_output_projection/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/ExpandDims/dim&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/ExpandDims&quot;\\n  op: &quot;ExpandDims&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attn_output_projection/BiasAdd&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/ExpandDims/dim&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tdim&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/concat_1/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/concat_1&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/Slice&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/ExpandDims&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/concat_1/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/Reshape_1/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\200\\\\004\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_1/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/concat_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/Reshape_1/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/MatMul_2&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attn_output_projection/BiasAdd&quot;\\n  input: &quot;Variable/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/add_2&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Decoder/MatMul_2&quot;\\n  input: &quot;Variable_1/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/Softmax_2&quot;\\n  op: &quot;Softmax&quot;\\n  input: &quot;Decoder/add_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/ArgMax_2/dimension&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: -1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/ArgMax_2&quot;\\n  op: &quot;ArgMax&quot;\\n  input: &quot;Decoder/Softmax_2&quot;\\n  input: &quot;Decoder/ArgMax_2/dimension&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/one_hot_2/on_value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/one_hot_2/off_value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/one_hot_2/depth&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 5\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/one_hot_2&quot;\\n  op: &quot;OneHot&quot;\\n  input: &quot;Decoder/ArgMax_2&quot;\\n  input: &quot;Decoder/one_hot_2/depth&quot;\\n  input: &quot;Decoder/one_hot_2/on_value&quot;\\n  input: &quot;Decoder/one_hot_2/off_value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;TI&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n  attr {\\n    key: &quot;axis&quot;\\n    value {\\n      i: -1\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\t\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/Reshape_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/attention_cell_wrapper/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/attention_cell_wrapper/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Decoder/one_hot_2&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/Reshape_3&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention_cell_wrapper/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/attention_cell_wrapper/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention_cell_wrapper/concat&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention_cell_wrapper/MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/BiasAdd&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/split/split_dim&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/split&quot;\\n  op: &quot;Split&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/split/split_dim&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;num_split&quot;\\n    value {\\n      i: 4\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/add/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/split:2&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/add/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/Sigmoid&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/Sigmoid&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/Sigmoid_1&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/split&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/Tanh&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/split:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/Sigmoid_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_1&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/Tanh_1&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/Sigmoid_2&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/split:3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/Tanh_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/Sigmoid_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/attention/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\t\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/attention/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/Reshape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/attention/Conv2D&quot;\\n  op: &quot;Conv2D&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/Reshape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_w/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;SAME&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/attention/attention/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/concat&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/attention/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/attention/MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/attention/Reshape_1/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/attention/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/BiasAdd&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/Reshape_1/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/attention/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/Conv2D&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/Reshape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/attention/Tanh&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/attention/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_v/read&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/attention/Sum/reduction_indices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\002\\\\000\\\\000\\\\000\\\\003\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/attention/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/Sum/reduction_indices&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/attention/Softmax&quot;\\n  op: &quot;Softmax&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/Sum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/attention/Reshape_2/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\t\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/attention/Reshape_2&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/Softmax&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/Reshape_2/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/attention/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/Reshape_2&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/attention/Sum_1/reduction_indices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/attention/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/mul_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/Sum_1/reduction_indices&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/attention/Reshape_3/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/attention/Reshape_3&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/Sum_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/Reshape_3/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/attention/Slice/begin&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/attention/Slice/size&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/attention/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/Reshape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/Slice/begin&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/Slice/size&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/Reshape_3&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/attn_output_projection/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/ExpandDims/dim&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/ExpandDims&quot;\\n  op: &quot;ExpandDims&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attn_output_projection/BiasAdd&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/ExpandDims/dim&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tdim&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/concat_1/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/concat_1&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/Slice&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/ExpandDims&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/concat_1/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/Reshape_1/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\200\\\\004\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_2/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/concat_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/Reshape_1/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/MatMul_3&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attn_output_projection/BiasAdd&quot;\\n  input: &quot;Variable/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/add_3&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Decoder/MatMul_3&quot;\\n  input: &quot;Variable_1/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/Softmax_3&quot;\\n  op: &quot;Softmax&quot;\\n  input: &quot;Decoder/add_3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/ArgMax_3/dimension&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: -1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/ArgMax_3&quot;\\n  op: &quot;ArgMax&quot;\\n  input: &quot;Decoder/Softmax_3&quot;\\n  input: &quot;Decoder/ArgMax_3/dimension&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/one_hot_3/on_value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/one_hot_3/off_value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/one_hot_3/depth&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 5\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/one_hot_3&quot;\\n  op: &quot;OneHot&quot;\\n  input: &quot;Decoder/ArgMax_3&quot;\\n  input: &quot;Decoder/one_hot_3/depth&quot;\\n  input: &quot;Decoder/one_hot_3/on_value&quot;\\n  input: &quot;Decoder/one_hot_3/off_value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;TI&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n  attr {\\n    key: &quot;axis&quot;\\n    value {\\n      i: -1\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\t\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/Reshape_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/attention_cell_wrapper/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/attention_cell_wrapper/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Decoder/one_hot_3&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/Reshape_3&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attention_cell_wrapper/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/attention_cell_wrapper/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attention_cell_wrapper/concat&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attention_cell_wrapper/MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/BiasAdd&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/split/split_dim&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/split&quot;\\n  op: &quot;Split&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/split/split_dim&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;num_split&quot;\\n    value {\\n      i: 4\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/add/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/split:2&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/add/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/Sigmoid&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/Sigmoid&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/Sigmoid_1&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/split&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/Tanh&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/split:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/Sigmoid_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_1&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/Tanh_1&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/Sigmoid_2&quot;\\n  op: &quot;Sigmoid&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/split:3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/Tanh_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/Sigmoid_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/attention/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\t\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/attention/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/Reshape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attention/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/attention/Conv2D&quot;\\n  op: &quot;Conv2D&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attention/Reshape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_w/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;SAME&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/attention/attention/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/concat&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/attention/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attention/attention/MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/attention/Reshape_1/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/attention/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attention/BiasAdd&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attention/Reshape_1/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/attention/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attention/Conv2D&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attention/Reshape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/attention/Tanh&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attention/add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/attention/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_v/read&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attention/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/attention/Sum/reduction_indices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\002\\\\000\\\\000\\\\000\\\\003\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/attention/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attention/mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attention/Sum/reduction_indices&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/attention/Softmax&quot;\\n  op: &quot;Softmax&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attention/Sum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/attention/Reshape_2/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\t\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/attention/Reshape_2&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attention/Softmax&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attention/Reshape_2/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/attention/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attention/Reshape_2&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/attention/Sum_1/reduction_indices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/attention/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attention/mul_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attention/Sum_1/reduction_indices&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/attention/Reshape_3/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/attention/Reshape_3&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attention/Sum_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attention/Reshape_3/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/attention/Slice/begin&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/attention/Slice/size&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377\\\\377&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/attention/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/Reshape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attention/Slice/begin&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attention/Slice/size&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attention/Reshape_3&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/attn_output_projection/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/biases/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/ExpandDims/dim&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/ExpandDims&quot;\\n  op: &quot;ExpandDims&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attn_output_projection/BiasAdd&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/ExpandDims/dim&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tdim&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/concat_1/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/concat_1&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attention/Slice&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/ExpandDims&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/concat_1/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/Reshape_1/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\200\\\\004\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper_3/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/concat_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/Reshape_1/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attn_output_projection/BiasAdd&quot;\\n  input: &quot;Variable/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;MatMul&quot;\\n  input: &quot;Variable_1/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Softmax&quot;\\n  op: &quot;Softmax&quot;\\n  input: &quot;add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;Decoder/Softmax&quot;\\n  input: &quot;Decoder/Softmax_1&quot;\\n  input: &quot;Decoder/Softmax_2&quot;\\n  input: &quot;Decoder/Softmax_3&quot;\\n  input: &quot;Softmax&quot;\\n  input: &quot;concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 5\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\005\\\\000\\\\000\\\\000\\\\014\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;concat&quot;\\n  input: &quot;Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;Reshape&quot;\\n  input: &quot;clip_by_value/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.000000013351432e-10\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value/Minimum&quot;\\n  input: &quot;clip_by_value/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Log&quot;\\n  op: &quot;Log&quot;\\n  input: &quot;clip_by_value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Placeholder_1&quot;\\n  input: &quot;Log&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Sum/reduction_indices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;mul&quot;\\n  input: &quot;Sum/reduction_indices&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Neg&quot;\\n  op: &quot;Neg&quot;\\n  input: &quot;Sum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Mean&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;Neg&quot;\\n  input: &quot;Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Shape&quot;\\n  input: &quot;gradients/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Fill&quot;\\n  input: &quot;gradients/Mean_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Neg&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/Mean_grad/Reshape&quot;\\n  input: &quot;gradients/Mean_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Neg&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean_grad/Shape_2&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean_grad/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean_grad/Prod&quot;\\n  op: &quot;Prod&quot;\\n  input: &quot;gradients/Mean_grad/Shape_1&quot;\\n  input: &quot;gradients/Mean_grad/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean_grad/Const_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean_grad/Prod_1&quot;\\n  op: &quot;Prod&quot;\\n  input: &quot;gradients/Mean_grad/Shape_2&quot;\\n  input: &quot;gradients/Mean_grad/Const_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean_grad/Maximum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean_grad/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;gradients/Mean_grad/Prod_1&quot;\\n  input: &quot;gradients/Mean_grad/Maximum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean_grad/floordiv&quot;\\n  op: &quot;FloorDiv&quot;\\n  input: &quot;gradients/Mean_grad/Prod&quot;\\n  input: &quot;gradients/Mean_grad/Maximum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean_grad/Cast&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;gradients/Mean_grad/floordiv&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Mean_grad/truediv&quot;\\n  op: &quot;RealDiv&quot;\\n  input: &quot;gradients/Mean_grad/Tile&quot;\\n  input: &quot;gradients/Mean_grad/Cast&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Neg_grad/Neg&quot;\\n  op: &quot;Neg&quot;\\n  input: &quot;gradients/Mean_grad/truediv&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Sum_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Sum_grad/Size&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 3\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Sum_grad/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Sum/reduction_indices&quot;\\n  input: &quot;gradients/Sum_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Sum_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;gradients/Sum_grad/add&quot;\\n  input: &quot;gradients/Sum_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Sum_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Sum_grad/range/start&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Sum_grad/range/delta&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Sum_grad/range&quot;\\n  op: &quot;Range&quot;\\n  input: &quot;gradients/Sum_grad/range/start&quot;\\n  input: &quot;gradients/Sum_grad/Size&quot;\\n  input: &quot;gradients/Sum_grad/range/delta&quot;\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Sum_grad/Fill/value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Sum_grad/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Sum_grad/Shape_1&quot;\\n  input: &quot;gradients/Sum_grad/Fill/value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Sum_grad/DynamicStitch&quot;\\n  op: &quot;DynamicStitch&quot;\\n  input: &quot;gradients/Sum_grad/range&quot;\\n  input: &quot;gradients/Sum_grad/mod&quot;\\n  input: &quot;gradients/Sum_grad/Shape&quot;\\n  input: &quot;gradients/Sum_grad/Fill&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Sum_grad/Maximum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Sum_grad/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;gradients/Sum_grad/DynamicStitch&quot;\\n  input: &quot;gradients/Sum_grad/Maximum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Sum_grad/floordiv&quot;\\n  op: &quot;FloorDiv&quot;\\n  input: &quot;gradients/Sum_grad/Shape&quot;\\n  input: &quot;gradients/Sum_grad/Maximum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Sum_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Neg_grad/Neg&quot;\\n  input: &quot;gradients/Sum_grad/DynamicStitch&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Sum_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/Sum_grad/Reshape&quot;\\n  input: &quot;gradients/Sum_grad/floordiv&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mul_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Placeholder_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mul_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Log&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mul_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/mul_grad/Shape&quot;\\n  input: &quot;gradients/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mul_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Sum_grad/Tile&quot;\\n  input: &quot;Log&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mul_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/mul_grad/mul&quot;\\n  input: &quot;gradients/mul_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mul_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/mul_grad/Sum&quot;\\n  input: &quot;gradients/mul_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mul_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Placeholder_1&quot;\\n  input: &quot;gradients/Sum_grad/Tile&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mul_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/mul_grad/mul_1&quot;\\n  input: &quot;gradients/mul_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mul_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/mul_grad/Sum_1&quot;\\n  input: &quot;gradients/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/mul_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/mul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/mul_grad/Reshape_1&quot;\\n  input: &quot;^gradients/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/mul_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Log_grad/Reciprocal&quot;\\n  op: &quot;Reciprocal&quot;\\n  input: &quot;clip_by_value&quot;\\n  input: &quot;^gradients/mul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Log_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/mul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Log_grad/Reciprocal&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/clip_by_value_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;clip_by_value/Minimum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/clip_by_value_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/clip_by_value_grad/Shape_2&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;gradients/Log_grad/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/clip_by_value_grad/zeros/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/clip_by_value_grad/zeros&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/clip_by_value_grad/Shape_2&quot;\\n  input: &quot;gradients/clip_by_value_grad/zeros/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/clip_by_value_grad/GreaterEqual&quot;\\n  op: &quot;GreaterEqual&quot;\\n  input: &quot;clip_by_value/Minimum&quot;\\n  input: &quot;clip_by_value/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/clip_by_value_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/clip_by_value_grad/Shape&quot;\\n  input: &quot;gradients/clip_by_value_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/clip_by_value_grad/Select&quot;\\n  op: &quot;Select&quot;\\n  input: &quot;gradients/clip_by_value_grad/GreaterEqual&quot;\\n  input: &quot;gradients/Log_grad/mul&quot;\\n  input: &quot;gradients/clip_by_value_grad/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/clip_by_value_grad/LogicalNot&quot;\\n  op: &quot;LogicalNot&quot;\\n  input: &quot;gradients/clip_by_value_grad/GreaterEqual&quot;\\n}\\nnode {\\n  name: &quot;gradients/clip_by_value_grad/Select_1&quot;\\n  op: &quot;Select&quot;\\n  input: &quot;gradients/clip_by_value_grad/LogicalNot&quot;\\n  input: &quot;gradients/Log_grad/mul&quot;\\n  input: &quot;gradients/clip_by_value_grad/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/clip_by_value_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/clip_by_value_grad/Select&quot;\\n  input: &quot;gradients/clip_by_value_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/clip_by_value_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/clip_by_value_grad/Sum&quot;\\n  input: &quot;gradients/clip_by_value_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/clip_by_value_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/clip_by_value_grad/Select_1&quot;\\n  input: &quot;gradients/clip_by_value_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/clip_by_value_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/clip_by_value_grad/Sum_1&quot;\\n  input: &quot;gradients/clip_by_value_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/clip_by_value_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/clip_by_value_grad/Reshape&quot;\\n  input: &quot;^gradients/clip_by_value_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/clip_by_value_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/clip_by_value_grad/Reshape&quot;\\n  input: &quot;^gradients/clip_by_value_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/clip_by_value_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/clip_by_value_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/clip_by_value_grad/Reshape_1&quot;\\n  input: &quot;^gradients/clip_by_value_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/clip_by_value_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/clip_by_value/Minimum_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/clip_by_value/Minimum_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/clip_by_value/Minimum_grad/Shape_2&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;gradients/clip_by_value_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/clip_by_value/Minimum_grad/zeros/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/clip_by_value/Minimum_grad/zeros&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/clip_by_value/Minimum_grad/Shape_2&quot;\\n  input: &quot;gradients/clip_by_value/Minimum_grad/zeros/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/clip_by_value/Minimum_grad/LessEqual&quot;\\n  op: &quot;LessEqual&quot;\\n  input: &quot;Reshape&quot;\\n  input: &quot;clip_by_value/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/clip_by_value/Minimum_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/clip_by_value/Minimum_grad/Shape&quot;\\n  input: &quot;gradients/clip_by_value/Minimum_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/clip_by_value/Minimum_grad/Select&quot;\\n  op: &quot;Select&quot;\\n  input: &quot;gradients/clip_by_value/Minimum_grad/LessEqual&quot;\\n  input: &quot;gradients/clip_by_value_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/clip_by_value/Minimum_grad/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/clip_by_value/Minimum_grad/LogicalNot&quot;\\n  op: &quot;LogicalNot&quot;\\n  input: &quot;gradients/clip_by_value/Minimum_grad/LessEqual&quot;\\n}\\nnode {\\n  name: &quot;gradients/clip_by_value/Minimum_grad/Select_1&quot;\\n  op: &quot;Select&quot;\\n  input: &quot;gradients/clip_by_value/Minimum_grad/LogicalNot&quot;\\n  input: &quot;gradients/clip_by_value_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/clip_by_value/Minimum_grad/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/clip_by_value/Minimum_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/clip_by_value/Minimum_grad/Select&quot;\\n  input: &quot;gradients/clip_by_value/Minimum_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/clip_by_value/Minimum_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/clip_by_value/Minimum_grad/Sum&quot;\\n  input: &quot;gradients/clip_by_value/Minimum_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/clip_by_value/Minimum_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/clip_by_value/Minimum_grad/Select_1&quot;\\n  input: &quot;gradients/clip_by_value/Minimum_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/clip_by_value/Minimum_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/clip_by_value/Minimum_grad/Sum_1&quot;\\n  input: &quot;gradients/clip_by_value/Minimum_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/clip_by_value/Minimum_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/clip_by_value/Minimum_grad/Reshape&quot;\\n  input: &quot;^gradients/clip_by_value/Minimum_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/clip_by_value/Minimum_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/clip_by_value/Minimum_grad/Reshape&quot;\\n  input: &quot;^gradients/clip_by_value/Minimum_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/clip_by_value/Minimum_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/clip_by_value/Minimum_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/clip_by_value/Minimum_grad/Reshape_1&quot;\\n  input: &quot;^gradients/clip_by_value/Minimum_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/clip_by_value/Minimum_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Reshape_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;concat&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Reshape_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/clip_by_value/Minimum_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Reshape_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;concat/axis&quot;\\n  input: &quot;gradients/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/Softmax&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Decoder/Softmax&quot;\\n  input: &quot;Decoder/Softmax_1&quot;\\n  input: &quot;Decoder/Softmax_2&quot;\\n  input: &quot;Decoder/Softmax_3&quot;\\n  input: &quot;Softmax&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 5\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/concat_grad/mod&quot;\\n  input: &quot;gradients/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/concat_grad/ShapeN:1&quot;\\n  input: &quot;gradients/concat_grad/ShapeN:2&quot;\\n  input: &quot;gradients/concat_grad/ShapeN:3&quot;\\n  input: &quot;gradients/concat_grad/ShapeN:4&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 5\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Reshape_grad/Reshape&quot;\\n  input: &quot;gradients/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Reshape_grad/Reshape&quot;\\n  input: &quot;gradients/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/concat_grad/Slice_2&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Reshape_grad/Reshape&quot;\\n  input: &quot;gradients/concat_grad/ConcatOffset:2&quot;\\n  input: &quot;gradients/concat_grad/ShapeN:2&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/concat_grad/Slice_3&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Reshape_grad/Reshape&quot;\\n  input: &quot;gradients/concat_grad/ConcatOffset:3&quot;\\n  input: &quot;gradients/concat_grad/ShapeN:3&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/concat_grad/Slice_4&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Reshape_grad/Reshape&quot;\\n  input: &quot;gradients/concat_grad/ConcatOffset:4&quot;\\n  input: &quot;gradients/concat_grad/ShapeN:4&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/concat_grad/Slice&quot;\\n  input: &quot;^gradients/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/concat_grad/Slice_2&quot;\\n  input: &quot;^gradients/concat_grad/Slice_3&quot;\\n  input: &quot;^gradients/concat_grad/Slice_4&quot;\\n}\\nnode {\\n  name: &quot;gradients/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/concat_grad/Slice&quot;\\n  input: &quot;^gradients/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/concat_grad/tuple/control_dependency_2&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/concat_grad/Slice_2&quot;\\n  input: &quot;^gradients/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/concat_grad/Slice_2&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/concat_grad/tuple/control_dependency_3&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/concat_grad/Slice_3&quot;\\n  input: &quot;^gradients/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/concat_grad/Slice_3&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/concat_grad/tuple/control_dependency_4&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/concat_grad/Slice_4&quot;\\n  input: &quot;^gradients/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/concat_grad/Slice_4&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Softmax_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/concat_grad/tuple/control_dependency_4&quot;\\n  input: &quot;Softmax&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Softmax_grad/Sum/reduction_indices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Softmax_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Softmax_grad/mul&quot;\\n  input: &quot;gradients/Softmax_grad/Sum/reduction_indices&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Softmax_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Softmax_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Softmax_grad/Sum&quot;\\n  input: &quot;gradients/Softmax_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Softmax_grad/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;gradients/concat_grad/tuple/control_dependency_4&quot;\\n  input: &quot;gradients/Softmax_grad/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Softmax_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Softmax_grad/sub&quot;\\n  input: &quot;Softmax&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/add_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;MatMul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/add_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 12\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/add_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/add_grad/Shape&quot;\\n  input: &quot;gradients/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/add_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Softmax_grad/mul_1&quot;\\n  input: &quot;gradients/add_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/add_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/add_grad/Sum&quot;\\n  input: &quot;gradients/add_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/add_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Softmax_grad/mul_1&quot;\\n  input: &quot;gradients/add_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/add_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/add_grad/Sum_1&quot;\\n  input: &quot;gradients/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/add_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/add_grad/Reshape&quot;\\n  input: &quot;^gradients/add_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/add_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/add_grad/Reshape&quot;\\n  input: &quot;^gradients/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/add_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/add_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/add_grad/Reshape_1&quot;\\n  input: &quot;^gradients/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/add_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/add_grad/tuple/control_dependency&quot;\\n  input: &quot;Variable/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attn_output_projection/BiasAdd&quot;\\n  input: &quot;gradients/add_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/MatMul_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat/axis&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attention/Reshape_3&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/mod&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Reshape_3_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attention/Sum_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Reshape_3_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Reshape_3_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attention/mul_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_1_grad/Size&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 4\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_1_grad/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attention/Sum_1/reduction_indices&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_1_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_1_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_1_grad/add&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_1_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_1_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_1_grad/range/start&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_1_grad/range/delta&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_1_grad/range&quot;\\n  op: &quot;Range&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_1_grad/range/start&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_1_grad/Size&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_1_grad/range/delta&quot;\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_1_grad/Fill/value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_1_grad/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_1_grad/Shape_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_1_grad/Fill/value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_1_grad/DynamicStitch&quot;\\n  op: &quot;DynamicStitch&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_1_grad/range&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_1_grad/mod&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_1_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_1_grad/Fill&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_1_grad/Maximum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_1_grad/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_1_grad/DynamicStitch&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_1_grad/Maximum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_1_grad/floordiv&quot;\\n  op: &quot;FloorDiv&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_1_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_1_grad/Maximum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Reshape_3_grad/Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_1_grad/DynamicStitch&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_1_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_1_grad/Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_1_grad/floordiv&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attention/Reshape_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_1_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_1_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_1_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_1_grad/Tile&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_1_grad/mul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_1_grad/Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_1_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attention/Reshape_2&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_1_grad/Tile&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_1_grad/mul_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_1_grad/Sum_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/attention/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/attention/mul_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/attention/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/attention/mul_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_1_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/attention/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/attention/mul_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Reshape_2_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attention/Softmax&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Reshape_2_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_1_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Reshape_2_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Softmax_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Reshape_2_grad/Reshape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attention/Softmax&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Softmax_grad/Sum/reduction_indices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Softmax_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Softmax_grad/mul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Softmax_grad/Sum/reduction_indices&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Softmax_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Softmax_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Softmax_grad/Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Softmax_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Softmax_grad/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Reshape_2_grad/Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Softmax_grad/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Softmax_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Softmax_grad/sub&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attention/Softmax&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attention/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_grad/Size&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 4\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_grad/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attention/Sum/reduction_indices&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_grad/add&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_grad/range/start&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_grad/range/delta&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_grad/range&quot;\\n  op: &quot;Range&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_grad/range/start&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_grad/Size&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_grad/range/delta&quot;\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_grad/Fill/value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_grad/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_grad/Shape_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_grad/Fill/value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_grad/DynamicStitch&quot;\\n  op: &quot;DynamicStitch&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_grad/range&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_grad/mod&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_grad/Fill&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_grad/Maximum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_grad/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_grad/DynamicStitch&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_grad/Maximum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_grad/floordiv&quot;\\n  op: &quot;FloorDiv&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_grad/Maximum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Softmax_grad/mul_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_grad/DynamicStitch&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_grad/Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_grad/floordiv&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_grad/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 128\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attention/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_grad/Tile&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attention/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_grad/mul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_grad/Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_v/read&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Sum_grad/Tile&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_grad/mul_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_grad/Sum_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/attention/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/attention/mul_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/attention/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/attention/mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/attention/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/attention/mul_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Tanh_grad/TanhGrad&quot;\\n  op: &quot;TanhGrad&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attention/Tanh&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/add_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attention/Conv2D&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/add_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attention/Reshape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/add_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/add_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/add_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Tanh_grad/TanhGrad&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/add_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/add_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/add_grad/Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/add_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/add_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Tanh_grad/TanhGrad&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/add_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/add_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/add_grad/Sum_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/add_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/attention/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/attention/add_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/add_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/attention/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/attention/add_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/add_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/add_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/attention/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/attention/add_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Conv2D_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n  op: &quot;Conv2DBackpropInput&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Conv2D_grad/Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_w/read&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/add_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;SAME&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Conv2D_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n  op: &quot;Conv2DBackpropFilter&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attention/Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Conv2D_grad/Shape_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/add_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;SAME&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Conv2D_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Conv2D_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/attention/Conv2D_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Conv2D_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/attention/Conv2D_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Reshape_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attention/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Reshape_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/add_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Reshape_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_1_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Conv2D_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/attention/mul_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Reshape_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Reshape_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/AddN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Reshape_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Reshape_1_grad/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/attention/Reshape_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/attention/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Reshape_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/attention/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/attention/Reshape_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/attention/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/attention/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/Reshape_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/Reshape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/Reshape_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Reshape_grad/Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/Reshape_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/attention/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/attention/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/concat&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/attention/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/attention/attention/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/attention/attention/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/attention/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/attention/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/attention/attention/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/attention/attention/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/attention/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/attention/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/attention/attention/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/attention/attention/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/Reshape_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/concat_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/Reshape_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/Reshape_grad/Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/Reshape_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/concat/axis&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/concat_grad/mod&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/attention/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/attention/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/concat_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/concat_1_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 3\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/concat_1_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/concat_1/axis&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/concat_1_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/concat_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/Slice&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/concat_1_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/Slice&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/ExpandDims&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/concat_1_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/concat_1_grad/mod&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/concat_1_grad/ShapeN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/concat_1_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/concat_1_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/Reshape_1_grad/Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/concat_1_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/concat_1_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/concat_1_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/Reshape_1_grad/Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/concat_1_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/concat_1_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/concat_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/concat_1_grad/Slice&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/concat_1_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/concat_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/concat_1_grad/Slice&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/concat_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_2/concat_1_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/concat_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/concat_1_grad/Slice_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/concat_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_2/concat_1_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_1&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/concat_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/Tanh_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/Sigmoid_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/AddN_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/Sigmoid_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/mul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/Tanh_1&quot;\\n  input: &quot;gradients/AddN_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/mul_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/Sum_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Slice_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 3\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Slice_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/Slice&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Slice_grad/stack/1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Slice_grad/stack&quot;\\n  op: &quot;Pack&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Slice_grad/Rank&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Slice_grad/stack/1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;axis&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Slice_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/Slice/begin&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Slice_grad/stack&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Slice_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Slice_grad/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Slice_grad/Shape_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Slice_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Slice_grad/sub_1&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Slice_grad/sub&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/Slice/begin&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Slice_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Slice_grad/sub_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Slice_grad/stack&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Slice_grad/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Slice_grad/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Slice_grad/Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Slice_grad/Reshape_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Slice_grad/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Slice_grad/Pad&quot;\\n  op: &quot;Pad&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/concat_1_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Slice_grad/concat&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tpaddings&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/ExpandDims_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attn_output_projection/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/ExpandDims_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/concat_1_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/ExpandDims_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/Tanh_1_grad/TanhGrad&quot;\\n  op: &quot;TanhGrad&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/Tanh_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/Sigmoid_2_grad/SigmoidGrad&quot;\\n  op: &quot;SigmoidGrad&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/Sigmoid_2&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_2&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/Tanh_1_grad/TanhGrad&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/AddN_2&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/AddN_2&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/Sum_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/Sigmoid&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/tuple/control_dependency&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/Sigmoid&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/mul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/mul_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/Sum_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/Sigmoid_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/tuple/control_dependency_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/mul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/Sigmoid_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/mul_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/Sum_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/Sigmoid_grad/SigmoidGrad&quot;\\n  op: &quot;SigmoidGrad&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/Sigmoid&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/Sigmoid_1_grad/SigmoidGrad&quot;\\n  op: &quot;SigmoidGrad&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/Sigmoid_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/Tanh_grad/TanhGrad&quot;\\n  op: &quot;TanhGrad&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/Tanh&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/split:2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/Sigmoid_grad/SigmoidGrad&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/Sigmoid_grad/SigmoidGrad&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/Sum_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/split_grad/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/Sigmoid_1_grad/SigmoidGrad&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/Tanh_grad/TanhGrad&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/Sigmoid_2_grad/SigmoidGrad&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/split/split_dim&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 4\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/split_grad/concat&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/split_grad/concat&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/split_grad/concat&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/split_grad/concat&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat/axis&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/BiasAdd&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/mod&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attention_cell_wrapper/concat&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention_cell_wrapper/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention_cell_wrapper/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/attention_cell_wrapper/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention_cell_wrapper/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/attention_cell_wrapper/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_3/attention_cell_wrapper/concat/axis&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/one_hot_3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Decoder/one_hot_3&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/Reshape_3&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/mod&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention_cell_wrapper/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention_cell_wrapper/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/Softmax_3_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/concat_grad/tuple/control_dependency_3&quot;\\n  input: &quot;Decoder/Softmax_3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/Softmax_3_grad/Sum/reduction_indices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/Softmax_3_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/Softmax_3_grad/mul&quot;\\n  input: &quot;gradients/Decoder/Softmax_3_grad/Sum/reduction_indices&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/Softmax_3_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/Softmax_3_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/Softmax_3_grad/Sum&quot;\\n  input: &quot;gradients/Decoder/Softmax_3_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/Softmax_3_grad/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;gradients/concat_grad/tuple/control_dependency_3&quot;\\n  input: &quot;gradients/Decoder/Softmax_3_grad/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/Softmax_3_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Decoder/Softmax_3_grad/sub&quot;\\n  input: &quot;Decoder/Softmax_3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/add_3_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/MatMul_3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/add_3_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 12\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/add_3_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Decoder/add_3_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/add_3_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/add_3_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/Softmax_3_grad/mul_1&quot;\\n  input: &quot;gradients/Decoder/add_3_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/add_3_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/add_3_grad/Sum&quot;\\n  input: &quot;gradients/Decoder/add_3_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/add_3_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/Softmax_3_grad/mul_1&quot;\\n  input: &quot;gradients/Decoder/add_3_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/add_3_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/add_3_grad/Sum_1&quot;\\n  input: &quot;gradients/Decoder/add_3_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/add_3_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/add_3_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/add_3_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/add_3_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/add_3_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/add_3_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/add_3_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/add_3_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/add_3_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Decoder/add_3_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/add_3_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/MatMul_3_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Decoder/add_3_grad/tuple/control_dependency&quot;\\n  input: &quot;Variable/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/MatMul_3_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attn_output_projection/BiasAdd&quot;\\n  input: &quot;gradients/Decoder/add_3_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/MatMul_3_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/MatMul_3_grad/MatMul&quot;\\n  input: &quot;^gradients/Decoder/MatMul_3_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/MatMul_3_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/MatMul_3_grad/MatMul&quot;\\n  input: &quot;^gradients/Decoder/MatMul_3_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/MatMul_3_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/MatMul_3_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/MatMul_3_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Decoder/MatMul_3_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/MatMul_3_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_3&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/ExpandDims_grad/Reshape&quot;\\n  input: &quot;gradients/Decoder/MatMul_3_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_2/ExpandDims_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/AddN_3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/AddN_3&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/AddN_3&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_2/ExpandDims_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat/axis&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/Reshape_3&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/mod&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_4&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Reshape_3_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/Sum_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Reshape_3_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/AddN_4&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Reshape_3_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/mul_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_1_grad/Size&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 4\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_1_grad/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/Sum_1/reduction_indices&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_1_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_1_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_1_grad/add&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_1_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_1_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_1_grad/range/start&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_1_grad/range/delta&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_1_grad/range&quot;\\n  op: &quot;Range&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_1_grad/range/start&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_1_grad/Size&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_1_grad/range/delta&quot;\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_1_grad/Fill/value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_1_grad/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_1_grad/Shape_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_1_grad/Fill/value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_1_grad/DynamicStitch&quot;\\n  op: &quot;DynamicStitch&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_1_grad/range&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_1_grad/mod&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_1_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_1_grad/Fill&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_1_grad/Maximum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_1_grad/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_1_grad/DynamicStitch&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_1_grad/Maximum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_1_grad/floordiv&quot;\\n  op: &quot;FloorDiv&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_1_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_1_grad/Maximum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Reshape_3_grad/Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_1_grad/DynamicStitch&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_1_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_1_grad/Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_1_grad/floordiv&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/Reshape_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_1_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_1_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_1_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_1_grad/Tile&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_1_grad/mul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_1_grad/Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_1_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/Reshape_2&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_1_grad/Tile&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_1_grad/mul_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_1_grad/Sum_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/attention/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/attention/mul_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/attention/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_2/attention/mul_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_1_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/attention/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_2/attention/mul_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Reshape_2_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/Softmax&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Reshape_2_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_1_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Reshape_2_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Softmax_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Reshape_2_grad/Reshape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/Softmax&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Softmax_grad/Sum/reduction_indices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Softmax_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Softmax_grad/mul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Softmax_grad/Sum/reduction_indices&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Softmax_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Softmax_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Softmax_grad/Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Softmax_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Softmax_grad/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Reshape_2_grad/Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Softmax_grad/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Softmax_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Softmax_grad/sub&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/Softmax&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_grad/Size&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 4\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_grad/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/Sum/reduction_indices&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_grad/add&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_grad/range/start&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_grad/range/delta&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_grad/range&quot;\\n  op: &quot;Range&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_grad/range/start&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_grad/Size&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_grad/range/delta&quot;\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_grad/Fill/value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_grad/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_grad/Shape_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_grad/Fill/value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_grad/DynamicStitch&quot;\\n  op: &quot;DynamicStitch&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_grad/range&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_grad/mod&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_grad/Fill&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_grad/Maximum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_grad/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_grad/DynamicStitch&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_grad/Maximum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_grad/floordiv&quot;\\n  op: &quot;FloorDiv&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_grad/Maximum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Softmax_grad/mul_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_grad/DynamicStitch&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_grad/Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_grad/floordiv&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_grad/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 128\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_grad/Tile&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_grad/mul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_grad/Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_v/read&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Sum_grad/Tile&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_grad/mul_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_grad/Sum_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/attention/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/attention/mul_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/attention/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_2/attention/mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/attention/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_2/attention/mul_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Tanh_grad/TanhGrad&quot;\\n  op: &quot;TanhGrad&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/Tanh&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/add_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/Conv2D&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/add_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/Reshape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/add_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/add_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/add_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Tanh_grad/TanhGrad&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/add_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/add_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/add_grad/Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/add_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/add_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Tanh_grad/TanhGrad&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/add_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/add_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/add_grad/Sum_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/add_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/attention/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/attention/add_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/add_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/attention/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_2/attention/add_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/add_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/add_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/attention/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_2/attention/add_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Conv2D_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n  op: &quot;Conv2DBackpropInput&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Conv2D_grad/Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_w/read&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/add_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;SAME&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Conv2D_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n  op: &quot;Conv2DBackpropFilter&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Conv2D_grad/Shape_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/add_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;SAME&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Conv2D_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Conv2D_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/attention/Conv2D_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_2/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Conv2D_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/attention/Conv2D_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_2/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Reshape_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Reshape_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/add_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Reshape_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_5&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_1_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Conv2D_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_2/attention/mul_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Reshape_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Reshape_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/AddN_5&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Reshape_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Reshape_1_grad/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/attention/Reshape_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/attention/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Reshape_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/attention/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_2/attention/Reshape_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/attention/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_2/attention/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_6&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Slice_grad/Pad&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Reshape_grad/Reshape&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_2/attention/Slice_grad/Pad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/Reshape_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/Reshape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/Reshape_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/AddN_6&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/Reshape_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/attention/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/attention/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/concat&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/attention/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/attention/attention/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/attention/attention/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/attention/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/attention/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/attention/attention/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_2/attention/attention/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/attention/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/attention/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/attention/attention/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_2/attention/attention/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/Reshape_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/concat_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/Reshape_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/Reshape_grad/Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/Reshape_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/concat/axis&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/concat_grad/mod&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/attention/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/attention/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/concat_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_2/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_2/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/concat_1_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 3\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/concat_1_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/concat_1/axis&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/concat_1_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/concat_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/Slice&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/concat_1_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/Slice&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/ExpandDims&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/concat_1_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/concat_1_grad/mod&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/concat_1_grad/ShapeN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/concat_1_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/concat_1_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/Reshape_1_grad/Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/concat_1_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/concat_1_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/concat_1_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/Reshape_1_grad/Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/concat_1_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/concat_1_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/concat_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/concat_1_grad/Slice&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/concat_1_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/concat_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/concat_1_grad/Slice&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/concat_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_1/concat_1_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/concat_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/concat_1_grad/Slice_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/concat_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_1/concat_1_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_7&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/concat_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 3\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/Tanh_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/Sigmoid_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/AddN_7&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/Sigmoid_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/mul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/Tanh_1&quot;\\n  input: &quot;gradients/AddN_7&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/mul_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/Sum_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Slice_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 3\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Slice_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/Slice&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Slice_grad/stack/1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Slice_grad/stack&quot;\\n  op: &quot;Pack&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Slice_grad/Rank&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Slice_grad/stack/1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;axis&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Slice_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/Slice/begin&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Slice_grad/stack&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Slice_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Slice_grad/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Slice_grad/Shape_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Slice_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Slice_grad/sub_1&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Slice_grad/sub&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/Slice/begin&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Slice_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Slice_grad/sub_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Slice_grad/stack&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Slice_grad/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Slice_grad/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Slice_grad/Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Slice_grad/Reshape_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Slice_grad/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Slice_grad/Pad&quot;\\n  op: &quot;Pad&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/concat_1_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Slice_grad/concat&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tpaddings&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/ExpandDims_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attn_output_projection/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/ExpandDims_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/concat_1_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/ExpandDims_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/Tanh_1_grad/TanhGrad&quot;\\n  op: &quot;TanhGrad&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/Tanh_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/Sigmoid_2_grad/SigmoidGrad&quot;\\n  op: &quot;SigmoidGrad&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/Sigmoid_2&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_8&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/Tanh_1_grad/TanhGrad&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 3\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/AddN_8&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/AddN_8&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/Sum_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/Sigmoid&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/tuple/control_dependency&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/Sigmoid&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/mul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/mul_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/Sum_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/Sigmoid_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/tuple/control_dependency_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/mul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/Sigmoid_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/mul_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/Sum_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/Sigmoid_grad/SigmoidGrad&quot;\\n  op: &quot;SigmoidGrad&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/Sigmoid&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/Sigmoid_1_grad/SigmoidGrad&quot;\\n  op: &quot;SigmoidGrad&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/Sigmoid_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/Tanh_grad/TanhGrad&quot;\\n  op: &quot;TanhGrad&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/Tanh&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/split:2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/Sigmoid_grad/SigmoidGrad&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/Sigmoid_grad/SigmoidGrad&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/Sum_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/split_grad/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/Sigmoid_1_grad/SigmoidGrad&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/Tanh_grad/TanhGrad&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/Sigmoid_2_grad/SigmoidGrad&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/split/split_dim&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 4\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/split_grad/concat&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/split_grad/concat&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/split_grad/concat&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/split_grad/concat&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat/axis&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/BiasAdd&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/mod&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_2/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention_cell_wrapper/concat&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention_cell_wrapper/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention_cell_wrapper/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/attention_cell_wrapper/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_2/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention_cell_wrapper/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/attention_cell_wrapper/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_2/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_2/attention_cell_wrapper/concat/axis&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/one_hot_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Decoder/one_hot_2&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/Reshape_3&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/mod&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention_cell_wrapper/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention_cell_wrapper/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/Softmax_2_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/concat_grad/tuple/control_dependency_2&quot;\\n  input: &quot;Decoder/Softmax_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/Softmax_2_grad/Sum/reduction_indices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/Softmax_2_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/Softmax_2_grad/mul&quot;\\n  input: &quot;gradients/Decoder/Softmax_2_grad/Sum/reduction_indices&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/Softmax_2_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/Softmax_2_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/Softmax_2_grad/Sum&quot;\\n  input: &quot;gradients/Decoder/Softmax_2_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/Softmax_2_grad/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;gradients/concat_grad/tuple/control_dependency_2&quot;\\n  input: &quot;gradients/Decoder/Softmax_2_grad/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/Softmax_2_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Decoder/Softmax_2_grad/sub&quot;\\n  input: &quot;Decoder/Softmax_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/add_2_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/MatMul_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/add_2_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 12\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/add_2_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Decoder/add_2_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/add_2_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/add_2_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/Softmax_2_grad/mul_1&quot;\\n  input: &quot;gradients/Decoder/add_2_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/add_2_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/add_2_grad/Sum&quot;\\n  input: &quot;gradients/Decoder/add_2_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/add_2_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/Softmax_2_grad/mul_1&quot;\\n  input: &quot;gradients/Decoder/add_2_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/add_2_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/add_2_grad/Sum_1&quot;\\n  input: &quot;gradients/Decoder/add_2_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/add_2_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/add_2_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/add_2_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/add_2_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/add_2_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/add_2_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/add_2_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/add_2_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/add_2_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Decoder/add_2_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/add_2_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/MatMul_2_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Decoder/add_2_grad/tuple/control_dependency&quot;\\n  input: &quot;Variable/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/MatMul_2_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attn_output_projection/BiasAdd&quot;\\n  input: &quot;gradients/Decoder/add_2_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/MatMul_2_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/MatMul_2_grad/MatMul&quot;\\n  input: &quot;^gradients/Decoder/MatMul_2_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/MatMul_2_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/MatMul_2_grad/MatMul&quot;\\n  input: &quot;^gradients/Decoder/MatMul_2_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/MatMul_2_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/MatMul_2_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/MatMul_2_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Decoder/MatMul_2_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/MatMul_2_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_9&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/ExpandDims_grad/Reshape&quot;\\n  input: &quot;gradients/Decoder/MatMul_2_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_1/ExpandDims_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/AddN_9&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/AddN_9&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/AddN_9&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_1/ExpandDims_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat/axis&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/Reshape_3&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/mod&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_10&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Reshape_3_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/Sum_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Reshape_3_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/AddN_10&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Reshape_3_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/mul_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_1_grad/Size&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 4\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_1_grad/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/Sum_1/reduction_indices&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_1_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_1_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_1_grad/add&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_1_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_1_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_1_grad/range/start&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_1_grad/range/delta&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_1_grad/range&quot;\\n  op: &quot;Range&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_1_grad/range/start&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_1_grad/Size&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_1_grad/range/delta&quot;\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_1_grad/Fill/value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_1_grad/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_1_grad/Shape_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_1_grad/Fill/value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_1_grad/DynamicStitch&quot;\\n  op: &quot;DynamicStitch&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_1_grad/range&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_1_grad/mod&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_1_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_1_grad/Fill&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_1_grad/Maximum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_1_grad/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_1_grad/DynamicStitch&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_1_grad/Maximum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_1_grad/floordiv&quot;\\n  op: &quot;FloorDiv&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_1_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_1_grad/Maximum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Reshape_3_grad/Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_1_grad/DynamicStitch&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_1_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_1_grad/Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_1_grad/floordiv&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/Reshape_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_1_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_1_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_1_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_1_grad/Tile&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_1_grad/mul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_1_grad/Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_1_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/Reshape_2&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_1_grad/Tile&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_1_grad/mul_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_1_grad/Sum_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/attention/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/attention/mul_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/attention/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_1/attention/mul_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_1_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/attention/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_1/attention/mul_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Reshape_2_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/Softmax&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Reshape_2_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_1_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Reshape_2_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Softmax_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Reshape_2_grad/Reshape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/Softmax&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Softmax_grad/Sum/reduction_indices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Softmax_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Softmax_grad/mul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Softmax_grad/Sum/reduction_indices&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Softmax_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Softmax_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Softmax_grad/Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Softmax_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Softmax_grad/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Reshape_2_grad/Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Softmax_grad/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Softmax_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Softmax_grad/sub&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/Softmax&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_grad/Size&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 4\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_grad/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/Sum/reduction_indices&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_grad/add&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_grad/range/start&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_grad/range/delta&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_grad/range&quot;\\n  op: &quot;Range&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_grad/range/start&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_grad/Size&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_grad/range/delta&quot;\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_grad/Fill/value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_grad/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_grad/Shape_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_grad/Fill/value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_grad/DynamicStitch&quot;\\n  op: &quot;DynamicStitch&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_grad/range&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_grad/mod&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_grad/Fill&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_grad/Maximum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_grad/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_grad/DynamicStitch&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_grad/Maximum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_grad/floordiv&quot;\\n  op: &quot;FloorDiv&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_grad/Maximum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Softmax_grad/mul_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_grad/DynamicStitch&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_grad/Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_grad/floordiv&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_grad/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 128\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_grad/Tile&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_grad/mul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_grad/Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_v/read&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Sum_grad/Tile&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_grad/mul_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_grad/Sum_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/attention/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/attention/mul_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/attention/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_1/attention/mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/attention/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_1/attention/mul_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Tanh_grad/TanhGrad&quot;\\n  op: &quot;TanhGrad&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/Tanh&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/add_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/Conv2D&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/add_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/Reshape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/add_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/add_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/add_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Tanh_grad/TanhGrad&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/add_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/add_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/add_grad/Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/add_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/add_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Tanh_grad/TanhGrad&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/add_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/add_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/add_grad/Sum_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/add_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/attention/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/attention/add_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/add_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/attention/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_1/attention/add_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/add_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/add_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/attention/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_1/attention/add_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Conv2D_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n  op: &quot;Conv2DBackpropInput&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Conv2D_grad/Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_w/read&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/add_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;SAME&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Conv2D_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n  op: &quot;Conv2DBackpropFilter&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Conv2D_grad/Shape_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/add_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;SAME&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Conv2D_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Conv2D_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/attention/Conv2D_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_1/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Conv2D_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/attention/Conv2D_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_1/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Reshape_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Reshape_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/add_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Reshape_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_11&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_1_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Conv2D_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_1/attention/mul_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Reshape_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Reshape_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/AddN_11&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Reshape_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Reshape_1_grad/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/attention/Reshape_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/attention/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Reshape_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/attention/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_1/attention/Reshape_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/attention/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_1/attention/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_12&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Slice_grad/Pad&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Reshape_grad/Reshape&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_1/attention/Slice_grad/Pad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/Reshape_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/Reshape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/Reshape_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/AddN_12&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/Reshape_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/attention/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/attention/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/concat&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/attention/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/attention/attention/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/attention/attention/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/attention/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/attention/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/attention/attention/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_1/attention/attention/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/attention/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/attention/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/attention/attention/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_1/attention/attention/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/Reshape_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/concat_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/Reshape_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/Reshape_grad/Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/Reshape_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/concat/axis&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/concat_grad/mod&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/attention/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/attention/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/concat_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_1/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_1/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/concat_1_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 3\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/concat_1_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/concat_1/axis&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/concat_1_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/concat_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/Slice&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/concat_1_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/Slice&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/ExpandDims&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/concat_1_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/concat_1_grad/mod&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/concat_1_grad/ShapeN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/concat_1_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/concat_1_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/Reshape_1_grad/Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/concat_1_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/concat_1_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/concat_1_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/Reshape_1_grad/Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/concat_1_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/concat_1_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/concat_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/concat_1_grad/Slice&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/concat_1_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/concat_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/concat_1_grad/Slice&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/concat_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper/concat_1_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/concat_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/concat_1_grad/Slice_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/concat_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper/concat_1_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_13&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/concat_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 3\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/Tanh_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/Sigmoid_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/AddN_13&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/Sigmoid_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/mul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/Tanh_1&quot;\\n  input: &quot;gradients/AddN_13&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/mul_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/Sum_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Slice_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 3\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Slice_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/Slice&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Slice_grad/stack/1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Slice_grad/stack&quot;\\n  op: &quot;Pack&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Slice_grad/Rank&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Slice_grad/stack/1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;axis&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Slice_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/Slice/begin&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Slice_grad/stack&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Slice_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Slice_grad/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Slice_grad/Shape_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Slice_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Slice_grad/sub_1&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Slice_grad/sub&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/Slice/begin&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Slice_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Slice_grad/sub_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Slice_grad/stack&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Slice_grad/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Slice_grad/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Slice_grad/Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Slice_grad/Reshape_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Slice_grad/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Slice_grad/Pad&quot;\\n  op: &quot;Pad&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/concat_1_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Slice_grad/concat&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tpaddings&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/ExpandDims_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/ExpandDims_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/concat_1_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/ExpandDims_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/Tanh_1_grad/TanhGrad&quot;\\n  op: &quot;TanhGrad&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/Tanh_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/Sigmoid_2_grad/SigmoidGrad&quot;\\n  op: &quot;SigmoidGrad&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/Sigmoid_2&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_14&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/Tanh_1_grad/TanhGrad&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 3\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/AddN_14&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/AddN_14&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/Sum_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/Sigmoid&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/tuple/control_dependency&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/Sigmoid&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/mul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/add_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/mul_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/Sum_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/Sigmoid_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/tuple/control_dependency_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/mul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/Sigmoid_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/mul_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/Sum_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/Sigmoid_grad/SigmoidGrad&quot;\\n  op: &quot;SigmoidGrad&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/Sigmoid&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/Sigmoid_1_grad/SigmoidGrad&quot;\\n  op: &quot;SigmoidGrad&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/Sigmoid_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/Tanh_grad/TanhGrad&quot;\\n  op: &quot;TanhGrad&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/Tanh&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/split:2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/Sigmoid_grad/SigmoidGrad&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/Sigmoid_grad/SigmoidGrad&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/Sum_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/split_grad/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/Sigmoid_1_grad/SigmoidGrad&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/Tanh_grad/TanhGrad&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/Sigmoid_2_grad/SigmoidGrad&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/split/split_dim&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 4\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/split_grad/concat&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/split_grad/concat&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/split_grad/concat&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/split_grad/concat&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat/axis&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/BiasAdd&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/mul_2&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/mod&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_1/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention_cell_wrapper/concat&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention_cell_wrapper/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention_cell_wrapper/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/attention_cell_wrapper/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_1/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention_cell_wrapper/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/attention_cell_wrapper/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_1/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Decoder/attention_cell_wrapper_1/attention_cell_wrapper/concat/axis&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/one_hot_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Decoder/one_hot_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/Reshape_3&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/mod&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention_cell_wrapper/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention_cell_wrapper/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/Softmax_1_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/concat_grad/tuple/control_dependency_1&quot;\\n  input: &quot;Decoder/Softmax_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/Softmax_1_grad/Sum/reduction_indices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/Softmax_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/Softmax_1_grad/mul&quot;\\n  input: &quot;gradients/Decoder/Softmax_1_grad/Sum/reduction_indices&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/Softmax_1_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/Softmax_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/Softmax_1_grad/Sum&quot;\\n  input: &quot;gradients/Decoder/Softmax_1_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/Softmax_1_grad/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;gradients/concat_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/Softmax_1_grad/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/Softmax_1_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Decoder/Softmax_1_grad/sub&quot;\\n  input: &quot;Decoder/Softmax_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/add_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/MatMul_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/add_1_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 12\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/add_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Decoder/add_1_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/add_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/add_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/Softmax_1_grad/mul_1&quot;\\n  input: &quot;gradients/Decoder/add_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/add_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/add_1_grad/Sum&quot;\\n  input: &quot;gradients/Decoder/add_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/add_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/Softmax_1_grad/mul_1&quot;\\n  input: &quot;gradients/Decoder/add_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/add_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/add_1_grad/Sum_1&quot;\\n  input: &quot;gradients/Decoder/add_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/add_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/add_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/add_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/add_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/add_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/add_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/add_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/add_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/add_1_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Decoder/add_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/add_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/MatMul_1_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Decoder/add_1_grad/tuple/control_dependency&quot;\\n  input: &quot;Variable/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/MatMul_1_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/BiasAdd&quot;\\n  input: &quot;gradients/Decoder/add_1_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/MatMul_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/MatMul_1_grad/MatMul&quot;\\n  input: &quot;^gradients/Decoder/MatMul_1_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/MatMul_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/MatMul_1_grad/MatMul&quot;\\n  input: &quot;^gradients/Decoder/MatMul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/MatMul_1_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/MatMul_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/MatMul_1_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Decoder/MatMul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/MatMul_1_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_15&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/ExpandDims_grad/Reshape&quot;\\n  input: &quot;gradients/Decoder/MatMul_1_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper/ExpandDims_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/AddN_15&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attn_output_projection/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/AddN_15&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attn_output_projection/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/AddN_15&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/attn_output_projection/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper/ExpandDims_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attn_output_projection/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/attn_output_projection/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attn_output_projection/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attn_output_projection/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_16&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attn_output_projection/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 4\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat/axis&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/mul_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/mul_2&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/Reshape_3&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/mod&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_17&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 4\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_18&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Reshape_3_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/Sum_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Reshape_3_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/AddN_18&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Reshape_3_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/mul_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_1_grad/Size&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 4\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_1_grad/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/Sum_1/reduction_indices&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_1_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_1_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_1_grad/add&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_1_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_1_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_1_grad/range/start&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_1_grad/range/delta&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_1_grad/range&quot;\\n  op: &quot;Range&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_1_grad/range/start&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_1_grad/Size&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_1_grad/range/delta&quot;\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_1_grad/Fill/value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_1_grad/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_1_grad/Shape_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_1_grad/Fill/value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_1_grad/DynamicStitch&quot;\\n  op: &quot;DynamicStitch&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_1_grad/range&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_1_grad/mod&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_1_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_1_grad/Fill&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_1_grad/Maximum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_1_grad/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_1_grad/DynamicStitch&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_1_grad/Maximum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_1_grad/floordiv&quot;\\n  op: &quot;FloorDiv&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_1_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_1_grad/Maximum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Reshape_3_grad/Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_1_grad/DynamicStitch&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_1_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_1_grad/Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_1_grad/floordiv&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/Reshape_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_1_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_1_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_1_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_1_grad/Tile&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_1_grad/mul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_1_grad/Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_1_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/Reshape_2&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_1_grad/Tile&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_1_grad/mul_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_1_grad/Sum_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/attention/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/attention/mul_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/attention/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper/attention/mul_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_1_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/attention/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper/attention/mul_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Reshape_2_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/Softmax&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Reshape_2_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_1_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Reshape_2_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Softmax_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Reshape_2_grad/Reshape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/Softmax&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Softmax_grad/Sum/reduction_indices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Softmax_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Softmax_grad/mul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Softmax_grad/Sum/reduction_indices&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Softmax_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Softmax_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Softmax_grad/Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Softmax_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Softmax_grad/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Reshape_2_grad/Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Softmax_grad/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Softmax_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Softmax_grad/sub&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/Softmax&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_grad/Size&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 4\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_grad/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/Sum/reduction_indices&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_grad/add&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_grad/range/start&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_grad/range/delta&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_grad/range&quot;\\n  op: &quot;Range&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_grad/range/start&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_grad/Size&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_grad/range/delta&quot;\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_grad/Fill/value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_grad/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_grad/Shape_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_grad/Fill/value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_grad/DynamicStitch&quot;\\n  op: &quot;DynamicStitch&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_grad/range&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_grad/mod&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_grad/Fill&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_grad/Maximum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_grad/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_grad/DynamicStitch&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_grad/Maximum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_grad/floordiv&quot;\\n  op: &quot;FloorDiv&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_grad/Maximum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Softmax_grad/mul_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_grad/DynamicStitch&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_grad/Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_grad/floordiv&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_grad/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 128\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_grad/Tile&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_grad/mul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_grad/Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_v/read&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Sum_grad/Tile&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_grad/mul_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_grad/Sum_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/attention/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/attention/mul_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/attention/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper/attention/mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/attention/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper/attention/mul_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_19&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/mul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/mul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/mul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 4\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/attention/mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Tanh_grad/TanhGrad&quot;\\n  op: &quot;TanhGrad&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/Tanh&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/add_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/Conv2D&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/add_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/Reshape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/add_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/add_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/add_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Tanh_grad/TanhGrad&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/add_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/add_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/add_grad/Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/add_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/add_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Tanh_grad/TanhGrad&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/add_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/add_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/add_grad/Sum_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/add_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/attention/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/attention/add_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/add_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/attention/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper/attention/add_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/add_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/add_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/attention/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper/attention/add_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Conv2D_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n  op: &quot;Conv2DBackpropInput&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Conv2D_grad/Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_w/read&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/add_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;SAME&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Conv2D_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n  op: &quot;Conv2DBackpropFilter&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Conv2D_grad/Shape_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/add_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;SAME&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Conv2D_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Conv2D_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/attention/Conv2D_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Conv2D_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/attention/Conv2D_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Reshape_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Reshape_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/add_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Reshape_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_20&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/mul_1_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Conv2D_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper/attention/mul_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Reshape_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/Reshape_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/AddN_20&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Reshape_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_21&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/Conv2D_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/Conv2D_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/Conv2D_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Conv2D_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 4\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Reshape_1_grad/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/attention/Reshape_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/attention/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Reshape_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/attention/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper/attention/Reshape_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/attention/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper/attention/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_22&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Slice_grad/Pad&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/Reshape_grad/Reshape&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper/attention/Slice_grad/Pad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/Reshape_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/Reshape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/Reshape_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/AddN_22&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/Reshape_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/attention/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/attention/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/concat&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/attention/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/attention/attention/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/attention/attention/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/attention/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/attention/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/attention/attention/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper/attention/attention/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention/attention/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/attention/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/attention/attention/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper/attention/attention/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_23&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 4\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/attention/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/Reshape_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/concat_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/Reshape_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/Reshape_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/Reshape_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/concat/axis&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/add_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/mul_2&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/concat_grad/mod&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/attention/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/attention/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_24&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention/attention/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention/attention/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention/attention/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention/attention/MatMul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 4\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/attention/attention/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/concat_1_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 3\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/concat_1_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/concat_1/axis&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/concat_1_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/concat_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/Slice&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/concat_1_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/Slice&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/ExpandDims&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/concat_1_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/concat_1_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/concat_1_grad/ShapeN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/concat_1_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/concat_1_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/Reshape_1_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/concat_1_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/concat_1_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/concat_1_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/Reshape_1_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/concat_1_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/concat_1_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/concat_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/concat_1_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/concat_1_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/concat_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/concat_1_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/concat_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/concat_1_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/concat_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/concat_1_grad/Slice_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/concat_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/concat_1_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_25&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/concat_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 3\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/Tanh_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/Sigmoid_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/AddN_25&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/Sigmoid_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/mul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/Tanh_1&quot;\\n  input: &quot;gradients/AddN_25&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/mul_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/Sum_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Slice_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 3\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Slice_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/Slice&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Slice_grad/stack/1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Slice_grad/stack&quot;\\n  op: &quot;Pack&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Slice_grad/Rank&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Slice_grad/stack/1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;axis&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Slice_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/Slice/begin&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Slice_grad/stack&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Slice_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Slice_grad/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Slice_grad/Shape_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Slice_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Slice_grad/sub_1&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Slice_grad/sub&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/Slice/begin&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Slice_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Slice_grad/sub_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Slice_grad/stack&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Slice_grad/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Slice_grad/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Slice_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Slice_grad/Reshape_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Slice_grad/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Slice_grad/Pad&quot;\\n  op: &quot;Pad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/concat_1_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Slice_grad/concat&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tpaddings&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/ExpandDims_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attn_output_projection/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/ExpandDims_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/concat_1_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/ExpandDims_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/Tanh_1_grad/TanhGrad&quot;\\n  op: &quot;TanhGrad&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/Tanh_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/Sigmoid_2_grad/SigmoidGrad&quot;\\n  op: &quot;SigmoidGrad&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/Sigmoid_2&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_26&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/Tanh_1_grad/TanhGrad&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 3\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/mul_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/AddN_26&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/AddN_26&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/Sum_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/Sigmoid&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/tuple/control_dependency&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/Sigmoid&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/mul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/mul_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/Sum_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/Sigmoid_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/tuple/control_dependency_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/mul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/Sigmoid_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/mul_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/Sum_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/Sigmoid_grad/SigmoidGrad&quot;\\n  op: &quot;SigmoidGrad&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/Sigmoid&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/Sigmoid_1_grad/SigmoidGrad&quot;\\n  op: &quot;SigmoidGrad&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/Sigmoid_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/Tanh_grad/TanhGrad&quot;\\n  op: &quot;TanhGrad&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/Tanh&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/split:2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/Sigmoid_grad/SigmoidGrad&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_grad/Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/Sigmoid_grad/SigmoidGrad&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_grad/Sum_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/split_grad/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/Sigmoid_1_grad/SigmoidGrad&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/Tanh_grad/TanhGrad&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/add_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/Sigmoid_2_grad/SigmoidGrad&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/split/split_dim&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 4\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/split_grad/concat&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/split_grad/concat&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/split_grad/concat&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/split_grad/concat&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_27&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 4\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat/axis&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_2&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/mod&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_28&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 4\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention_cell_wrapper/concat&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention_cell_wrapper/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention_cell_wrapper/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/attention_cell_wrapper/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention_cell_wrapper/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/attention_cell_wrapper/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_29&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 4\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention_cell_wrapper/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention_cell_wrapper/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention_cell_wrapper/concat/axis&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention_cell_wrapper/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention_cell_wrapper/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/one_hot&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention_cell_wrapper/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Decoder/one_hot&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/Reshape_3&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention_cell_wrapper/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention_cell_wrapper/concat_grad/mod&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention_cell_wrapper/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention_cell_wrapper/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention_cell_wrapper/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention_cell_wrapper/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention_cell_wrapper/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention_cell_wrapper/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention_cell_wrapper/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention_cell_wrapper/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention_cell_wrapper/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention_cell_wrapper/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/attention_cell_wrapper/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention_cell_wrapper/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention_cell_wrapper/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/attention_cell_wrapper/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper/attention_cell_wrapper/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/attention_cell_wrapper/attention_cell_wrapper/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/Decoder/attention_cell_wrapper/attention_cell_wrapper/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_30&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_3/attention_cell_wrapper/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_2/attention_cell_wrapper/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper_1/attention_cell_wrapper/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention_cell_wrapper/MatMul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 4\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper_3/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/Softmax_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;Decoder/Softmax&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/Softmax_grad/Sum/reduction_indices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/Softmax_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/Softmax_grad/mul&quot;\\n  input: &quot;gradients/Decoder/Softmax_grad/Sum/reduction_indices&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/Softmax_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/Softmax_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/Softmax_grad/Sum&quot;\\n  input: &quot;gradients/Decoder/Softmax_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/Softmax_grad/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;gradients/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Decoder/Softmax_grad/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/Softmax_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Decoder/Softmax_grad/sub&quot;\\n  input: &quot;Decoder/Softmax&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/add_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Decoder/MatMul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/add_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 12\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/add_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Decoder/add_grad/Shape&quot;\\n  input: &quot;gradients/Decoder/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/add_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/Softmax_grad/mul_1&quot;\\n  input: &quot;gradients/Decoder/add_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/add_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/add_grad/Sum&quot;\\n  input: &quot;gradients/Decoder/add_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/add_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Decoder/Softmax_grad/mul_1&quot;\\n  input: &quot;gradients/Decoder/add_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/add_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Decoder/add_grad/Sum_1&quot;\\n  input: &quot;gradients/Decoder/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/add_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/add_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/add_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Decoder/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/add_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/add_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/add_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Decoder/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/add_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Decoder/add_grad/tuple/control_dependency&quot;\\n  input: &quot;Variable/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attn_output_projection/BiasAdd&quot;\\n  input: &quot;gradients/Decoder/add_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Decoder/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Decoder/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Decoder/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Decoder/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Decoder/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Decoder/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Decoder/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_31&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/add_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/add_3_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/add_2_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/add_1_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/add_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 5\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/add_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_32&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/ExpandDims_grad/Reshape&quot;\\n  input: &quot;gradients/Decoder/MatMul_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/ExpandDims_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/AddN_32&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/AddN_32&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/AddN_32&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/ExpandDims_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_33&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/MatMul_3_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/MatMul_2_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/MatMul_1_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Decoder/MatMul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 5\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/concat&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/concat/axis&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/Reshape_3&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/concat_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_34&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/attention_cell_wrapper/concat_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Reshape_3_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/Sum_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Reshape_3_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/AddN_34&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Reshape_3_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/mul_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_1_grad/Size&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 4\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_1_grad/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/Sum_1/reduction_indices&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_1_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_1_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_1_grad/add&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_1_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_1_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_1_grad/range/start&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_1_grad/range/delta&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_1_grad/range&quot;\\n  op: &quot;Range&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_1_grad/range/start&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_1_grad/Size&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_1_grad/range/delta&quot;\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_1_grad/Fill/value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_1_grad/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_1_grad/Shape_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_1_grad/Fill/value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_1_grad/DynamicStitch&quot;\\n  op: &quot;DynamicStitch&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_1_grad/range&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_1_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_1_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_1_grad/Fill&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_1_grad/Maximum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_1_grad/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_1_grad/DynamicStitch&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_1_grad/Maximum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_1_grad/floordiv&quot;\\n  op: &quot;FloorDiv&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_1_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_1_grad/Maximum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Reshape_3_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_1_grad/DynamicStitch&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_1_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_1_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_1_grad/floordiv&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/Reshape_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_1_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_1_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_1_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_1_grad/Tile&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_1_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_1_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_1_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/Reshape_2&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_1_grad/Tile&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_1_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_1_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/attention/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/attention/mul_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/attention/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/attention/mul_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_1_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/attention/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/attention/mul_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Reshape_2_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/Softmax&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Reshape_2_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_1_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Reshape_2_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Softmax_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Reshape_2_grad/Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/Softmax&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Softmax_grad/Sum/reduction_indices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Softmax_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Softmax_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Softmax_grad/Sum/reduction_indices&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Softmax_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Softmax_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Softmax_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Softmax_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Softmax_grad/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Reshape_2_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Softmax_grad/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Softmax_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Softmax_grad/sub&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/Softmax&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_grad/Size&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 4\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_grad/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/Sum/reduction_indices&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_grad/add&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_grad/range/start&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_grad/range/delta&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_grad/range&quot;\\n  op: &quot;Range&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_grad/range/start&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_grad/Size&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_grad/range/delta&quot;\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_grad/Fill/value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_grad/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_grad/Shape_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_grad/Fill/value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_grad/DynamicStitch&quot;\\n  op: &quot;DynamicStitch&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_grad/range&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_grad/Fill&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_grad/Maximum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_grad/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_grad/DynamicStitch&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_grad/Maximum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_grad/floordiv&quot;\\n  op: &quot;FloorDiv&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_grad/Maximum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Softmax_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_grad/DynamicStitch&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_grad/floordiv&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_grad/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 128\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_grad/Tile&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_v/read&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Sum_grad/Tile&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/attention/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/attention/mul_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/attention/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/attention/mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/attention/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/attention/mul_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Tanh_grad/TanhGrad&quot;\\n  op: &quot;TanhGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/Tanh&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/add_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/Conv2D&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/add_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/Reshape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/add_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/add_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/add_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Tanh_grad/TanhGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/add_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/add_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/add_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/add_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/add_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Tanh_grad/TanhGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/add_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/add_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/add_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/add_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/attention/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/attention/add_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/add_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/attention/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/attention/add_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/add_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/add_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/attention/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/attention/add_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Conv2D_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n  op: &quot;Conv2DBackpropInput&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Conv2D_grad/Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_w/read&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/add_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;SAME&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Conv2D_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n  op: &quot;Conv2DBackpropFilter&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Conv2D_grad/Shape_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/add_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;SAME&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Conv2D_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Conv2D_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/attention/Conv2D_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Conv2D_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/attention/Conv2D_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Reshape_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Reshape_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/add_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Reshape_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_35&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_1_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Conv2D_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/attention/mul_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Reshape_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Reshape_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/AddN_35&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Reshape_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Reshape_1_grad/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/attention/Reshape_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/attention/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Reshape_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/attention/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/attention/Reshape_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/attention/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/attention/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_36&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Slice_grad/Pad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Reshape_grad/Reshape&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/attention/Slice_grad/Pad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/Reshape_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/Reshape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/Reshape_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/AddN_36&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/Reshape_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/attention/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/attention/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/concat&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/attention/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/attention/attention/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/attention/attention/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/attention/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/attention/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/attention/attention/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/attention/attention/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/attention/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/attention/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/attention/attention/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/attention/attention/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/Reshape_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/concat_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/Reshape_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/Reshape_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/Reshape_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/concat/axis&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_2&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/concat_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/attention/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/attention/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/concat_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/concat_1_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 3\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/concat_1_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/concat_1/axis&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/concat_1_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/concat_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/Slice&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/concat_1_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/Slice&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/ExpandDims&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/concat_1_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/concat_1_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/concat_1_grad/ShapeN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/concat_1_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/concat_1_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/Reshape_1_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/concat_1_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/concat_1_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/concat_1_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/Reshape_1_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/concat_1_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/concat_1_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/concat_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/concat_1_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/concat_1_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/concat_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/concat_1_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/concat_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_7/concat_1_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/concat_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/concat_1_grad/Slice_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/concat_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_7/concat_1_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_37&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/concat_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 3\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_2_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/Tanh_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_2_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/Sigmoid_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_2_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_2_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_2_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_2_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/AddN_37&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/Sigmoid_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_2_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_2_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_2_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_2_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_2_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_2_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/Tanh_1&quot;\\n  input: &quot;gradients/AddN_37&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_2_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_2_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_2_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_2_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_2_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_2_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_2_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_2_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_2_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_2_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Slice_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 3\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Slice_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/Slice&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Slice_grad/stack/1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Slice_grad/stack&quot;\\n  op: &quot;Pack&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Slice_grad/Rank&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Slice_grad/stack/1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;axis&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Slice_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/Slice/begin&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Slice_grad/stack&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Slice_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Slice_grad/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Slice_grad/Shape_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Slice_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Slice_grad/sub_1&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Slice_grad/sub&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/Slice/begin&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Slice_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Slice_grad/sub_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Slice_grad/stack&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Slice_grad/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Slice_grad/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Slice_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Slice_grad/Reshape_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Slice_grad/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Slice_grad/Pad&quot;\\n  op: &quot;Pad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/concat_1_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Slice_grad/concat&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tpaddings&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/ExpandDims_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attn_output_projection/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/ExpandDims_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/concat_1_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/ExpandDims_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/Tanh_1_grad/TanhGrad&quot;\\n  op: &quot;TanhGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/Tanh_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_2_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/Sigmoid_2_grad/SigmoidGrad&quot;\\n  op: &quot;SigmoidGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/Sigmoid_2&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_2_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/ExpandDims_grad/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/ExpandDims_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/ExpandDims_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_7/ExpandDims_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_38&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/Tanh_1_grad/TanhGrad&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 3\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Decoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_1_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_1_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/AddN_38&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_1_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/AddN_38&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_1_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/concat&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/Sigmoid&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_1_grad/tuple/control_dependency&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/Sigmoid&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_1_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/Sigmoid_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_1_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_1_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_1_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_1_grad/tuple/control_dependency_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_1_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_1_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_1_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/Sigmoid_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_1_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_1_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_1_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/concat/axis&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/Reshape_3&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/concat_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/Sigmoid_grad/SigmoidGrad&quot;\\n  op: &quot;SigmoidGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/Sigmoid&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/Sigmoid_1_grad/SigmoidGrad&quot;\\n  op: &quot;SigmoidGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/Sigmoid_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_1_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/Tanh_grad/TanhGrad&quot;\\n  op: &quot;TanhGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/Tanh&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_1_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/split:2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/Sigmoid_grad/SigmoidGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/Sigmoid_grad/SigmoidGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/split_grad/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/Sigmoid_1_grad/SigmoidGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/Tanh_grad/TanhGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/add_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/Sigmoid_2_grad/SigmoidGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/split/split_dim&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 4\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/split_grad/concat&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/split_grad/concat&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/split_grad/concat&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/split_grad/concat&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/concat&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/concat/axis&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_2&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/concat_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention_cell_wrapper/concat&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention_cell_wrapper/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention_cell_wrapper/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/attention_cell_wrapper/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention_cell_wrapper/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/attention_cell_wrapper/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention_cell_wrapper/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention_cell_wrapper/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_8/attention_cell_wrapper/concat/axis&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention_cell_wrapper/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention_cell_wrapper/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/strided_slice_8&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention_cell_wrapper/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Encoder/strided_slice_8&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/Reshape_3&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention_cell_wrapper/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention_cell_wrapper/concat_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention_cell_wrapper/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention_cell_wrapper/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention_cell_wrapper/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention_cell_wrapper/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention_cell_wrapper/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention_cell_wrapper/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention_cell_wrapper/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention_cell_wrapper/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention_cell_wrapper/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention_cell_wrapper/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/attention_cell_wrapper/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention_cell_wrapper/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention_cell_wrapper/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/attention_cell_wrapper/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/attention_cell_wrapper/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_8/attention_cell_wrapper/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_8/attention_cell_wrapper/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_39&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention_cell_wrapper/concat_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Reshape_3_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/Sum_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Reshape_3_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/AddN_39&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Reshape_3_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/mul_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_1_grad/Size&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 4\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_1_grad/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/Sum_1/reduction_indices&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_1_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_1_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_1_grad/add&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_1_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_1_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_1_grad/range/start&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_1_grad/range/delta&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_1_grad/range&quot;\\n  op: &quot;Range&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_1_grad/range/start&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_1_grad/Size&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_1_grad/range/delta&quot;\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_1_grad/Fill/value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_1_grad/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_1_grad/Shape_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_1_grad/Fill/value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_1_grad/DynamicStitch&quot;\\n  op: &quot;DynamicStitch&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_1_grad/range&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_1_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_1_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_1_grad/Fill&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_1_grad/Maximum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_1_grad/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_1_grad/DynamicStitch&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_1_grad/Maximum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_1_grad/floordiv&quot;\\n  op: &quot;FloorDiv&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_1_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_1_grad/Maximum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Reshape_3_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_1_grad/DynamicStitch&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_1_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_1_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_1_grad/floordiv&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/Reshape_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_1_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_1_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_1_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_1_grad/Tile&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_1_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_1_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_1_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/Reshape_2&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_1_grad/Tile&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_1_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_1_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/attention/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/attention/mul_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/attention/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_7/attention/mul_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_1_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/attention/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_7/attention/mul_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Reshape_2_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/Softmax&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Reshape_2_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_1_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Reshape_2_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Softmax_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Reshape_2_grad/Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/Softmax&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Softmax_grad/Sum/reduction_indices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Softmax_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Softmax_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Softmax_grad/Sum/reduction_indices&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Softmax_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Softmax_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Softmax_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Softmax_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Softmax_grad/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Reshape_2_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Softmax_grad/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Softmax_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Softmax_grad/sub&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/Softmax&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_grad/Size&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 4\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_grad/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/Sum/reduction_indices&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_grad/add&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_grad/range/start&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_grad/range/delta&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_grad/range&quot;\\n  op: &quot;Range&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_grad/range/start&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_grad/Size&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_grad/range/delta&quot;\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_grad/Fill/value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_grad/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_grad/Shape_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_grad/Fill/value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_grad/DynamicStitch&quot;\\n  op: &quot;DynamicStitch&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_grad/range&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_grad/Fill&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_grad/Maximum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_grad/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_grad/DynamicStitch&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_grad/Maximum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_grad/floordiv&quot;\\n  op: &quot;FloorDiv&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_grad/Maximum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Softmax_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_grad/DynamicStitch&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_grad/floordiv&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_grad/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 128\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_grad/Tile&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_v/read&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Sum_grad/Tile&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/attention/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/attention/mul_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/attention/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_7/attention/mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/attention/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_7/attention/mul_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Tanh_grad/TanhGrad&quot;\\n  op: &quot;TanhGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/Tanh&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/add_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/Conv2D&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/add_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/Reshape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/add_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/add_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/add_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Tanh_grad/TanhGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/add_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/add_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/add_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/add_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/add_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Tanh_grad/TanhGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/add_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/add_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/add_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/add_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/attention/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/attention/add_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/add_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/attention/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_7/attention/add_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/add_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/add_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/attention/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_7/attention/add_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Conv2D_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n  op: &quot;Conv2DBackpropInput&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Conv2D_grad/Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_w/read&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/add_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;SAME&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Conv2D_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n  op: &quot;Conv2DBackpropFilter&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Conv2D_grad/Shape_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/add_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;SAME&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Conv2D_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Conv2D_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/attention/Conv2D_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_7/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Conv2D_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/attention/Conv2D_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_7/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Reshape_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Reshape_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/add_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Reshape_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_40&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_1_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Conv2D_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_7/attention/mul_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Reshape_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Reshape_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/AddN_40&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Reshape_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Reshape_1_grad/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/attention/Reshape_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/attention/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Reshape_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/attention/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_7/attention/Reshape_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/attention/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_7/attention/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_41&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Slice_grad/Pad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Reshape_grad/Reshape&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_7/attention/Slice_grad/Pad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/Reshape_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/Reshape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/Reshape_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/AddN_41&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/Reshape_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/attention/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/attention/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/concat&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/attention/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/attention/attention/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/attention/attention/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/attention/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/attention/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/attention/attention/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_7/attention/attention/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/attention/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/attention/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/attention/attention/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_7/attention/attention/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/Reshape_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/concat_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/Reshape_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/Reshape_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/Reshape_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/concat/axis&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_2&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/concat_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/attention/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/attention/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/concat_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_7/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_7/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/concat_1_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 3\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/concat_1_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/concat_1/axis&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/concat_1_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/concat_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/Slice&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/concat_1_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/Slice&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/ExpandDims&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/concat_1_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/concat_1_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/concat_1_grad/ShapeN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/concat_1_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/concat_1_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/Reshape_1_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/concat_1_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/concat_1_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/concat_1_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/Reshape_1_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/concat_1_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/concat_1_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/concat_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/concat_1_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/concat_1_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/concat_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/concat_1_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/concat_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_6/concat_1_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/concat_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/concat_1_grad/Slice_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/concat_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_6/concat_1_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_42&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/concat_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 3\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_2_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/Tanh_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_2_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/Sigmoid_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_2_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_2_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_2_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_2_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/AddN_42&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/Sigmoid_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_2_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_2_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_2_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_2_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_2_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_2_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/Tanh_1&quot;\\n  input: &quot;gradients/AddN_42&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_2_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_2_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_2_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_2_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_2_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_2_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_2_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_2_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_2_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_2_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Slice_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 3\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Slice_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/Slice&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Slice_grad/stack/1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Slice_grad/stack&quot;\\n  op: &quot;Pack&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Slice_grad/Rank&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Slice_grad/stack/1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;axis&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Slice_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/Slice/begin&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Slice_grad/stack&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Slice_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Slice_grad/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Slice_grad/Shape_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Slice_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Slice_grad/sub_1&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Slice_grad/sub&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/Slice/begin&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Slice_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Slice_grad/sub_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Slice_grad/stack&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Slice_grad/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Slice_grad/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Slice_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Slice_grad/Reshape_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Slice_grad/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Slice_grad/Pad&quot;\\n  op: &quot;Pad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/concat_1_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Slice_grad/concat&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tpaddings&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/ExpandDims_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attn_output_projection/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/ExpandDims_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/concat_1_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/ExpandDims_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/Tanh_1_grad/TanhGrad&quot;\\n  op: &quot;TanhGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/Tanh_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_2_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/Sigmoid_2_grad/SigmoidGrad&quot;\\n  op: &quot;SigmoidGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/Sigmoid_2&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_2_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/ExpandDims_grad/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/ExpandDims_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/ExpandDims_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_6/ExpandDims_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_43&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/Tanh_1_grad/TanhGrad&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 3\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_1_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_1_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/AddN_43&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_1_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/AddN_43&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_1_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/concat&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/Sigmoid&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_1_grad/tuple/control_dependency&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/Sigmoid&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_1_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/Sigmoid_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_1_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_1_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_1_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_1_grad/tuple/control_dependency_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_1_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_1_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_1_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/Sigmoid_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_1_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_1_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_1_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/concat/axis&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/Reshape_3&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/concat_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/Sigmoid_grad/SigmoidGrad&quot;\\n  op: &quot;SigmoidGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/Sigmoid&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/Sigmoid_1_grad/SigmoidGrad&quot;\\n  op: &quot;SigmoidGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/Sigmoid_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_1_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/Tanh_grad/TanhGrad&quot;\\n  op: &quot;TanhGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/Tanh&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_1_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/split:2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/Sigmoid_grad/SigmoidGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/Sigmoid_grad/SigmoidGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/split_grad/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/Sigmoid_1_grad/SigmoidGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/Tanh_grad/TanhGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/add_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/Sigmoid_2_grad/SigmoidGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/split/split_dim&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 4\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/split_grad/concat&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/split_grad/concat&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/split_grad/concat&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/split_grad/concat&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/concat&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/concat/axis&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_2&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/concat_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_7/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention_cell_wrapper/concat&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention_cell_wrapper/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention_cell_wrapper/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/attention_cell_wrapper/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_7/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention_cell_wrapper/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/attention_cell_wrapper/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_7/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention_cell_wrapper/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention_cell_wrapper/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_7/attention_cell_wrapper/concat/axis&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention_cell_wrapper/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention_cell_wrapper/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/strided_slice_7&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention_cell_wrapper/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Encoder/strided_slice_7&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/Reshape_3&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention_cell_wrapper/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention_cell_wrapper/concat_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention_cell_wrapper/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention_cell_wrapper/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention_cell_wrapper/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention_cell_wrapper/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention_cell_wrapper/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention_cell_wrapper/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention_cell_wrapper/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention_cell_wrapper/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention_cell_wrapper/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention_cell_wrapper/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/attention_cell_wrapper/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention_cell_wrapper/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention_cell_wrapper/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/attention_cell_wrapper/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_7/attention_cell_wrapper/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_7/attention_cell_wrapper/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_7/attention_cell_wrapper/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_7/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_44&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention_cell_wrapper/concat_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Reshape_3_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/Sum_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Reshape_3_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/AddN_44&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Reshape_3_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/mul_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_1_grad/Size&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 4\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_1_grad/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/Sum_1/reduction_indices&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_1_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_1_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_1_grad/add&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_1_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_1_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_1_grad/range/start&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_1_grad/range/delta&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_1_grad/range&quot;\\n  op: &quot;Range&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_1_grad/range/start&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_1_grad/Size&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_1_grad/range/delta&quot;\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_1_grad/Fill/value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_1_grad/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_1_grad/Shape_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_1_grad/Fill/value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_1_grad/DynamicStitch&quot;\\n  op: &quot;DynamicStitch&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_1_grad/range&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_1_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_1_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_1_grad/Fill&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_1_grad/Maximum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_1_grad/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_1_grad/DynamicStitch&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_1_grad/Maximum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_1_grad/floordiv&quot;\\n  op: &quot;FloorDiv&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_1_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_1_grad/Maximum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Reshape_3_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_1_grad/DynamicStitch&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_1_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_1_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_1_grad/floordiv&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/Reshape_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_1_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_1_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_1_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_1_grad/Tile&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_1_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_1_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_1_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/Reshape_2&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_1_grad/Tile&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_1_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_1_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/attention/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/attention/mul_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/attention/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_6/attention/mul_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_1_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/attention/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_6/attention/mul_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Reshape_2_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/Softmax&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Reshape_2_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_1_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Reshape_2_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Softmax_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Reshape_2_grad/Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/Softmax&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Softmax_grad/Sum/reduction_indices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Softmax_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Softmax_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Softmax_grad/Sum/reduction_indices&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Softmax_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Softmax_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Softmax_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Softmax_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Softmax_grad/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Reshape_2_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Softmax_grad/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Softmax_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Softmax_grad/sub&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/Softmax&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_grad/Size&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 4\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_grad/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/Sum/reduction_indices&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_grad/add&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_grad/range/start&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_grad/range/delta&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_grad/range&quot;\\n  op: &quot;Range&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_grad/range/start&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_grad/Size&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_grad/range/delta&quot;\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_grad/Fill/value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_grad/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_grad/Shape_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_grad/Fill/value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_grad/DynamicStitch&quot;\\n  op: &quot;DynamicStitch&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_grad/range&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_grad/Fill&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_grad/Maximum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_grad/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_grad/DynamicStitch&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_grad/Maximum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_grad/floordiv&quot;\\n  op: &quot;FloorDiv&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_grad/Maximum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Softmax_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_grad/DynamicStitch&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_grad/floordiv&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_grad/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 128\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_grad/Tile&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_v/read&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Sum_grad/Tile&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/attention/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/attention/mul_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/attention/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_6/attention/mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/attention/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_6/attention/mul_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Tanh_grad/TanhGrad&quot;\\n  op: &quot;TanhGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/Tanh&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/add_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/Conv2D&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/add_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/Reshape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/add_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/add_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/add_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Tanh_grad/TanhGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/add_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/add_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/add_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/add_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/add_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Tanh_grad/TanhGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/add_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/add_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/add_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/add_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/attention/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/attention/add_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/add_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/attention/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_6/attention/add_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/add_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/add_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/attention/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_6/attention/add_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Conv2D_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n  op: &quot;Conv2DBackpropInput&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Conv2D_grad/Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_w/read&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/add_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;SAME&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Conv2D_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n  op: &quot;Conv2DBackpropFilter&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Conv2D_grad/Shape_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/add_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;SAME&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Conv2D_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Conv2D_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/attention/Conv2D_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_6/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Conv2D_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/attention/Conv2D_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_6/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Reshape_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Reshape_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/add_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Reshape_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_45&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_1_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Conv2D_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_6/attention/mul_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Reshape_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Reshape_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/AddN_45&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Reshape_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Reshape_1_grad/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/attention/Reshape_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/attention/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Reshape_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/attention/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_6/attention/Reshape_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/attention/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_6/attention/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_46&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Slice_grad/Pad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Reshape_grad/Reshape&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_6/attention/Slice_grad/Pad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/Reshape_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/Reshape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/Reshape_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/AddN_46&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/Reshape_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/attention/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/attention/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/concat&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/attention/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/attention/attention/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/attention/attention/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/attention/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/attention/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/attention/attention/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_6/attention/attention/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/attention/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/attention/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/attention/attention/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_6/attention/attention/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/Reshape_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/concat_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/Reshape_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/Reshape_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/Reshape_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/concat/axis&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_2&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/concat_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/attention/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/attention/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/concat_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_6/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_6/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/concat_1_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 3\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/concat_1_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/concat_1/axis&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/concat_1_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/concat_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/Slice&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/concat_1_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/Slice&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/ExpandDims&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/concat_1_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/concat_1_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/concat_1_grad/ShapeN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/concat_1_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/concat_1_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/Reshape_1_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/concat_1_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/concat_1_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/concat_1_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/Reshape_1_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/concat_1_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/concat_1_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/concat_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/concat_1_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/concat_1_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/concat_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/concat_1_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/concat_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_5/concat_1_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/concat_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/concat_1_grad/Slice_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/concat_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_5/concat_1_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_47&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/concat_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 3\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_2_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/Tanh_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_2_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/Sigmoid_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_2_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_2_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_2_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_2_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/AddN_47&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/Sigmoid_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_2_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_2_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_2_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_2_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_2_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_2_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/Tanh_1&quot;\\n  input: &quot;gradients/AddN_47&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_2_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_2_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_2_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_2_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_2_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_2_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_2_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_2_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_2_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_2_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Slice_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 3\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Slice_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/Slice&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Slice_grad/stack/1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Slice_grad/stack&quot;\\n  op: &quot;Pack&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Slice_grad/Rank&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Slice_grad/stack/1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;axis&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Slice_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/Slice/begin&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Slice_grad/stack&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Slice_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Slice_grad/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Slice_grad/Shape_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Slice_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Slice_grad/sub_1&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Slice_grad/sub&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/Slice/begin&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Slice_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Slice_grad/sub_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Slice_grad/stack&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Slice_grad/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Slice_grad/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Slice_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Slice_grad/Reshape_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Slice_grad/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Slice_grad/Pad&quot;\\n  op: &quot;Pad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/concat_1_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Slice_grad/concat&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tpaddings&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/ExpandDims_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attn_output_projection/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/ExpandDims_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/concat_1_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/ExpandDims_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/Tanh_1_grad/TanhGrad&quot;\\n  op: &quot;TanhGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/Tanh_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_2_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/Sigmoid_2_grad/SigmoidGrad&quot;\\n  op: &quot;SigmoidGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/Sigmoid_2&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_2_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/ExpandDims_grad/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/ExpandDims_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/ExpandDims_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_5/ExpandDims_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_48&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/Tanh_1_grad/TanhGrad&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 3\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_1_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_1_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/AddN_48&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_1_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/AddN_48&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_1_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/concat&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/Sigmoid&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_1_grad/tuple/control_dependency&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/Sigmoid&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_1_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/Sigmoid_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_1_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_1_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_1_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_1_grad/tuple/control_dependency_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_1_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_1_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_1_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/Sigmoid_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_1_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_1_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_1_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/concat/axis&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/Reshape_3&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/concat_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/Sigmoid_grad/SigmoidGrad&quot;\\n  op: &quot;SigmoidGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/Sigmoid&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/Sigmoid_1_grad/SigmoidGrad&quot;\\n  op: &quot;SigmoidGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/Sigmoid_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_1_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/Tanh_grad/TanhGrad&quot;\\n  op: &quot;TanhGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/Tanh&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_1_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/split:2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/Sigmoid_grad/SigmoidGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/Sigmoid_grad/SigmoidGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/split_grad/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/Sigmoid_1_grad/SigmoidGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/Tanh_grad/TanhGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/add_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/Sigmoid_2_grad/SigmoidGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/split/split_dim&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 4\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/split_grad/concat&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/split_grad/concat&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/split_grad/concat&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/split_grad/concat&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/concat&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/concat/axis&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_2&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/concat_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_6/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention_cell_wrapper/concat&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention_cell_wrapper/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention_cell_wrapper/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/attention_cell_wrapper/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_6/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention_cell_wrapper/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/attention_cell_wrapper/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_6/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention_cell_wrapper/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention_cell_wrapper/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_6/attention_cell_wrapper/concat/axis&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention_cell_wrapper/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention_cell_wrapper/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/strided_slice_6&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention_cell_wrapper/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Encoder/strided_slice_6&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/Reshape_3&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention_cell_wrapper/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention_cell_wrapper/concat_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention_cell_wrapper/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention_cell_wrapper/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention_cell_wrapper/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention_cell_wrapper/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention_cell_wrapper/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention_cell_wrapper/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention_cell_wrapper/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention_cell_wrapper/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention_cell_wrapper/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention_cell_wrapper/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/attention_cell_wrapper/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention_cell_wrapper/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention_cell_wrapper/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/attention_cell_wrapper/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_6/attention_cell_wrapper/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_6/attention_cell_wrapper/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_6/attention_cell_wrapper/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_6/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_49&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention_cell_wrapper/concat_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Reshape_3_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/Sum_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Reshape_3_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/AddN_49&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Reshape_3_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/mul_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_1_grad/Size&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 4\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_1_grad/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/Sum_1/reduction_indices&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_1_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_1_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_1_grad/add&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_1_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_1_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_1_grad/range/start&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_1_grad/range/delta&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_1_grad/range&quot;\\n  op: &quot;Range&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_1_grad/range/start&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_1_grad/Size&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_1_grad/range/delta&quot;\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_1_grad/Fill/value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_1_grad/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_1_grad/Shape_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_1_grad/Fill/value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_1_grad/DynamicStitch&quot;\\n  op: &quot;DynamicStitch&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_1_grad/range&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_1_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_1_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_1_grad/Fill&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_1_grad/Maximum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_1_grad/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_1_grad/DynamicStitch&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_1_grad/Maximum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_1_grad/floordiv&quot;\\n  op: &quot;FloorDiv&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_1_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_1_grad/Maximum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Reshape_3_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_1_grad/DynamicStitch&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_1_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_1_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_1_grad/floordiv&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/Reshape_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_1_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_1_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_1_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_1_grad/Tile&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_1_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_1_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_1_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/Reshape_2&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_1_grad/Tile&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_1_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_1_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/attention/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/attention/mul_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/attention/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_5/attention/mul_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_1_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/attention/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_5/attention/mul_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Reshape_2_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/Softmax&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Reshape_2_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_1_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Reshape_2_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Softmax_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Reshape_2_grad/Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/Softmax&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Softmax_grad/Sum/reduction_indices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Softmax_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Softmax_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Softmax_grad/Sum/reduction_indices&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Softmax_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Softmax_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Softmax_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Softmax_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Softmax_grad/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Reshape_2_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Softmax_grad/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Softmax_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Softmax_grad/sub&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/Softmax&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_grad/Size&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 4\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_grad/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/Sum/reduction_indices&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_grad/add&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_grad/range/start&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_grad/range/delta&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_grad/range&quot;\\n  op: &quot;Range&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_grad/range/start&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_grad/Size&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_grad/range/delta&quot;\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_grad/Fill/value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_grad/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_grad/Shape_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_grad/Fill/value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_grad/DynamicStitch&quot;\\n  op: &quot;DynamicStitch&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_grad/range&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_grad/Fill&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_grad/Maximum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_grad/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_grad/DynamicStitch&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_grad/Maximum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_grad/floordiv&quot;\\n  op: &quot;FloorDiv&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_grad/Maximum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Softmax_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_grad/DynamicStitch&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_grad/floordiv&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_grad/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 128\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_grad/Tile&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_v/read&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Sum_grad/Tile&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/attention/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/attention/mul_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/attention/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_5/attention/mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/attention/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_5/attention/mul_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Tanh_grad/TanhGrad&quot;\\n  op: &quot;TanhGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/Tanh&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/add_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/Conv2D&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/add_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/Reshape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/add_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/add_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/add_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Tanh_grad/TanhGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/add_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/add_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/add_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/add_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/add_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Tanh_grad/TanhGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/add_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/add_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/add_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/add_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/attention/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/attention/add_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/add_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/attention/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_5/attention/add_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/add_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/add_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/attention/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_5/attention/add_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Conv2D_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n  op: &quot;Conv2DBackpropInput&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Conv2D_grad/Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_w/read&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/add_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;SAME&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Conv2D_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n  op: &quot;Conv2DBackpropFilter&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Conv2D_grad/Shape_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/add_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;SAME&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Conv2D_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Conv2D_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/attention/Conv2D_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_5/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Conv2D_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/attention/Conv2D_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_5/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Reshape_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Reshape_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/add_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Reshape_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_50&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_1_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Conv2D_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_5/attention/mul_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Reshape_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Reshape_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/AddN_50&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Reshape_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Reshape_1_grad/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/attention/Reshape_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/attention/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Reshape_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/attention/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_5/attention/Reshape_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/attention/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_5/attention/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_51&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Slice_grad/Pad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Reshape_grad/Reshape&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_5/attention/Slice_grad/Pad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/Reshape_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/Reshape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/Reshape_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/AddN_51&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/Reshape_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/attention/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/attention/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/concat&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/attention/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/attention/attention/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/attention/attention/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/attention/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/attention/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/attention/attention/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_5/attention/attention/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/attention/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/attention/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/attention/attention/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_5/attention/attention/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/Reshape_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/concat_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/Reshape_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/Reshape_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/Reshape_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/concat/axis&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_2&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/concat_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/attention/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/attention/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/concat_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_5/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_5/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/concat_1_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 3\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/concat_1_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/concat_1/axis&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/concat_1_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/concat_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/Slice&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/concat_1_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/Slice&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/ExpandDims&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/concat_1_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/concat_1_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/concat_1_grad/ShapeN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/concat_1_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/concat_1_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/Reshape_1_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/concat_1_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/concat_1_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/concat_1_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/Reshape_1_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/concat_1_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/concat_1_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/concat_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/concat_1_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/concat_1_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/concat_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/concat_1_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/concat_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_4/concat_1_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/concat_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/concat_1_grad/Slice_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/concat_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_4/concat_1_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_52&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/concat_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 3\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_2_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/Tanh_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_2_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/Sigmoid_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_2_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_2_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_2_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_2_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/AddN_52&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/Sigmoid_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_2_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_2_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_2_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_2_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_2_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_2_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/Tanh_1&quot;\\n  input: &quot;gradients/AddN_52&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_2_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_2_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_2_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_2_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_2_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_2_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_2_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_2_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_2_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_2_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Slice_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 3\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Slice_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/Slice&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Slice_grad/stack/1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Slice_grad/stack&quot;\\n  op: &quot;Pack&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Slice_grad/Rank&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Slice_grad/stack/1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;axis&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Slice_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/Slice/begin&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Slice_grad/stack&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Slice_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Slice_grad/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Slice_grad/Shape_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Slice_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Slice_grad/sub_1&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Slice_grad/sub&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/Slice/begin&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Slice_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Slice_grad/sub_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Slice_grad/stack&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Slice_grad/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Slice_grad/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Slice_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Slice_grad/Reshape_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Slice_grad/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Slice_grad/Pad&quot;\\n  op: &quot;Pad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/concat_1_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Slice_grad/concat&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tpaddings&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/ExpandDims_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attn_output_projection/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/ExpandDims_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/concat_1_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/ExpandDims_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/Tanh_1_grad/TanhGrad&quot;\\n  op: &quot;TanhGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/Tanh_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_2_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/Sigmoid_2_grad/SigmoidGrad&quot;\\n  op: &quot;SigmoidGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/Sigmoid_2&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_2_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/ExpandDims_grad/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/ExpandDims_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/ExpandDims_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_4/ExpandDims_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_53&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/Tanh_1_grad/TanhGrad&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 3\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_1_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_1_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/AddN_53&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_1_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/AddN_53&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_1_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/concat&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/Sigmoid&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_1_grad/tuple/control_dependency&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/Sigmoid&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_1_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/Sigmoid_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_1_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_1_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_1_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_1_grad/tuple/control_dependency_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_1_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_1_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_1_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/Sigmoid_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_1_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_1_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_1_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/concat/axis&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/Reshape_3&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/concat_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/Sigmoid_grad/SigmoidGrad&quot;\\n  op: &quot;SigmoidGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/Sigmoid&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/Sigmoid_1_grad/SigmoidGrad&quot;\\n  op: &quot;SigmoidGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/Sigmoid_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_1_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/Tanh_grad/TanhGrad&quot;\\n  op: &quot;TanhGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/Tanh&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_1_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/split:2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/Sigmoid_grad/SigmoidGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/Sigmoid_grad/SigmoidGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/split_grad/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/Sigmoid_1_grad/SigmoidGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/Tanh_grad/TanhGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/add_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/Sigmoid_2_grad/SigmoidGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/split/split_dim&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 4\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/split_grad/concat&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/split_grad/concat&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/split_grad/concat&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/split_grad/concat&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/concat&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/concat/axis&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_2&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/concat_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_5/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention_cell_wrapper/concat&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention_cell_wrapper/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention_cell_wrapper/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/attention_cell_wrapper/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_5/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention_cell_wrapper/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/attention_cell_wrapper/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_5/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention_cell_wrapper/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention_cell_wrapper/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_5/attention_cell_wrapper/concat/axis&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention_cell_wrapper/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention_cell_wrapper/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/strided_slice_5&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention_cell_wrapper/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Encoder/strided_slice_5&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/Reshape_3&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention_cell_wrapper/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention_cell_wrapper/concat_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention_cell_wrapper/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention_cell_wrapper/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention_cell_wrapper/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention_cell_wrapper/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention_cell_wrapper/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention_cell_wrapper/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention_cell_wrapper/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention_cell_wrapper/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention_cell_wrapper/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention_cell_wrapper/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/attention_cell_wrapper/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention_cell_wrapper/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention_cell_wrapper/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/attention_cell_wrapper/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_5/attention_cell_wrapper/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_5/attention_cell_wrapper/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_5/attention_cell_wrapper/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_5/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_54&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention_cell_wrapper/concat_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Reshape_3_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/Sum_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Reshape_3_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/AddN_54&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Reshape_3_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/mul_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_1_grad/Size&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 4\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_1_grad/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/Sum_1/reduction_indices&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_1_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_1_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_1_grad/add&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_1_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_1_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_1_grad/range/start&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_1_grad/range/delta&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_1_grad/range&quot;\\n  op: &quot;Range&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_1_grad/range/start&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_1_grad/Size&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_1_grad/range/delta&quot;\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_1_grad/Fill/value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_1_grad/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_1_grad/Shape_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_1_grad/Fill/value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_1_grad/DynamicStitch&quot;\\n  op: &quot;DynamicStitch&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_1_grad/range&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_1_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_1_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_1_grad/Fill&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_1_grad/Maximum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_1_grad/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_1_grad/DynamicStitch&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_1_grad/Maximum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_1_grad/floordiv&quot;\\n  op: &quot;FloorDiv&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_1_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_1_grad/Maximum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Reshape_3_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_1_grad/DynamicStitch&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_1_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_1_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_1_grad/floordiv&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/Reshape_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_1_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_1_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_1_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_1_grad/Tile&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_1_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_1_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_1_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/Reshape_2&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_1_grad/Tile&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_1_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_1_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/attention/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/attention/mul_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/attention/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_4/attention/mul_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_1_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/attention/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_4/attention/mul_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Reshape_2_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/Softmax&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Reshape_2_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_1_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Reshape_2_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Softmax_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Reshape_2_grad/Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/Softmax&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Softmax_grad/Sum/reduction_indices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Softmax_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Softmax_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Softmax_grad/Sum/reduction_indices&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Softmax_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Softmax_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Softmax_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Softmax_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Softmax_grad/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Reshape_2_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Softmax_grad/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Softmax_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Softmax_grad/sub&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/Softmax&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_grad/Size&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 4\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_grad/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/Sum/reduction_indices&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_grad/add&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_grad/range/start&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_grad/range/delta&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_grad/range&quot;\\n  op: &quot;Range&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_grad/range/start&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_grad/Size&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_grad/range/delta&quot;\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_grad/Fill/value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_grad/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_grad/Shape_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_grad/Fill/value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_grad/DynamicStitch&quot;\\n  op: &quot;DynamicStitch&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_grad/range&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_grad/Fill&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_grad/Maximum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_grad/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_grad/DynamicStitch&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_grad/Maximum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_grad/floordiv&quot;\\n  op: &quot;FloorDiv&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_grad/Maximum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Softmax_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_grad/DynamicStitch&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_grad/floordiv&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_grad/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 128\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_grad/Tile&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_v/read&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Sum_grad/Tile&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/attention/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/attention/mul_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/attention/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_4/attention/mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/attention/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_4/attention/mul_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Tanh_grad/TanhGrad&quot;\\n  op: &quot;TanhGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/Tanh&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/add_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/Conv2D&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/add_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/Reshape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/add_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/add_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/add_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Tanh_grad/TanhGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/add_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/add_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/add_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/add_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/add_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Tanh_grad/TanhGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/add_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/add_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/add_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/add_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/attention/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/attention/add_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/add_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/attention/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_4/attention/add_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/add_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/add_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/attention/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_4/attention/add_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Conv2D_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n  op: &quot;Conv2DBackpropInput&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Conv2D_grad/Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_w/read&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/add_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;SAME&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Conv2D_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n  op: &quot;Conv2DBackpropFilter&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Conv2D_grad/Shape_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/add_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;SAME&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Conv2D_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Conv2D_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/attention/Conv2D_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_4/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Conv2D_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/attention/Conv2D_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_4/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Reshape_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Reshape_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/add_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Reshape_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_55&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_1_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Conv2D_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_4/attention/mul_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Reshape_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Reshape_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/AddN_55&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Reshape_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Reshape_1_grad/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/attention/Reshape_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/attention/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Reshape_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/attention/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_4/attention/Reshape_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/attention/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_4/attention/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_56&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Slice_grad/Pad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Reshape_grad/Reshape&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_4/attention/Slice_grad/Pad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/Reshape_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/Reshape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/Reshape_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/AddN_56&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/Reshape_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/attention/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/attention/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/concat&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/attention/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/attention/attention/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/attention/attention/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/attention/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/attention/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/attention/attention/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_4/attention/attention/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/attention/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/attention/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/attention/attention/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_4/attention/attention/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/Reshape_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/concat_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/Reshape_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/Reshape_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/Reshape_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/concat/axis&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_2&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/concat_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/attention/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/attention/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/concat_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_4/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_4/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/concat_1_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 3\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/concat_1_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/concat_1/axis&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/concat_1_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/concat_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/Slice&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/concat_1_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/Slice&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/ExpandDims&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/concat_1_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/concat_1_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/concat_1_grad/ShapeN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/concat_1_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/concat_1_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/Reshape_1_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/concat_1_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/concat_1_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/concat_1_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/Reshape_1_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/concat_1_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/concat_1_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/concat_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/concat_1_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/concat_1_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/concat_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/concat_1_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/concat_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_3/concat_1_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/concat_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/concat_1_grad/Slice_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/concat_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_3/concat_1_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_57&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/concat_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 3\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_2_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/Tanh_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_2_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/Sigmoid_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_2_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_2_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_2_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_2_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/AddN_57&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/Sigmoid_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_2_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_2_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_2_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_2_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_2_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_2_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/Tanh_1&quot;\\n  input: &quot;gradients/AddN_57&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_2_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_2_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_2_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_2_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_2_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_2_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_2_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_2_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_2_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_2_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Slice_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 3\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Slice_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/Slice&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Slice_grad/stack/1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Slice_grad/stack&quot;\\n  op: &quot;Pack&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Slice_grad/Rank&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Slice_grad/stack/1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;axis&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Slice_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/Slice/begin&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Slice_grad/stack&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Slice_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Slice_grad/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Slice_grad/Shape_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Slice_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Slice_grad/sub_1&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Slice_grad/sub&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/Slice/begin&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Slice_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Slice_grad/sub_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Slice_grad/stack&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Slice_grad/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Slice_grad/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Slice_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Slice_grad/Reshape_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Slice_grad/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Slice_grad/Pad&quot;\\n  op: &quot;Pad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/concat_1_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Slice_grad/concat&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tpaddings&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/ExpandDims_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attn_output_projection/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/ExpandDims_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/concat_1_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/ExpandDims_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/Tanh_1_grad/TanhGrad&quot;\\n  op: &quot;TanhGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/Tanh_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_2_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/Sigmoid_2_grad/SigmoidGrad&quot;\\n  op: &quot;SigmoidGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/Sigmoid_2&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_2_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/ExpandDims_grad/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/ExpandDims_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/ExpandDims_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_3/ExpandDims_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_58&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/Tanh_1_grad/TanhGrad&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 3\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_1_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_1_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/AddN_58&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_1_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/AddN_58&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_1_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/Sigmoid&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_1_grad/tuple/control_dependency&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/Sigmoid&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_1_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/Sigmoid_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_1_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_1_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_1_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_1_grad/tuple/control_dependency_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_1_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_1_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_1_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/Sigmoid_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_1_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_1_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_1_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat/axis&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/Reshape_3&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/Sigmoid_grad/SigmoidGrad&quot;\\n  op: &quot;SigmoidGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/Sigmoid&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/Sigmoid_1_grad/SigmoidGrad&quot;\\n  op: &quot;SigmoidGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/Sigmoid_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_1_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/Tanh_grad/TanhGrad&quot;\\n  op: &quot;TanhGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/Tanh&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_1_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/split:2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/Sigmoid_grad/SigmoidGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/Sigmoid_grad/SigmoidGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/split_grad/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/Sigmoid_1_grad/SigmoidGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/Tanh_grad/TanhGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/add_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/Sigmoid_2_grad/SigmoidGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/split/split_dim&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 4\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/split_grad/concat&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/split_grad/concat&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/split_grad/concat&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/split_grad/concat&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/concat&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/concat/axis&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/concat_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_4/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention_cell_wrapper/concat&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention_cell_wrapper/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention_cell_wrapper/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/attention_cell_wrapper/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_4/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention_cell_wrapper/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/attention_cell_wrapper/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_4/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention_cell_wrapper/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention_cell_wrapper/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_4/attention_cell_wrapper/concat/axis&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention_cell_wrapper/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention_cell_wrapper/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/strided_slice_4&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention_cell_wrapper/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Encoder/strided_slice_4&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/Reshape_3&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention_cell_wrapper/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention_cell_wrapper/concat_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention_cell_wrapper/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention_cell_wrapper/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention_cell_wrapper/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention_cell_wrapper/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention_cell_wrapper/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention_cell_wrapper/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention_cell_wrapper/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention_cell_wrapper/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention_cell_wrapper/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention_cell_wrapper/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/attention_cell_wrapper/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention_cell_wrapper/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention_cell_wrapper/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/attention_cell_wrapper/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_4/attention_cell_wrapper/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_4/attention_cell_wrapper/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_4/attention_cell_wrapper/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_4/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_59&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention_cell_wrapper/concat_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Reshape_3_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/Sum_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Reshape_3_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/AddN_59&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Reshape_3_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/mul_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_1_grad/Size&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 4\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_1_grad/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/Sum_1/reduction_indices&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_1_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_1_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_1_grad/add&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_1_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_1_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_1_grad/range/start&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_1_grad/range/delta&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_1_grad/range&quot;\\n  op: &quot;Range&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_1_grad/range/start&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_1_grad/Size&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_1_grad/range/delta&quot;\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_1_grad/Fill/value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_1_grad/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_1_grad/Shape_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_1_grad/Fill/value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_1_grad/DynamicStitch&quot;\\n  op: &quot;DynamicStitch&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_1_grad/range&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_1_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_1_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_1_grad/Fill&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_1_grad/Maximum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_1_grad/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_1_grad/DynamicStitch&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_1_grad/Maximum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_1_grad/floordiv&quot;\\n  op: &quot;FloorDiv&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_1_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_1_grad/Maximum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Reshape_3_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_1_grad/DynamicStitch&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_1_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_1_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_1_grad/floordiv&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/Reshape_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_1_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_1_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_1_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_1_grad/Tile&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_1_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_1_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_1_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/Reshape_2&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_1_grad/Tile&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_1_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_1_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/attention/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/attention/mul_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/attention/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_3/attention/mul_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_1_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/attention/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_3/attention/mul_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Reshape_2_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/Softmax&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Reshape_2_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_1_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Reshape_2_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Softmax_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Reshape_2_grad/Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/Softmax&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Softmax_grad/Sum/reduction_indices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Softmax_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Softmax_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Softmax_grad/Sum/reduction_indices&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Softmax_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Softmax_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Softmax_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Softmax_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Softmax_grad/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Reshape_2_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Softmax_grad/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Softmax_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Softmax_grad/sub&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/Softmax&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_grad/Size&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 4\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_grad/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/Sum/reduction_indices&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_grad/add&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_grad/range/start&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_grad/range/delta&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_grad/range&quot;\\n  op: &quot;Range&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_grad/range/start&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_grad/Size&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_grad/range/delta&quot;\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_grad/Fill/value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_grad/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_grad/Shape_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_grad/Fill/value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_grad/DynamicStitch&quot;\\n  op: &quot;DynamicStitch&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_grad/range&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_grad/Fill&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_grad/Maximum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_grad/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_grad/DynamicStitch&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_grad/Maximum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_grad/floordiv&quot;\\n  op: &quot;FloorDiv&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_grad/Maximum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Softmax_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_grad/DynamicStitch&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_grad/floordiv&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_grad/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 128\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_grad/Tile&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_v/read&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Sum_grad/Tile&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/attention/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/attention/mul_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/attention/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_3/attention/mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/attention/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_3/attention/mul_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Tanh_grad/TanhGrad&quot;\\n  op: &quot;TanhGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/Tanh&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/add_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/Conv2D&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/add_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/Reshape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/add_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/add_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/add_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Tanh_grad/TanhGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/add_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/add_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/add_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/add_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/add_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Tanh_grad/TanhGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/add_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/add_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/add_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/add_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/attention/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/attention/add_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/add_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/attention/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_3/attention/add_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/add_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/add_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/attention/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_3/attention/add_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Conv2D_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n  op: &quot;Conv2DBackpropInput&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Conv2D_grad/Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_w/read&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/add_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;SAME&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Conv2D_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n  op: &quot;Conv2DBackpropFilter&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Conv2D_grad/Shape_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/add_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;SAME&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Conv2D_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Conv2D_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/attention/Conv2D_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_3/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Conv2D_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/attention/Conv2D_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_3/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Reshape_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Reshape_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/add_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Reshape_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_60&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_1_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Conv2D_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_3/attention/mul_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Reshape_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Reshape_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/AddN_60&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Reshape_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Reshape_1_grad/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/attention/Reshape_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/attention/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Reshape_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/attention/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_3/attention/Reshape_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/attention/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_3/attention/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_61&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Slice_grad/Pad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Reshape_grad/Reshape&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_3/attention/Slice_grad/Pad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/Reshape_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/Reshape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/Reshape_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/AddN_61&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/Reshape_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/attention/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/attention/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/concat&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/attention/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/attention/attention/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/attention/attention/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/attention/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/attention/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/attention/attention/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_3/attention/attention/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/attention/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/attention/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/attention/attention/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_3/attention/attention/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/Reshape_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/concat_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/Reshape_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/Reshape_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/Reshape_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/concat/axis&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/concat_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/attention/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/attention/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/concat_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_3/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_3/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/concat_1_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 3\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/concat_1_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/concat_1/axis&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/concat_1_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/concat_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/Slice&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/concat_1_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/Slice&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/ExpandDims&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/concat_1_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/concat_1_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/concat_1_grad/ShapeN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/concat_1_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/concat_1_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/Reshape_1_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/concat_1_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/concat_1_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/concat_1_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/Reshape_1_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/concat_1_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/concat_1_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/concat_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/concat_1_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/concat_1_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/concat_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/concat_1_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/concat_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_2/concat_1_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/concat_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/concat_1_grad/Slice_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/concat_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_2/concat_1_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_62&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/concat_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 3\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/Tanh_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/Sigmoid_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/AddN_62&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/Sigmoid_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/Tanh_1&quot;\\n  input: &quot;gradients/AddN_62&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Slice_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 3\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Slice_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/Slice&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Slice_grad/stack/1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Slice_grad/stack&quot;\\n  op: &quot;Pack&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Slice_grad/Rank&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Slice_grad/stack/1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;axis&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Slice_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/Slice/begin&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Slice_grad/stack&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Slice_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Slice_grad/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Slice_grad/Shape_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Slice_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Slice_grad/sub_1&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Slice_grad/sub&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/Slice/begin&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Slice_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Slice_grad/sub_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Slice_grad/stack&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Slice_grad/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Slice_grad/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Slice_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Slice_grad/Reshape_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Slice_grad/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Slice_grad/Pad&quot;\\n  op: &quot;Pad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/concat_1_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Slice_grad/concat&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tpaddings&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/ExpandDims_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attn_output_projection/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/ExpandDims_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/concat_1_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/ExpandDims_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/Tanh_1_grad/TanhGrad&quot;\\n  op: &quot;TanhGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/Tanh_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/Sigmoid_2_grad/SigmoidGrad&quot;\\n  op: &quot;SigmoidGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/Sigmoid_2&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_2_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/ExpandDims_grad/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/ExpandDims_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/ExpandDims_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_2/ExpandDims_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_63&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/Tanh_1_grad/TanhGrad&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 3\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/AddN_63&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/AddN_63&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/Sigmoid&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/tuple/control_dependency&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/Sigmoid&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/Sigmoid_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/tuple/control_dependency_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/Sigmoid_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_1_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat/axis&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/Reshape_3&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/Sigmoid_grad/SigmoidGrad&quot;\\n  op: &quot;SigmoidGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/Sigmoid&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/Sigmoid_1_grad/SigmoidGrad&quot;\\n  op: &quot;SigmoidGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/Sigmoid_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/Tanh_grad/TanhGrad&quot;\\n  op: &quot;TanhGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/Tanh&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_1_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/split:2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/Sigmoid_grad/SigmoidGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/Sigmoid_grad/SigmoidGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/split_grad/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/Sigmoid_1_grad/SigmoidGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/Tanh_grad/TanhGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/add_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/Sigmoid_2_grad/SigmoidGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/split/split_dim&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 4\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/split_grad/concat&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/split_grad/concat&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/split_grad/concat&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/split_grad/concat&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat/axis&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_3/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention_cell_wrapper/concat&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention_cell_wrapper/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention_cell_wrapper/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/attention_cell_wrapper/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_3/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention_cell_wrapper/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/attention_cell_wrapper/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_3/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_3/attention_cell_wrapper/concat/axis&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/strided_slice_3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Encoder/strided_slice_3&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/Reshape_3&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention_cell_wrapper/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention_cell_wrapper/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_64&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention_cell_wrapper/concat_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Reshape_3_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/Sum_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Reshape_3_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/AddN_64&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Reshape_3_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/mul_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_1_grad/Size&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 4\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_1_grad/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/Sum_1/reduction_indices&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_1_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_1_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_1_grad/add&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_1_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_1_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_1_grad/range/start&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_1_grad/range/delta&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_1_grad/range&quot;\\n  op: &quot;Range&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_1_grad/range/start&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_1_grad/Size&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_1_grad/range/delta&quot;\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_1_grad/Fill/value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_1_grad/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_1_grad/Shape_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_1_grad/Fill/value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_1_grad/DynamicStitch&quot;\\n  op: &quot;DynamicStitch&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_1_grad/range&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_1_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_1_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_1_grad/Fill&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_1_grad/Maximum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_1_grad/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_1_grad/DynamicStitch&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_1_grad/Maximum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_1_grad/floordiv&quot;\\n  op: &quot;FloorDiv&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_1_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_1_grad/Maximum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Reshape_3_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_1_grad/DynamicStitch&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_1_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_1_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_1_grad/floordiv&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/Reshape_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_1_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_1_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_1_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_1_grad/Tile&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_1_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_1_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_1_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/Reshape_2&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_1_grad/Tile&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_1_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_1_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/attention/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/attention/mul_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/attention/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_2/attention/mul_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_1_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/attention/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_2/attention/mul_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Reshape_2_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/Softmax&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Reshape_2_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_1_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Reshape_2_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Softmax_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Reshape_2_grad/Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/Softmax&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Softmax_grad/Sum/reduction_indices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Softmax_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Softmax_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Softmax_grad/Sum/reduction_indices&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Softmax_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Softmax_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Softmax_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Softmax_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Softmax_grad/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Reshape_2_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Softmax_grad/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Softmax_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Softmax_grad/sub&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/Softmax&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_grad/Size&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 4\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_grad/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/Sum/reduction_indices&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_grad/add&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_grad/range/start&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_grad/range/delta&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_grad/range&quot;\\n  op: &quot;Range&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_grad/range/start&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_grad/Size&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_grad/range/delta&quot;\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_grad/Fill/value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_grad/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_grad/Shape_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_grad/Fill/value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_grad/DynamicStitch&quot;\\n  op: &quot;DynamicStitch&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_grad/range&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_grad/Fill&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_grad/Maximum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_grad/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_grad/DynamicStitch&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_grad/Maximum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_grad/floordiv&quot;\\n  op: &quot;FloorDiv&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_grad/Maximum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Softmax_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_grad/DynamicStitch&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_grad/floordiv&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_grad/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 128\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_grad/Tile&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_v/read&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Sum_grad/Tile&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/attention/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/attention/mul_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/attention/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_2/attention/mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/attention/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_2/attention/mul_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Tanh_grad/TanhGrad&quot;\\n  op: &quot;TanhGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/Tanh&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/add_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/Conv2D&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/add_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/Reshape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/add_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/add_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/add_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Tanh_grad/TanhGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/add_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/add_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/add_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/add_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/add_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Tanh_grad/TanhGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/add_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/add_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/add_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/add_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/attention/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/attention/add_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/add_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/attention/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_2/attention/add_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/add_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/add_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/attention/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_2/attention/add_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Conv2D_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n  op: &quot;Conv2DBackpropInput&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Conv2D_grad/Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_w/read&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/add_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;SAME&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Conv2D_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n  op: &quot;Conv2DBackpropFilter&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Conv2D_grad/Shape_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/add_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;SAME&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Conv2D_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Conv2D_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/attention/Conv2D_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_2/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Conv2D_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/attention/Conv2D_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_2/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Reshape_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Reshape_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/add_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Reshape_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_65&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_1_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Conv2D_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_2/attention/mul_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Reshape_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Reshape_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/AddN_65&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Reshape_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Reshape_1_grad/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/attention/Reshape_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/attention/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Reshape_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/attention/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_2/attention/Reshape_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/attention/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_2/attention/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_66&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Slice_grad/Pad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Reshape_grad/Reshape&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_2/attention/Slice_grad/Pad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/Reshape_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/Reshape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/Reshape_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/AddN_66&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/Reshape_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/attention/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/attention/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/concat&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/attention/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/attention/attention/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/attention/attention/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/attention/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/attention/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/attention/attention/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_2/attention/attention/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/attention/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/attention/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/attention/attention/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_2/attention/attention/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/Reshape_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/concat_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/Reshape_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/Reshape_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/Reshape_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/concat/axis&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/concat_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/attention/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/attention/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/concat_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_2/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_2/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/concat_1_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 3\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/concat_1_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/concat_1/axis&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/concat_1_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/concat_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/Slice&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/concat_1_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/Slice&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/ExpandDims&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/concat_1_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/concat_1_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/concat_1_grad/ShapeN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/concat_1_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/concat_1_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/Reshape_1_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/concat_1_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/concat_1_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/concat_1_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/Reshape_1_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/concat_1_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/concat_1_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/concat_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/concat_1_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/concat_1_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/concat_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/concat_1_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/concat_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_1/concat_1_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/concat_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/concat_1_grad/Slice_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/concat_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_1/concat_1_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_67&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/concat_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 3\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/Tanh_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/Sigmoid_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/AddN_67&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/Sigmoid_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/Tanh_1&quot;\\n  input: &quot;gradients/AddN_67&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Slice_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 3\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Slice_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/Slice&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Slice_grad/stack/1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Slice_grad/stack&quot;\\n  op: &quot;Pack&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Slice_grad/Rank&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Slice_grad/stack/1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;axis&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Slice_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/Slice/begin&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Slice_grad/stack&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Slice_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Slice_grad/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Slice_grad/Shape_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Slice_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Slice_grad/sub_1&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Slice_grad/sub&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/Slice/begin&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Slice_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Slice_grad/sub_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Slice_grad/stack&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Slice_grad/concat/axis&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Slice_grad/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Slice_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Slice_grad/Reshape_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Slice_grad/concat/axis&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Slice_grad/Pad&quot;\\n  op: &quot;Pad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/concat_1_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Slice_grad/concat&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tpaddings&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/ExpandDims_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attn_output_projection/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/ExpandDims_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/concat_1_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/ExpandDims_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/Tanh_1_grad/TanhGrad&quot;\\n  op: &quot;TanhGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/Tanh_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/Sigmoid_2_grad/SigmoidGrad&quot;\\n  op: &quot;SigmoidGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/Sigmoid_2&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_2_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/ExpandDims_grad/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/ExpandDims_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/ExpandDims_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_1/ExpandDims_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_68&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/Tanh_1_grad/TanhGrad&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 3\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/AddN_68&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/AddN_68&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/Sigmoid&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/tuple/control_dependency&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/Sigmoid&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/Sigmoid_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/tuple/control_dependency_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/Sigmoid_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_1_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat/axis&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/Reshape_3&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/Sigmoid_grad/SigmoidGrad&quot;\\n  op: &quot;SigmoidGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/Sigmoid&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/Sigmoid_1_grad/SigmoidGrad&quot;\\n  op: &quot;SigmoidGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/Sigmoid_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/Tanh_grad/TanhGrad&quot;\\n  op: &quot;TanhGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/Tanh&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_1_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/split:2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/Sigmoid_grad/SigmoidGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/Sigmoid_grad/SigmoidGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/split_grad/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/Sigmoid_1_grad/SigmoidGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/Tanh_grad/TanhGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/add_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/Sigmoid_2_grad/SigmoidGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/split/split_dim&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 4\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/split_grad/concat&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/split_grad/concat&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/split_grad/concat&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/split_grad/concat&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat/axis&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_2/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention_cell_wrapper/concat&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention_cell_wrapper/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention_cell_wrapper/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/attention_cell_wrapper/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_2/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention_cell_wrapper/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/attention_cell_wrapper/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_2/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_2/attention_cell_wrapper/concat/axis&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/strided_slice_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Encoder/strided_slice_2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/Reshape_3&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention_cell_wrapper/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention_cell_wrapper/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_69&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention_cell_wrapper/concat_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Reshape_3_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/Sum_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Reshape_3_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/AddN_69&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Reshape_3_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/mul_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_1_grad/Size&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 4\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_1_grad/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/Sum_1/reduction_indices&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_1_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_1_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_1_grad/add&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_1_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_1_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_1_grad/range/start&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_1_grad/range/delta&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_1_grad/range&quot;\\n  op: &quot;Range&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_1_grad/range/start&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_1_grad/Size&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_1_grad/range/delta&quot;\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_1_grad/Fill/value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_1_grad/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_1_grad/Shape_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_1_grad/Fill/value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_1_grad/DynamicStitch&quot;\\n  op: &quot;DynamicStitch&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_1_grad/range&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_1_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_1_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_1_grad/Fill&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_1_grad/Maximum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_1_grad/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_1_grad/DynamicStitch&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_1_grad/Maximum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_1_grad/floordiv&quot;\\n  op: &quot;FloorDiv&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_1_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_1_grad/Maximum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Reshape_3_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_1_grad/DynamicStitch&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_1_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_1_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_1_grad/floordiv&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/Reshape_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_1_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_1_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_1_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_1_grad/Tile&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_1_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_1_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_1_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/Reshape_2&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_1_grad/Tile&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_1_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_1_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/attention/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/attention/mul_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/attention/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_1/attention/mul_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_1_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/attention/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_1/attention/mul_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Reshape_2_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/Softmax&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Reshape_2_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_1_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Reshape_2_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Softmax_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Reshape_2_grad/Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/Softmax&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Softmax_grad/Sum/reduction_indices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Softmax_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Softmax_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Softmax_grad/Sum/reduction_indices&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Softmax_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Softmax_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Softmax_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Softmax_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Softmax_grad/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Reshape_2_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Softmax_grad/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Softmax_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Softmax_grad/sub&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/Softmax&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_grad/Size&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 4\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_grad/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/Sum/reduction_indices&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_grad/add&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_grad/range/start&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_grad/range/delta&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_grad/range&quot;\\n  op: &quot;Range&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_grad/range/start&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_grad/Size&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_grad/range/delta&quot;\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_grad/Fill/value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_grad/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_grad/Shape_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_grad/Fill/value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_grad/DynamicStitch&quot;\\n  op: &quot;DynamicStitch&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_grad/range&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_grad/Fill&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_grad/Maximum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_grad/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_grad/DynamicStitch&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_grad/Maximum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_grad/floordiv&quot;\\n  op: &quot;FloorDiv&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_grad/Maximum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Softmax_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_grad/DynamicStitch&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_grad/floordiv&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_grad/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 128\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_grad/Tile&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_v/read&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Sum_grad/Tile&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/attention/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/attention/mul_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/attention/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_1/attention/mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/attention/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_1/attention/mul_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Tanh_grad/TanhGrad&quot;\\n  op: &quot;TanhGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/Tanh&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/add_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/Conv2D&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/add_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/Reshape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/add_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/add_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/add_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Tanh_grad/TanhGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/add_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/add_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/add_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/add_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/add_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Tanh_grad/TanhGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/add_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/add_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/add_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/add_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/attention/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/attention/add_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/add_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/attention/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_1/attention/add_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/add_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/add_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/attention/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_1/attention/add_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Conv2D_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n  op: &quot;Conv2DBackpropInput&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Conv2D_grad/Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_w/read&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/add_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;SAME&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Conv2D_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n  op: &quot;Conv2DBackpropFilter&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Conv2D_grad/Shape_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/add_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;SAME&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Conv2D_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Conv2D_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/attention/Conv2D_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_1/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Conv2D_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/attention/Conv2D_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_1/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Reshape_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Reshape_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/add_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Reshape_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_70&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_1_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Conv2D_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_1/attention/mul_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Reshape_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Reshape_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/AddN_70&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Reshape_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Reshape_1_grad/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/attention/Reshape_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/attention/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Reshape_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/attention/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_1/attention/Reshape_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/attention/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_1/attention/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_71&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Slice_grad/Pad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Reshape_grad/Reshape&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_1/attention/Slice_grad/Pad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/Reshape_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/Reshape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/Reshape_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/AddN_71&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/Reshape_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/attention/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/attention/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/concat&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/attention/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/attention/attention/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/attention/attention/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/attention/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/attention/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/attention/attention/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_1/attention/attention/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/attention/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/attention/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/attention/attention/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_1/attention/attention/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/Reshape_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/concat_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/Reshape_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/Reshape_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/Reshape_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/concat/axis&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/concat_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/attention/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/attention/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/concat_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_1/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_1/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/concat_1_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 3\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/concat_1_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/concat_1/axis&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/concat_1_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/concat_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/Slice&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/concat_1_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/Slice&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/ExpandDims&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/concat_1_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/concat_1_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/concat_1_grad/ShapeN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/concat_1_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/concat_1_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/Reshape_1_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/concat_1_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/concat_1_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/concat_1_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/Reshape_1_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/concat_1_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/concat_1_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/concat_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/concat_1_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/concat_1_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/concat_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/concat_1_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/concat_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper/concat_1_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/concat_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/concat_1_grad/Slice_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/concat_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper/concat_1_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_72&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/concat_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 3\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/Tanh_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/Sigmoid_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/AddN_72&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/Sigmoid_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/Tanh_1&quot;\\n  input: &quot;gradients/AddN_72&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/ExpandDims_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/ExpandDims_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/concat_1_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/ExpandDims_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/Tanh_1_grad/TanhGrad&quot;\\n  op: &quot;TanhGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/Tanh_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/Sigmoid_2_grad/SigmoidGrad&quot;\\n  op: &quot;SigmoidGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/Sigmoid_2&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_2_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/ExpandDims_grad/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attn_output_projection/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/ExpandDims_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attn_output_projection/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/ExpandDims_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/attn_output_projection/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper/ExpandDims_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attn_output_projection/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/attn_output_projection/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_73&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/Tanh_1_grad/TanhGrad&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 3\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/AddN_73&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/AddN_73&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attn_output_projection/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attn_output_projection/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_74&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attn_output_projection/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 9\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/Sigmoid&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/tuple/control_dependency&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/Sigmoid&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/add_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/Sigmoid_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/tuple/control_dependency_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/Sigmoid_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_1_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat/axis&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/mul_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/mul_2&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/Reshape_3&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_75&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/MatMul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 9\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/attn_output_projection/attn_output_projection/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/Sigmoid_grad/SigmoidGrad&quot;\\n  op: &quot;SigmoidGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/Sigmoid&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/Sigmoid_1_grad/SigmoidGrad&quot;\\n  op: &quot;SigmoidGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/Sigmoid_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/Tanh_grad/TanhGrad&quot;\\n  op: &quot;TanhGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/Tanh&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_1_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/split:2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/Sigmoid_grad/SigmoidGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/Sigmoid_grad/SigmoidGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/split_grad/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/Sigmoid_1_grad/SigmoidGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/Tanh_grad/TanhGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/add_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/Sigmoid_2_grad/SigmoidGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/split/split_dim&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 4\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/split_grad/concat&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/split_grad/concat&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/split_grad/concat&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/split_grad/concat&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat/axis&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/BiasAdd&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/mul_2&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_1/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention_cell_wrapper/concat&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention_cell_wrapper/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention_cell_wrapper/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/attention_cell_wrapper/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_1/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention_cell_wrapper/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/attention_cell_wrapper/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_1/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Encoder/attention_cell_wrapper_1/attention_cell_wrapper/concat/axis&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/strided_slice_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Encoder/strided_slice_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/Reshape_3&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention_cell_wrapper/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention_cell_wrapper/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_76&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention_cell_wrapper/concat_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Reshape_3_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/Sum_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Reshape_3_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/AddN_76&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Reshape_3_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/mul_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_1_grad/Size&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 4\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_1_grad/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/Sum_1/reduction_indices&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_1_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_1_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_1_grad/add&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_1_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_1_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_1_grad/range/start&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_1_grad/range/delta&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_1_grad/range&quot;\\n  op: &quot;Range&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_1_grad/range/start&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_1_grad/Size&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_1_grad/range/delta&quot;\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_1_grad/Fill/value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_1_grad/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_1_grad/Shape_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_1_grad/Fill/value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_1_grad/DynamicStitch&quot;\\n  op: &quot;DynamicStitch&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_1_grad/range&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_1_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_1_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_1_grad/Fill&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_1_grad/Maximum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_1_grad/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_1_grad/DynamicStitch&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_1_grad/Maximum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_1_grad/floordiv&quot;\\n  op: &quot;FloorDiv&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_1_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_1_grad/Maximum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Reshape_3_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_1_grad/DynamicStitch&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_1_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_1_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_1_grad/floordiv&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/mul_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/Reshape_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/mul_1_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/mul_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/mul_1_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/mul_1_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_1_grad/Tile&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/mul_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/mul_1_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/mul_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/mul_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/mul_1_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/mul_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/mul_1_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/Reshape_2&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_1_grad/Tile&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/mul_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/mul_1_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/mul_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/mul_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/mul_1_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/mul_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/attention/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/attention/mul_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/mul_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/attention/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper/attention/mul_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/mul_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/mul_1_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/attention/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper/attention/mul_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Reshape_2_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/Softmax&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Reshape_2_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/mul_1_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Reshape_2_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Softmax_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Reshape_2_grad/Reshape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/Softmax&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Softmax_grad/Sum/reduction_indices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Softmax_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Softmax_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Softmax_grad/Sum/reduction_indices&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Softmax_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Softmax_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Softmax_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Softmax_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Softmax_grad/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Reshape_2_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Softmax_grad/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Softmax_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Softmax_grad/sub&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/Softmax&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_grad/Size&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 4\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_grad/add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/Sum/reduction_indices&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_grad/add&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_grad/Size&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_grad/range/start&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_grad/range/delta&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_grad/range&quot;\\n  op: &quot;Range&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_grad/range/start&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_grad/Size&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_grad/range/delta&quot;\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_grad/Fill/value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_grad/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_grad/Shape_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_grad/Fill/value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_grad/DynamicStitch&quot;\\n  op: &quot;DynamicStitch&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_grad/range&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_grad/Fill&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_grad/Maximum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_grad/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_grad/DynamicStitch&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_grad/Maximum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_grad/floordiv&quot;\\n  op: &quot;FloorDiv&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_grad/Maximum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Softmax_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_grad/DynamicStitch&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_grad/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_grad/floordiv&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/mul_grad/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 128\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/mul_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/mul_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/mul_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/mul_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_grad/Tile&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/mul_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/mul_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/mul_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/mul_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/mul_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/mul_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/mul_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_v/read&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Sum_grad/Tile&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/mul_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/mul_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/mul_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/mul_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/mul_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/mul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/attention/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/attention/mul_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/mul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/attention/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper/attention/mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/mul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/mul_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/attention/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper/attention/mul_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_77&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/mul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/mul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/mul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/mul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/mul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/mul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/mul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/mul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/mul_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 9\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/attention/mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Tanh_grad/TanhGrad&quot;\\n  op: &quot;TanhGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/Tanh&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/mul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/add_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/Conv2D&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/add_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/Reshape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/add_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/add_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/add_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Tanh_grad/TanhGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/add_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/add_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/add_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/add_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/add_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Tanh_grad/TanhGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/add_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/add_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/add_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/add_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/attention/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/attention/add_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/add_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/attention/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper/attention/add_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/add_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/add_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/attention/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper/attention/add_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Conv2D_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n  op: &quot;Conv2DBackpropInput&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Conv2D_grad/Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_w/read&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/add_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;SAME&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Conv2D_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n  op: &quot;Conv2DBackpropFilter&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Conv2D_grad/Shape_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/add_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;padding&quot;\\n    value {\\n      s: &quot;SAME&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;strides&quot;\\n    value {\\n      list {\\n        i: 1\\n        i: 1\\n        i: 1\\n        i: 1\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_cudnn_on_gpu&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Conv2D_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Conv2D_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/attention/Conv2D_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper/attention/Conv2D_grad/Conv2DBackpropInput&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Conv2D_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/attention/Conv2D_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Reshape_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/Reshape_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/add_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Reshape_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_78&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/Conv2D_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/Conv2D_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/Conv2D_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/Conv2D_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/Conv2D_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/Conv2D_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/Conv2D_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/Conv2D_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Conv2D_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 9\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/attention/Conv2D_grad/Conv2DBackpropFilter&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Reshape_1_grad/Reshape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/attention/Reshape_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/attention/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/Reshape_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/attention/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper/attention/Reshape_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/attention/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper/attention/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/attention/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/attention/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/concat&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/attention/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/attention/attention/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/attention/attention/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/attention/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/attention/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/attention/attention/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper/attention/attention/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention/attention/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/attention/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/attention/attention/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper/attention/attention/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_79&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 9\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/attention/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/concat/axis&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/add_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/add_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/mul_2&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/concat_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/attention/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/attention/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_80&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention/attention/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention/attention/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention/attention/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention/attention/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention/attention/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention/attention/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention/attention/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention/attention/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention/attention/MatMul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 9\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/attention/attention/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_81&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/concat_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 3\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper/attn_output_projection/attn_output_projection/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/Tanh_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/Sigmoid_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/AddN_81&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/Sigmoid_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/Tanh_1&quot;\\n  input: &quot;gradients/AddN_81&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/Tanh_1_grad/TanhGrad&quot;\\n  op: &quot;TanhGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/Tanh_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/Sigmoid_2_grad/SigmoidGrad&quot;\\n  op: &quot;SigmoidGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/Sigmoid_2&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_2_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_82&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/Tanh_1_grad/TanhGrad&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 3\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/mul_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/AddN_82&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/AddN_82&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/Sigmoid&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/tuple/control_dependency&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/Sigmoid&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;zeros&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/Sigmoid_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/tuple/control_dependency_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/mul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/Sigmoid_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_1_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/mul_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/Sigmoid_grad/SigmoidGrad&quot;\\n  op: &quot;SigmoidGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/Sigmoid&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/Sigmoid_1_grad/SigmoidGrad&quot;\\n  op: &quot;SigmoidGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/Sigmoid_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/Tanh_grad/TanhGrad&quot;\\n  op: &quot;TanhGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/Tanh&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/mul_1_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/split:2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_grad/Shape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/Sigmoid_grad/SigmoidGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_grad/Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/Sigmoid_grad/SigmoidGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_grad/Sum_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_grad/Reshape&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_grad/Reshape_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/split_grad/concat&quot;\\n  op: &quot;ConcatV2&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/Sigmoid_1_grad/SigmoidGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/Tanh_grad/TanhGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/add_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/Sigmoid_2_grad/SigmoidGrad&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/split/split_dim&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 4\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/split_grad/concat&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/split_grad/concat&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/split_grad/concat&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/split_grad/concat&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_83&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 9\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/Rank&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/mod&quot;\\n  op: &quot;FloorMod&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat/axis&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/Rank&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN&quot;\\n  op: &quot;ShapeN&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/BiasAdd&quot;\\n  input: &quot;zeros_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/ConcatOffset&quot;\\n  op: &quot;ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/mod&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/ConcatOffset&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n  op: &quot;Slice&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/ConcatOffset:1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/ShapeN:1&quot;\\n  attr {\\n    key: &quot;Index&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_84&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/MatMul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 9\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/basic_lstm_cell/basic_lstm_cell/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/tuple/control_dependency&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper/basic_lstm_cell/basic_lstm_cell/concat_grad/Slice&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention_cell_wrapper/concat&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention_cell_wrapper/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention_cell_wrapper/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/attention_cell_wrapper/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper/attention_cell_wrapper/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Encoder/attention_cell_wrapper/attention_cell_wrapper/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/Encoder/attention_cell_wrapper/attention_cell_wrapper/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_85&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 9\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/AddN_86&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_8/attention_cell_wrapper/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_7/attention_cell_wrapper/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_6/attention_cell_wrapper/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_5/attention_cell_wrapper/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_4/attention_cell_wrapper/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_3/attention_cell_wrapper/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_2/attention_cell_wrapper/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper_1/attention_cell_wrapper/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;gradients/Encoder/attention_cell_wrapper/attention_cell_wrapper/MatMul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 9\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Encoder/attention_cell_wrapper_8/attention_cell_wrapper/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;beta1_power/initial_value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.8999999761581421\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;beta1_power&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;beta1_power/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;beta1_power&quot;\\n  input: &quot;beta1_power/initial_value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;beta1_power/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;beta1_power&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;beta2_power/initial_value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.9990000128746033\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;beta2_power&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;beta2_power/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;beta2_power&quot;\\n  input: &quot;beta2_power/initial_value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;beta2_power/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;beta2_power&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros_5&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 140\\n          }\\n          dim {\\n            size: 12\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/weights/Adam&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 140\\n        }\\n        dim {\\n          size: 12\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/weights/Adam/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/weights/Adam&quot;\\n  input: &quot;zeros_5&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/weights/Adam/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/weights/Adam&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros_6&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 140\\n          }\\n          dim {\\n            size: 12\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/weights/Adam_1&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 140\\n        }\\n        dim {\\n          size: 12\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/weights/Adam_1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/weights/Adam_1&quot;\\n  input: &quot;zeros_6&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/weights/Adam_1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/weights/Adam_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros_7&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 12\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/biases/Adam&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 12\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/biases/Adam/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/biases/Adam&quot;\\n  input: &quot;zeros_7&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/biases/Adam/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/biases/Adam&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/biases&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros_8&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 12\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/biases/Adam_1&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 12\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/biases/Adam_1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/biases/Adam_1&quot;\\n  input: &quot;zeros_8&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/biases/Adam_1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/biases/Adam_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/biases&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros_9&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 140\\n          }\\n          dim {\\n            size: 512\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 140\\n        }\\n        dim {\\n          size: 512\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam&quot;\\n  input: &quot;zeros_9&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros_10&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 140\\n          }\\n          dim {\\n            size: 512\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam_1&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 140\\n        }\\n        dim {\\n          size: 512\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam_1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam_1&quot;\\n  input: &quot;zeros_10&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam_1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros_11&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 512\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/basic_lstm_cell/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 512\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam&quot;\\n  input: &quot;zeros_11&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/basic_lstm_cell/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/basic_lstm_cell/biases&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros_12&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 512\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam_1&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/basic_lstm_cell/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 512\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam_1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam_1&quot;\\n  input: &quot;zeros_12&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/basic_lstm_cell/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam_1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/basic_lstm_cell/biases&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros_13&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n          dim {\\n            size: 1\\n          }\\n          dim {\\n            size: 128\\n          }\\n          dim {\\n            size: 128\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/attn_w/Adam&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/attn_w&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 1\\n        }\\n        dim {\\n          size: 1\\n        }\\n        dim {\\n          size: 128\\n        }\\n        dim {\\n          size: 128\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/attn_w/Adam/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_w/Adam&quot;\\n  input: &quot;zeros_13&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/attn_w&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/attn_w/Adam/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_w/Adam&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/attn_w&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros_14&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n          dim {\\n            size: 1\\n          }\\n          dim {\\n            size: 128\\n          }\\n          dim {\\n            size: 128\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/attn_w/Adam_1&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/attn_w&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 1\\n        }\\n        dim {\\n          size: 1\\n        }\\n        dim {\\n          size: 128\\n        }\\n        dim {\\n          size: 128\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/attn_w/Adam_1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_w/Adam_1&quot;\\n  input: &quot;zeros_14&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/attn_w&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/attn_w/Adam_1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_w/Adam_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/attn_w&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros_15&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 128\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/attn_v/Adam&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/attn_v&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 128\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/attn_v/Adam/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_v/Adam&quot;\\n  input: &quot;zeros_15&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/attn_v&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/attn_v/Adam/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_v/Adam&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/attn_v&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros_16&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 128\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/attn_v/Adam_1&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/attn_v&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 128\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/attn_v/Adam_1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_v/Adam_1&quot;\\n  input: &quot;zeros_16&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/attn_v&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/attn_v/Adam_1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_v/Adam_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/attn_v&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros_17&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 256\\n          }\\n          dim {\\n            size: 128\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/weights/Adam&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 256\\n        }\\n        dim {\\n          size: 128\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/weights/Adam/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/weights/Adam&quot;\\n  input: &quot;zeros_17&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/weights/Adam/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/weights/Adam&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros_18&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 256\\n          }\\n          dim {\\n            size: 128\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/weights/Adam_1&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 256\\n        }\\n        dim {\\n          size: 128\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/weights/Adam_1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/weights/Adam_1&quot;\\n  input: &quot;zeros_18&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/weights/Adam_1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/weights/Adam_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros_19&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 128\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/biases/Adam&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 128\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/biases/Adam/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/biases/Adam&quot;\\n  input: &quot;zeros_19&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/biases/Adam/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/biases/Adam&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/biases&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros_20&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 128\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/biases/Adam_1&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 128\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/biases/Adam_1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/biases/Adam_1&quot;\\n  input: &quot;zeros_20&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attention/biases/Adam_1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/biases/Adam_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/biases&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros_21&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 256\\n          }\\n          dim {\\n            size: 128\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/Adam&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 256\\n        }\\n        dim {\\n          size: 128\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/Adam/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/Adam&quot;\\n  input: &quot;zeros_21&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/Adam/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/Adam&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros_22&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 256\\n          }\\n          dim {\\n            size: 128\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/Adam_1&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 256\\n        }\\n        dim {\\n          size: 128\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/Adam_1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/Adam_1&quot;\\n  input: &quot;zeros_22&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/Adam_1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/Adam_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros_23&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 128\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attn_output_projection/biases/Adam&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attn_output_projection/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 128\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attn_output_projection/biases/Adam/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/biases/Adam&quot;\\n  input: &quot;zeros_23&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attn_output_projection/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attn_output_projection/biases/Adam/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/biases/Adam&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attn_output_projection/biases&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros_24&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 128\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attn_output_projection/biases/Adam_1&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attn_output_projection/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 128\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attn_output_projection/biases/Adam_1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/biases/Adam_1&quot;\\n  input: &quot;zeros_24&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attn_output_projection/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Encoder/attention_cell_wrapper/attn_output_projection/biases/Adam_1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/biases/Adam_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attn_output_projection/biases&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros_25&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 128\\n          }\\n          dim {\\n            size: 12\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/Adam&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Variable&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 128\\n        }\\n        dim {\\n          size: 12\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/Adam/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Variable/Adam&quot;\\n  input: &quot;zeros_25&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Variable&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/Adam/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Variable/Adam&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Variable&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros_26&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 128\\n          }\\n          dim {\\n            size: 12\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/Adam_1&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Variable&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 128\\n        }\\n        dim {\\n          size: 12\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/Adam_1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Variable/Adam_1&quot;\\n  input: &quot;zeros_26&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Variable&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/Adam_1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Variable/Adam_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Variable&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros_27&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 12\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable_1/Adam&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Variable_1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 12\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable_1/Adam/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Variable_1/Adam&quot;\\n  input: &quot;zeros_27&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Variable_1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable_1/Adam/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Variable_1/Adam&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Variable_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros_28&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 12\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable_1/Adam_1&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Variable_1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 12\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable_1/Adam_1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Variable_1/Adam_1&quot;\\n  input: &quot;zeros_28&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Variable_1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable_1/Adam_1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Variable_1/Adam_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Variable_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros_29&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 133\\n          }\\n          dim {\\n            size: 5\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/weights/Adam&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 133\\n        }\\n        dim {\\n          size: 5\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/weights/Adam/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/weights/Adam&quot;\\n  input: &quot;zeros_29&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/weights/Adam/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/weights/Adam&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros_30&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 133\\n          }\\n          dim {\\n            size: 5\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/weights/Adam_1&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 133\\n        }\\n        dim {\\n          size: 5\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/weights/Adam_1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/weights/Adam_1&quot;\\n  input: &quot;zeros_30&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/weights/Adam_1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/weights/Adam_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros_31&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 5\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/biases/Adam&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 5\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/biases/Adam/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/biases/Adam&quot;\\n  input: &quot;zeros_31&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/biases/Adam/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/biases/Adam&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/biases&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros_32&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 5\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/biases/Adam_1&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 5\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/biases/Adam_1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/biases/Adam_1&quot;\\n  input: &quot;zeros_32&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/biases/Adam_1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/biases/Adam_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/biases&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros_33&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 133\\n          }\\n          dim {\\n            size: 512\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 133\\n        }\\n        dim {\\n          size: 512\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam&quot;\\n  input: &quot;zeros_33&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros_34&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 133\\n          }\\n          dim {\\n            size: 512\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam_1&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 133\\n        }\\n        dim {\\n          size: 512\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam_1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam_1&quot;\\n  input: &quot;zeros_34&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam_1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros_35&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 512\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/basic_lstm_cell/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 512\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam&quot;\\n  input: &quot;zeros_35&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/basic_lstm_cell/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/basic_lstm_cell/biases&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros_36&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 512\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam_1&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/basic_lstm_cell/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 512\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam_1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam_1&quot;\\n  input: &quot;zeros_36&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/basic_lstm_cell/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam_1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/basic_lstm_cell/biases&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros_37&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n          dim {\\n            size: 1\\n          }\\n          dim {\\n            size: 128\\n          }\\n          dim {\\n            size: 128\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/attn_w/Adam&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/attn_w&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 1\\n        }\\n        dim {\\n          size: 1\\n        }\\n        dim {\\n          size: 128\\n        }\\n        dim {\\n          size: 128\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/attn_w/Adam/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_w/Adam&quot;\\n  input: &quot;zeros_37&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/attn_w&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/attn_w/Adam/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_w/Adam&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/attn_w&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros_38&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n          dim {\\n            size: 1\\n          }\\n          dim {\\n            size: 128\\n          }\\n          dim {\\n            size: 128\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/attn_w/Adam_1&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/attn_w&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 1\\n        }\\n        dim {\\n          size: 1\\n        }\\n        dim {\\n          size: 128\\n        }\\n        dim {\\n          size: 128\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/attn_w/Adam_1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_w/Adam_1&quot;\\n  input: &quot;zeros_38&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/attn_w&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/attn_w/Adam_1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_w/Adam_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/attn_w&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros_39&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 128\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/attn_v/Adam&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/attn_v&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 128\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/attn_v/Adam/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_v/Adam&quot;\\n  input: &quot;zeros_39&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/attn_v&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/attn_v/Adam/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_v/Adam&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/attn_v&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros_40&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 128\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/attn_v/Adam_1&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/attn_v&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 128\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/attn_v/Adam_1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_v/Adam_1&quot;\\n  input: &quot;zeros_40&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/attn_v&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/attn_v/Adam_1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_v/Adam_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/attn_v&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros_41&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 256\\n          }\\n          dim {\\n            size: 128\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/weights/Adam&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 256\\n        }\\n        dim {\\n          size: 128\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/weights/Adam/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/weights/Adam&quot;\\n  input: &quot;zeros_41&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/weights/Adam/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/weights/Adam&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros_42&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 256\\n          }\\n          dim {\\n            size: 128\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/weights/Adam_1&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 256\\n        }\\n        dim {\\n          size: 128\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/weights/Adam_1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/weights/Adam_1&quot;\\n  input: &quot;zeros_42&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/weights/Adam_1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/weights/Adam_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros_43&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 128\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/biases/Adam&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 128\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/biases/Adam/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/biases/Adam&quot;\\n  input: &quot;zeros_43&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/biases/Adam/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/biases/Adam&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/biases&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros_44&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 128\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/biases/Adam_1&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 128\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/biases/Adam_1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/biases/Adam_1&quot;\\n  input: &quot;zeros_44&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attention/biases/Adam_1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/biases/Adam_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/biases&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros_45&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 256\\n          }\\n          dim {\\n            size: 128\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights/Adam&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 256\\n        }\\n        dim {\\n          size: 128\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights/Adam/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights/Adam&quot;\\n  input: &quot;zeros_45&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights/Adam/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights/Adam&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros_46&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 256\\n          }\\n          dim {\\n            size: 128\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights/Adam_1&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 256\\n        }\\n        dim {\\n          size: 128\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights/Adam_1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights/Adam_1&quot;\\n  input: &quot;zeros_46&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights/Adam_1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights/Adam_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros_47&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 128\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attn_output_projection/biases/Adam&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attn_output_projection/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 128\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attn_output_projection/biases/Adam/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/biases/Adam&quot;\\n  input: &quot;zeros_47&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attn_output_projection/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attn_output_projection/biases/Adam/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/biases/Adam&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attn_output_projection/biases&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;zeros_48&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 128\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attn_output_projection/biases/Adam_1&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attn_output_projection/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 128\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attn_output_projection/biases/Adam_1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/biases/Adam_1&quot;\\n  input: &quot;zeros_48&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attn_output_projection/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Decoder/attention_cell_wrapper/attn_output_projection/biases/Adam_1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/biases/Adam_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attn_output_projection/biases&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adam/learning_rate&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0010000000474974513\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adam/beta1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.8999999761581421\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adam/beta2&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.9990000128746033\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adam/epsilon&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 9.99999993922529e-09\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adam/update_Encoder/attention_cell_wrapper/weights/ApplyAdam&quot;\\n  op: &quot;ApplyAdam&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/weights&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/weights/Adam&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/weights/Adam_1&quot;\\n  input: &quot;beta1_power/read&quot;\\n  input: &quot;beta2_power/read&quot;\\n  input: &quot;Adam/learning_rate&quot;\\n  input: &quot;Adam/beta1&quot;\\n  input: &quot;Adam/beta2&quot;\\n  input: &quot;Adam/epsilon&quot;\\n  input: &quot;gradients/AddN_86&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adam/update_Encoder/attention_cell_wrapper/biases/ApplyAdam&quot;\\n  op: &quot;ApplyAdam&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/biases&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/biases/Adam&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/biases/Adam_1&quot;\\n  input: &quot;beta1_power/read&quot;\\n  input: &quot;beta2_power/read&quot;\\n  input: &quot;Adam/learning_rate&quot;\\n  input: &quot;Adam/beta1&quot;\\n  input: &quot;Adam/beta2&quot;\\n  input: &quot;Adam/epsilon&quot;\\n  input: &quot;gradients/AddN_85&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adam/update_Encoder/attention_cell_wrapper/basic_lstm_cell/weights/ApplyAdam&quot;\\n  op: &quot;ApplyAdam&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam_1&quot;\\n  input: &quot;beta1_power/read&quot;\\n  input: &quot;beta2_power/read&quot;\\n  input: &quot;Adam/learning_rate&quot;\\n  input: &quot;Adam/beta1&quot;\\n  input: &quot;Adam/beta2&quot;\\n  input: &quot;Adam/epsilon&quot;\\n  input: &quot;gradients/AddN_84&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adam/update_Encoder/attention_cell_wrapper/basic_lstm_cell/biases/ApplyAdam&quot;\\n  op: &quot;ApplyAdam&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/biases&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam_1&quot;\\n  input: &quot;beta1_power/read&quot;\\n  input: &quot;beta2_power/read&quot;\\n  input: &quot;Adam/learning_rate&quot;\\n  input: &quot;Adam/beta1&quot;\\n  input: &quot;Adam/beta2&quot;\\n  input: &quot;Adam/epsilon&quot;\\n  input: &quot;gradients/AddN_83&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/basic_lstm_cell/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adam/update_Encoder/attention_cell_wrapper/attention/attn_w/ApplyAdam&quot;\\n  op: &quot;ApplyAdam&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_w&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_w/Adam&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_w/Adam_1&quot;\\n  input: &quot;beta1_power/read&quot;\\n  input: &quot;beta2_power/read&quot;\\n  input: &quot;Adam/learning_rate&quot;\\n  input: &quot;Adam/beta1&quot;\\n  input: &quot;Adam/beta2&quot;\\n  input: &quot;Adam/epsilon&quot;\\n  input: &quot;gradients/AddN_78&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/attn_w&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adam/update_Encoder/attention_cell_wrapper/attention/attn_v/ApplyAdam&quot;\\n  op: &quot;ApplyAdam&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_v&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_v/Adam&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_v/Adam_1&quot;\\n  input: &quot;beta1_power/read&quot;\\n  input: &quot;beta2_power/read&quot;\\n  input: &quot;Adam/learning_rate&quot;\\n  input: &quot;Adam/beta1&quot;\\n  input: &quot;Adam/beta2&quot;\\n  input: &quot;Adam/epsilon&quot;\\n  input: &quot;gradients/AddN_77&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/attn_v&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adam/update_Encoder/attention_cell_wrapper/attention/weights/ApplyAdam&quot;\\n  op: &quot;ApplyAdam&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/weights&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/weights/Adam&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/weights/Adam_1&quot;\\n  input: &quot;beta1_power/read&quot;\\n  input: &quot;beta2_power/read&quot;\\n  input: &quot;Adam/learning_rate&quot;\\n  input: &quot;Adam/beta1&quot;\\n  input: &quot;Adam/beta2&quot;\\n  input: &quot;Adam/epsilon&quot;\\n  input: &quot;gradients/AddN_80&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adam/update_Encoder/attention_cell_wrapper/attention/biases/ApplyAdam&quot;\\n  op: &quot;ApplyAdam&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/biases&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/biases/Adam&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/biases/Adam_1&quot;\\n  input: &quot;beta1_power/read&quot;\\n  input: &quot;beta2_power/read&quot;\\n  input: &quot;Adam/learning_rate&quot;\\n  input: &quot;Adam/beta1&quot;\\n  input: &quot;Adam/beta2&quot;\\n  input: &quot;Adam/epsilon&quot;\\n  input: &quot;gradients/AddN_79&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adam/update_Encoder/attention_cell_wrapper/attn_output_projection/weights/ApplyAdam&quot;\\n  op: &quot;ApplyAdam&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/Adam&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/Adam_1&quot;\\n  input: &quot;beta1_power/read&quot;\\n  input: &quot;beta2_power/read&quot;\\n  input: &quot;Adam/learning_rate&quot;\\n  input: &quot;Adam/beta1&quot;\\n  input: &quot;Adam/beta2&quot;\\n  input: &quot;Adam/epsilon&quot;\\n  input: &quot;gradients/AddN_75&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adam/update_Encoder/attention_cell_wrapper/attn_output_projection/biases/ApplyAdam&quot;\\n  op: &quot;ApplyAdam&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/biases&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/biases/Adam&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/biases/Adam_1&quot;\\n  input: &quot;beta1_power/read&quot;\\n  input: &quot;beta2_power/read&quot;\\n  input: &quot;Adam/learning_rate&quot;\\n  input: &quot;Adam/beta1&quot;\\n  input: &quot;Adam/beta2&quot;\\n  input: &quot;Adam/epsilon&quot;\\n  input: &quot;gradients/AddN_74&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attn_output_projection/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adam/update_Variable/ApplyAdam&quot;\\n  op: &quot;ApplyAdam&quot;\\n  input: &quot;Variable&quot;\\n  input: &quot;Variable/Adam&quot;\\n  input: &quot;Variable/Adam_1&quot;\\n  input: &quot;beta1_power/read&quot;\\n  input: &quot;beta2_power/read&quot;\\n  input: &quot;Adam/learning_rate&quot;\\n  input: &quot;Adam/beta1&quot;\\n  input: &quot;Adam/beta2&quot;\\n  input: &quot;Adam/epsilon&quot;\\n  input: &quot;gradients/AddN_33&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Variable&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adam/update_Variable_1/ApplyAdam&quot;\\n  op: &quot;ApplyAdam&quot;\\n  input: &quot;Variable_1&quot;\\n  input: &quot;Variable_1/Adam&quot;\\n  input: &quot;Variable_1/Adam_1&quot;\\n  input: &quot;beta1_power/read&quot;\\n  input: &quot;beta2_power/read&quot;\\n  input: &quot;Adam/learning_rate&quot;\\n  input: &quot;Adam/beta1&quot;\\n  input: &quot;Adam/beta2&quot;\\n  input: &quot;Adam/epsilon&quot;\\n  input: &quot;gradients/AddN_31&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Variable_1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adam/update_Decoder/attention_cell_wrapper/weights/ApplyAdam&quot;\\n  op: &quot;ApplyAdam&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/weights&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/weights/Adam&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/weights/Adam_1&quot;\\n  input: &quot;beta1_power/read&quot;\\n  input: &quot;beta2_power/read&quot;\\n  input: &quot;Adam/learning_rate&quot;\\n  input: &quot;Adam/beta1&quot;\\n  input: &quot;Adam/beta2&quot;\\n  input: &quot;Adam/epsilon&quot;\\n  input: &quot;gradients/AddN_30&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adam/update_Decoder/attention_cell_wrapper/biases/ApplyAdam&quot;\\n  op: &quot;ApplyAdam&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/biases&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/biases/Adam&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/biases/Adam_1&quot;\\n  input: &quot;beta1_power/read&quot;\\n  input: &quot;beta2_power/read&quot;\\n  input: &quot;Adam/learning_rate&quot;\\n  input: &quot;Adam/beta1&quot;\\n  input: &quot;Adam/beta2&quot;\\n  input: &quot;Adam/epsilon&quot;\\n  input: &quot;gradients/AddN_29&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adam/update_Decoder/attention_cell_wrapper/basic_lstm_cell/weights/ApplyAdam&quot;\\n  op: &quot;ApplyAdam&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam_1&quot;\\n  input: &quot;beta1_power/read&quot;\\n  input: &quot;beta2_power/read&quot;\\n  input: &quot;Adam/learning_rate&quot;\\n  input: &quot;Adam/beta1&quot;\\n  input: &quot;Adam/beta2&quot;\\n  input: &quot;Adam/epsilon&quot;\\n  input: &quot;gradients/AddN_28&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adam/update_Decoder/attention_cell_wrapper/basic_lstm_cell/biases/ApplyAdam&quot;\\n  op: &quot;ApplyAdam&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/biases&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam_1&quot;\\n  input: &quot;beta1_power/read&quot;\\n  input: &quot;beta2_power/read&quot;\\n  input: &quot;Adam/learning_rate&quot;\\n  input: &quot;Adam/beta1&quot;\\n  input: &quot;Adam/beta2&quot;\\n  input: &quot;Adam/epsilon&quot;\\n  input: &quot;gradients/AddN_27&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/basic_lstm_cell/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adam/update_Decoder/attention_cell_wrapper/attention/attn_w/ApplyAdam&quot;\\n  op: &quot;ApplyAdam&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_w&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_w/Adam&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_w/Adam_1&quot;\\n  input: &quot;beta1_power/read&quot;\\n  input: &quot;beta2_power/read&quot;\\n  input: &quot;Adam/learning_rate&quot;\\n  input: &quot;Adam/beta1&quot;\\n  input: &quot;Adam/beta2&quot;\\n  input: &quot;Adam/epsilon&quot;\\n  input: &quot;gradients/AddN_21&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/attn_w&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adam/update_Decoder/attention_cell_wrapper/attention/attn_v/ApplyAdam&quot;\\n  op: &quot;ApplyAdam&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_v&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_v/Adam&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_v/Adam_1&quot;\\n  input: &quot;beta1_power/read&quot;\\n  input: &quot;beta2_power/read&quot;\\n  input: &quot;Adam/learning_rate&quot;\\n  input: &quot;Adam/beta1&quot;\\n  input: &quot;Adam/beta2&quot;\\n  input: &quot;Adam/epsilon&quot;\\n  input: &quot;gradients/AddN_19&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/attn_v&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adam/update_Decoder/attention_cell_wrapper/attention/weights/ApplyAdam&quot;\\n  op: &quot;ApplyAdam&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/weights&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/weights/Adam&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/weights/Adam_1&quot;\\n  input: &quot;beta1_power/read&quot;\\n  input: &quot;beta2_power/read&quot;\\n  input: &quot;Adam/learning_rate&quot;\\n  input: &quot;Adam/beta1&quot;\\n  input: &quot;Adam/beta2&quot;\\n  input: &quot;Adam/epsilon&quot;\\n  input: &quot;gradients/AddN_24&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adam/update_Decoder/attention_cell_wrapper/attention/biases/ApplyAdam&quot;\\n  op: &quot;ApplyAdam&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/biases&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/biases/Adam&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/biases/Adam_1&quot;\\n  input: &quot;beta1_power/read&quot;\\n  input: &quot;beta2_power/read&quot;\\n  input: &quot;Adam/learning_rate&quot;\\n  input: &quot;Adam/beta1&quot;\\n  input: &quot;Adam/beta2&quot;\\n  input: &quot;Adam/epsilon&quot;\\n  input: &quot;gradients/AddN_23&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adam/update_Decoder/attention_cell_wrapper/attn_output_projection/weights/ApplyAdam&quot;\\n  op: &quot;ApplyAdam&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights/Adam&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights/Adam_1&quot;\\n  input: &quot;beta1_power/read&quot;\\n  input: &quot;beta2_power/read&quot;\\n  input: &quot;Adam/learning_rate&quot;\\n  input: &quot;Adam/beta1&quot;\\n  input: &quot;Adam/beta2&quot;\\n  input: &quot;Adam/epsilon&quot;\\n  input: &quot;gradients/AddN_17&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adam/update_Decoder/attention_cell_wrapper/attn_output_projection/biases/ApplyAdam&quot;\\n  op: &quot;ApplyAdam&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/biases&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/biases/Adam&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/biases/Adam_1&quot;\\n  input: &quot;beta1_power/read&quot;\\n  input: &quot;beta2_power/read&quot;\\n  input: &quot;Adam/learning_rate&quot;\\n  input: &quot;Adam/beta1&quot;\\n  input: &quot;Adam/beta2&quot;\\n  input: &quot;Adam/epsilon&quot;\\n  input: &quot;gradients/AddN_16&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attn_output_projection/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adam/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;beta1_power/read&quot;\\n  input: &quot;Adam/beta1&quot;\\n  input: &quot;^Adam/update_Encoder/attention_cell_wrapper/weights/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Encoder/attention_cell_wrapper/biases/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Encoder/attention_cell_wrapper/basic_lstm_cell/weights/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Encoder/attention_cell_wrapper/basic_lstm_cell/biases/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Encoder/attention_cell_wrapper/attention/attn_w/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Encoder/attention_cell_wrapper/attention/attn_v/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Encoder/attention_cell_wrapper/attention/weights/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Encoder/attention_cell_wrapper/attention/biases/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Encoder/attention_cell_wrapper/attn_output_projection/weights/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Encoder/attention_cell_wrapper/attn_output_projection/biases/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Variable/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Variable_1/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Decoder/attention_cell_wrapper/weights/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Decoder/attention_cell_wrapper/biases/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Decoder/attention_cell_wrapper/basic_lstm_cell/weights/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Decoder/attention_cell_wrapper/basic_lstm_cell/biases/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Decoder/attention_cell_wrapper/attention/attn_w/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Decoder/attention_cell_wrapper/attention/attn_v/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Decoder/attention_cell_wrapper/attention/weights/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Decoder/attention_cell_wrapper/attention/biases/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Decoder/attention_cell_wrapper/attn_output_projection/weights/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Decoder/attention_cell_wrapper/attn_output_projection/biases/ApplyAdam&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adam/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;beta1_power&quot;\\n  input: &quot;Adam/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adam/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;beta2_power/read&quot;\\n  input: &quot;Adam/beta2&quot;\\n  input: &quot;^Adam/update_Encoder/attention_cell_wrapper/weights/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Encoder/attention_cell_wrapper/biases/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Encoder/attention_cell_wrapper/basic_lstm_cell/weights/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Encoder/attention_cell_wrapper/basic_lstm_cell/biases/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Encoder/attention_cell_wrapper/attention/attn_w/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Encoder/attention_cell_wrapper/attention/attn_v/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Encoder/attention_cell_wrapper/attention/weights/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Encoder/attention_cell_wrapper/attention/biases/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Encoder/attention_cell_wrapper/attn_output_projection/weights/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Encoder/attention_cell_wrapper/attn_output_projection/biases/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Variable/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Variable_1/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Decoder/attention_cell_wrapper/weights/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Decoder/attention_cell_wrapper/biases/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Decoder/attention_cell_wrapper/basic_lstm_cell/weights/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Decoder/attention_cell_wrapper/basic_lstm_cell/biases/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Decoder/attention_cell_wrapper/attention/attn_w/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Decoder/attention_cell_wrapper/attention/attn_v/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Decoder/attention_cell_wrapper/attention/weights/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Decoder/attention_cell_wrapper/attention/biases/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Decoder/attention_cell_wrapper/attn_output_projection/weights/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Decoder/attention_cell_wrapper/attn_output_projection/biases/ApplyAdam&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adam/Assign_1&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;beta2_power&quot;\\n  input: &quot;Adam/mul_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Adam&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^Adam/update_Encoder/attention_cell_wrapper/weights/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Encoder/attention_cell_wrapper/biases/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Encoder/attention_cell_wrapper/basic_lstm_cell/weights/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Encoder/attention_cell_wrapper/basic_lstm_cell/biases/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Encoder/attention_cell_wrapper/attention/attn_w/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Encoder/attention_cell_wrapper/attention/attn_v/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Encoder/attention_cell_wrapper/attention/weights/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Encoder/attention_cell_wrapper/attention/biases/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Encoder/attention_cell_wrapper/attn_output_projection/weights/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Encoder/attention_cell_wrapper/attn_output_projection/biases/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Variable/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Variable_1/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Decoder/attention_cell_wrapper/weights/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Decoder/attention_cell_wrapper/biases/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Decoder/attention_cell_wrapper/basic_lstm_cell/weights/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Decoder/attention_cell_wrapper/basic_lstm_cell/biases/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Decoder/attention_cell_wrapper/attention/attn_w/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Decoder/attention_cell_wrapper/attention/attn_v/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Decoder/attention_cell_wrapper/attention/weights/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Decoder/attention_cell_wrapper/attention/biases/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Decoder/attention_cell_wrapper/attn_output_projection/weights/ApplyAdam&quot;\\n  input: &quot;^Adam/update_Decoder/attention_cell_wrapper/attn_output_projection/biases/ApplyAdam&quot;\\n  input: &quot;^Adam/Assign&quot;\\n  input: &quot;^Adam/Assign_1&quot;\\n}\\nnode {\\n  name: &quot;ArgMax/dimension&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: -1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ArgMax&quot;\\n  op: &quot;ArgMax&quot;\\n  input: &quot;Reshape&quot;\\n  input: &quot;ArgMax/dimension&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ArgMax_1/dimension&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: -1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;ArgMax_1&quot;\\n  op: &quot;ArgMax&quot;\\n  input: &quot;Placeholder_1&quot;\\n  input: &quot;ArgMax_1/dimension&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Equal&quot;\\n  op: &quot;Equal&quot;\\n  input: &quot;ArgMax&quot;\\n  input: &quot;ArgMax_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Cast&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;Equal&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_BOOL\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Const_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Mean_1&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;Cast&quot;\\n  input: &quot;Const_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;init&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/weights/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/biases/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/basic_lstm_cell/weights/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/basic_lstm_cell/biases/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attention/attn_w/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attention/attn_v/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attention/weights/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attention/biases/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attn_output_projection/weights/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attn_output_projection/biases/Assign&quot;\\n  input: &quot;^Variable/Assign&quot;\\n  input: &quot;^Variable_1/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/weights/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/biases/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/basic_lstm_cell/weights/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/basic_lstm_cell/biases/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attention/attn_w/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attention/attn_v/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attention/weights/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attention/biases/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attn_output_projection/weights/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attn_output_projection/biases/Assign&quot;\\n  input: &quot;^beta1_power/Assign&quot;\\n  input: &quot;^beta2_power/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/weights/Adam/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/weights/Adam_1/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/biases/Adam/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/biases/Adam_1/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam_1/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam_1/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attention/attn_w/Adam/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attention/attn_w/Adam_1/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attention/attn_v/Adam/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attention/attn_v/Adam_1/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attention/weights/Adam/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attention/weights/Adam_1/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attention/biases/Adam/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attention/biases/Adam_1/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attn_output_projection/weights/Adam/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attn_output_projection/weights/Adam_1/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attn_output_projection/biases/Adam/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attn_output_projection/biases/Adam_1/Assign&quot;\\n  input: &quot;^Variable/Adam/Assign&quot;\\n  input: &quot;^Variable/Adam_1/Assign&quot;\\n  input: &quot;^Variable_1/Adam/Assign&quot;\\n  input: &quot;^Variable_1/Adam_1/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/weights/Adam/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/weights/Adam_1/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/biases/Adam/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/biases/Adam_1/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam_1/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam_1/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attention/attn_w/Adam/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attention/attn_w/Adam_1/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attention/attn_v/Adam/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attention/attn_v/Adam_1/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attention/weights/Adam/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attention/weights/Adam_1/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attention/biases/Adam/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attention/biases/Adam_1/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attn_output_projection/weights/Adam/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attn_output_projection/weights/Adam_1/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attn_output_projection/biases/Adam/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attn_output_projection/biases/Adam_1/Assign&quot;\\n}\\nnode {\\n  name: &quot;init_1&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/weights/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/biases/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/basic_lstm_cell/weights/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/basic_lstm_cell/biases/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attention/attn_w/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attention/attn_v/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attention/weights/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attention/biases/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attn_output_projection/weights/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attn_output_projection/biases/Assign&quot;\\n  input: &quot;^Variable/Assign&quot;\\n  input: &quot;^Variable_1/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/weights/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/biases/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/basic_lstm_cell/weights/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/basic_lstm_cell/biases/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attention/attn_w/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attention/attn_v/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attention/weights/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attention/biases/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attn_output_projection/weights/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attn_output_projection/biases/Assign&quot;\\n  input: &quot;^beta1_power/Assign&quot;\\n  input: &quot;^beta2_power/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/weights/Adam/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/weights/Adam_1/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/biases/Adam/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/biases/Adam_1/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam_1/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam_1/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attention/attn_w/Adam/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attention/attn_w/Adam_1/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attention/attn_v/Adam/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attention/attn_v/Adam_1/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attention/weights/Adam/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attention/weights/Adam_1/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attention/biases/Adam/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attention/biases/Adam_1/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attn_output_projection/weights/Adam/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attn_output_projection/weights/Adam_1/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attn_output_projection/biases/Adam/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attn_output_projection/biases/Adam_1/Assign&quot;\\n  input: &quot;^Variable/Adam/Assign&quot;\\n  input: &quot;^Variable/Adam_1/Assign&quot;\\n  input: &quot;^Variable_1/Adam/Assign&quot;\\n  input: &quot;^Variable_1/Adam_1/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/weights/Adam/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/weights/Adam_1/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/biases/Adam/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/biases/Adam_1/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam_1/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam_1/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attention/attn_w/Adam/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attention/attn_w/Adam_1/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attention/attn_v/Adam/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attention/attn_v/Adam_1/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attention/weights/Adam/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attention/weights/Adam_1/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attention/biases/Adam/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attention/biases/Adam_1/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attn_output_projection/weights/Adam/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attn_output_projection/weights/Adam_1/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attn_output_projection/biases/Adam/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attn_output_projection/biases/Adam_1/Assign&quot;\\n}\\nnode {\\n  name: &quot;init_2&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/weights/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/biases/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/basic_lstm_cell/weights/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/basic_lstm_cell/biases/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attention/attn_w/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attention/attn_v/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attention/weights/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attention/biases/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attn_output_projection/weights/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attn_output_projection/biases/Assign&quot;\\n  input: &quot;^Variable/Assign&quot;\\n  input: &quot;^Variable_1/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/weights/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/biases/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/basic_lstm_cell/weights/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/basic_lstm_cell/biases/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attention/attn_w/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attention/attn_v/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attention/weights/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attention/biases/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attn_output_projection/weights/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attn_output_projection/biases/Assign&quot;\\n  input: &quot;^beta1_power/Assign&quot;\\n  input: &quot;^beta2_power/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/weights/Adam/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/weights/Adam_1/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/biases/Adam/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/biases/Adam_1/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam_1/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam_1/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attention/attn_w/Adam/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attention/attn_w/Adam_1/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attention/attn_v/Adam/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attention/attn_v/Adam_1/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attention/weights/Adam/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attention/weights/Adam_1/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attention/biases/Adam/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attention/biases/Adam_1/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attn_output_projection/weights/Adam/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attn_output_projection/weights/Adam_1/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attn_output_projection/biases/Adam/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attn_output_projection/biases/Adam_1/Assign&quot;\\n  input: &quot;^Variable/Adam/Assign&quot;\\n  input: &quot;^Variable/Adam_1/Assign&quot;\\n  input: &quot;^Variable_1/Adam/Assign&quot;\\n  input: &quot;^Variable_1/Adam_1/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/weights/Adam/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/weights/Adam_1/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/biases/Adam/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/biases/Adam_1/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam_1/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam_1/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attention/attn_w/Adam/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attention/attn_w/Adam_1/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attention/attn_v/Adam/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attention/attn_v/Adam_1/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attention/weights/Adam/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attention/weights/Adam_1/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attention/biases/Adam/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attention/biases/Adam_1/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attn_output_projection/weights/Adam/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attn_output_projection/weights/Adam_1/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attn_output_projection/biases/Adam/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attn_output_projection/biases/Adam_1/Assign&quot;\\n}\\nnode {\\n  name: &quot;save/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n        }\\n        string_val: &quot;model&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 68\\n          }\\n        }\\n        string_val: &quot;Decoder/attention_cell_wrapper/attention/attn_v&quot;\\n        string_val: &quot;Decoder/attention_cell_wrapper/attention/attn_v/Adam&quot;\\n        string_val: &quot;Decoder/attention_cell_wrapper/attention/attn_v/Adam_1&quot;\\n        string_val: &quot;Decoder/attention_cell_wrapper/attention/attn_w&quot;\\n        string_val: &quot;Decoder/attention_cell_wrapper/attention/attn_w/Adam&quot;\\n        string_val: &quot;Decoder/attention_cell_wrapper/attention/attn_w/Adam_1&quot;\\n        string_val: &quot;Decoder/attention_cell_wrapper/attention/biases&quot;\\n        string_val: &quot;Decoder/attention_cell_wrapper/attention/biases/Adam&quot;\\n        string_val: &quot;Decoder/attention_cell_wrapper/attention/biases/Adam_1&quot;\\n        string_val: &quot;Decoder/attention_cell_wrapper/attention/weights&quot;\\n        string_val: &quot;Decoder/attention_cell_wrapper/attention/weights/Adam&quot;\\n        string_val: &quot;Decoder/attention_cell_wrapper/attention/weights/Adam_1&quot;\\n        string_val: &quot;Decoder/attention_cell_wrapper/attn_output_projection/biases&quot;\\n        string_val: &quot;Decoder/attention_cell_wrapper/attn_output_projection/biases/Adam&quot;\\n        string_val: &quot;Decoder/attention_cell_wrapper/attn_output_projection/biases/Adam_1&quot;\\n        string_val: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n        string_val: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights/Adam&quot;\\n        string_val: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights/Adam_1&quot;\\n        string_val: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/biases&quot;\\n        string_val: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam&quot;\\n        string_val: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam_1&quot;\\n        string_val: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n        string_val: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam&quot;\\n        string_val: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam_1&quot;\\n        string_val: &quot;Decoder/attention_cell_wrapper/biases&quot;\\n        string_val: &quot;Decoder/attention_cell_wrapper/biases/Adam&quot;\\n        string_val: &quot;Decoder/attention_cell_wrapper/biases/Adam_1&quot;\\n        string_val: &quot;Decoder/attention_cell_wrapper/weights&quot;\\n        string_val: &quot;Decoder/attention_cell_wrapper/weights/Adam&quot;\\n        string_val: &quot;Decoder/attention_cell_wrapper/weights/Adam_1&quot;\\n        string_val: &quot;Encoder/attention_cell_wrapper/attention/attn_v&quot;\\n        string_val: &quot;Encoder/attention_cell_wrapper/attention/attn_v/Adam&quot;\\n        string_val: &quot;Encoder/attention_cell_wrapper/attention/attn_v/Adam_1&quot;\\n        string_val: &quot;Encoder/attention_cell_wrapper/attention/attn_w&quot;\\n        string_val: &quot;Encoder/attention_cell_wrapper/attention/attn_w/Adam&quot;\\n        string_val: &quot;Encoder/attention_cell_wrapper/attention/attn_w/Adam_1&quot;\\n        string_val: &quot;Encoder/attention_cell_wrapper/attention/biases&quot;\\n        string_val: &quot;Encoder/attention_cell_wrapper/attention/biases/Adam&quot;\\n        string_val: &quot;Encoder/attention_cell_wrapper/attention/biases/Adam_1&quot;\\n        string_val: &quot;Encoder/attention_cell_wrapper/attention/weights&quot;\\n        string_val: &quot;Encoder/attention_cell_wrapper/attention/weights/Adam&quot;\\n        string_val: &quot;Encoder/attention_cell_wrapper/attention/weights/Adam_1&quot;\\n        string_val: &quot;Encoder/attention_cell_wrapper/attn_output_projection/biases&quot;\\n        string_val: &quot;Encoder/attention_cell_wrapper/attn_output_projection/biases/Adam&quot;\\n        string_val: &quot;Encoder/attention_cell_wrapper/attn_output_projection/biases/Adam_1&quot;\\n        string_val: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n        string_val: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/Adam&quot;\\n        string_val: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/Adam_1&quot;\\n        string_val: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/biases&quot;\\n        string_val: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam&quot;\\n        string_val: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam_1&quot;\\n        string_val: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n        string_val: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam&quot;\\n        string_val: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam_1&quot;\\n        string_val: &quot;Encoder/attention_cell_wrapper/biases&quot;\\n        string_val: &quot;Encoder/attention_cell_wrapper/biases/Adam&quot;\\n        string_val: &quot;Encoder/attention_cell_wrapper/biases/Adam_1&quot;\\n        string_val: &quot;Encoder/attention_cell_wrapper/weights&quot;\\n        string_val: &quot;Encoder/attention_cell_wrapper/weights/Adam&quot;\\n        string_val: &quot;Encoder/attention_cell_wrapper/weights/Adam_1&quot;\\n        string_val: &quot;Variable&quot;\\n        string_val: &quot;Variable/Adam&quot;\\n        string_val: &quot;Variable/Adam_1&quot;\\n        string_val: &quot;Variable_1&quot;\\n        string_val: &quot;Variable_1/Adam&quot;\\n        string_val: &quot;Variable_1/Adam_1&quot;\\n        string_val: &quot;beta1_power&quot;\\n        string_val: &quot;beta2_power&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 68\\n          }\\n        }\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2&quot;\\n  op: &quot;SaveV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/SaveV2/tensor_names&quot;\\n  input: &quot;save/SaveV2/shape_and_slices&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_v&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_v/Adam&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_v/Adam_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_w&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_w/Adam&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_w/Adam_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/biases&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/biases/Adam&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/biases/Adam_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/weights&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/weights/Adam&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/weights/Adam_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/biases&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/biases/Adam&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/biases/Adam_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights/Adam&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights/Adam_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/biases&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/biases&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/biases/Adam&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/biases/Adam_1&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/weights&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/weights/Adam&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/weights/Adam_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_v&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_v/Adam&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_v/Adam_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_w&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_w/Adam&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_w/Adam_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/biases&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/biases/Adam&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/biases/Adam_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/weights&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/weights/Adam&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/weights/Adam_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/biases&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/biases/Adam&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/biases/Adam_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/Adam&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/Adam_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/biases&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/biases&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/biases/Adam&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/biases/Adam_1&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/weights&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/weights/Adam&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/weights/Adam_1&quot;\\n  input: &quot;Variable&quot;\\n  input: &quot;Variable/Adam&quot;\\n  input: &quot;Variable/Adam_1&quot;\\n  input: &quot;Variable_1&quot;\\n  input: &quot;Variable_1/Adam&quot;\\n  input: &quot;Variable_1/Adam_1&quot;\\n  input: &quot;beta1_power&quot;\\n  input: &quot;beta2_power&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;^save/SaveV2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@save/Const&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Decoder/attention_cell_wrapper/attention/attn_v&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2/tensor_names&quot;\\n  input: &quot;save/RestoreV2/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_v&quot;\\n  input: &quot;save/RestoreV2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/attn_v&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_1/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Decoder/attention_cell_wrapper/attention/attn_v/Adam&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_1/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_1&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_1/tensor_names&quot;\\n  input: &quot;save/RestoreV2_1/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_1&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_v/Adam&quot;\\n  input: &quot;save/RestoreV2_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/attn_v&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Decoder/attention_cell_wrapper/attention/attn_v/Adam_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_2&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_2/tensor_names&quot;\\n  input: &quot;save/RestoreV2_2/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_2&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_v/Adam_1&quot;\\n  input: &quot;save/RestoreV2_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/attn_v&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_3/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Decoder/attention_cell_wrapper/attention/attn_w&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_3/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_3&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_3/tensor_names&quot;\\n  input: &quot;save/RestoreV2_3/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_3&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_w&quot;\\n  input: &quot;save/RestoreV2_3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/attn_w&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_4/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Decoder/attention_cell_wrapper/attention/attn_w/Adam&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_4/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_4&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_4/tensor_names&quot;\\n  input: &quot;save/RestoreV2_4/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_4&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_w/Adam&quot;\\n  input: &quot;save/RestoreV2_4&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/attn_w&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_5/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Decoder/attention_cell_wrapper/attention/attn_w/Adam_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_5/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_5&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_5/tensor_names&quot;\\n  input: &quot;save/RestoreV2_5/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_5&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/attn_w/Adam_1&quot;\\n  input: &quot;save/RestoreV2_5&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/attn_w&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_6/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Decoder/attention_cell_wrapper/attention/biases&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_6/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_6&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_6/tensor_names&quot;\\n  input: &quot;save/RestoreV2_6/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_6&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/biases&quot;\\n  input: &quot;save/RestoreV2_6&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_7/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Decoder/attention_cell_wrapper/attention/biases/Adam&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_7/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_7&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_7/tensor_names&quot;\\n  input: &quot;save/RestoreV2_7/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_7&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/biases/Adam&quot;\\n  input: &quot;save/RestoreV2_7&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_8/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Decoder/attention_cell_wrapper/attention/biases/Adam_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_8/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_8&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_8/tensor_names&quot;\\n  input: &quot;save/RestoreV2_8/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_8&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/biases/Adam_1&quot;\\n  input: &quot;save/RestoreV2_8&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_9/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Decoder/attention_cell_wrapper/attention/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_9/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_9&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_9/tensor_names&quot;\\n  input: &quot;save/RestoreV2_9/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_9&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/weights&quot;\\n  input: &quot;save/RestoreV2_9&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_10/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Decoder/attention_cell_wrapper/attention/weights/Adam&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_10/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_10&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_10/tensor_names&quot;\\n  input: &quot;save/RestoreV2_10/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_10&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/weights/Adam&quot;\\n  input: &quot;save/RestoreV2_10&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_11/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Decoder/attention_cell_wrapper/attention/weights/Adam_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_11/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_11&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_11/tensor_names&quot;\\n  input: &quot;save/RestoreV2_11/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_11&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attention/weights/Adam_1&quot;\\n  input: &quot;save/RestoreV2_11&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attention/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_12/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Decoder/attention_cell_wrapper/attn_output_projection/biases&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_12/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_12&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_12/tensor_names&quot;\\n  input: &quot;save/RestoreV2_12/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_12&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/biases&quot;\\n  input: &quot;save/RestoreV2_12&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attn_output_projection/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_13/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Decoder/attention_cell_wrapper/attn_output_projection/biases/Adam&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_13/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_13&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_13/tensor_names&quot;\\n  input: &quot;save/RestoreV2_13/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_13&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/biases/Adam&quot;\\n  input: &quot;save/RestoreV2_13&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attn_output_projection/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_14/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Decoder/attention_cell_wrapper/attn_output_projection/biases/Adam_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_14/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_14&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_14/tensor_names&quot;\\n  input: &quot;save/RestoreV2_14/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_14&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/biases/Adam_1&quot;\\n  input: &quot;save/RestoreV2_14&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attn_output_projection/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_15/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_15/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_15&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_15/tensor_names&quot;\\n  input: &quot;save/RestoreV2_15/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_15&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n  input: &quot;save/RestoreV2_15&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_16/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights/Adam&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_16/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_16&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_16/tensor_names&quot;\\n  input: &quot;save/RestoreV2_16/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_16&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights/Adam&quot;\\n  input: &quot;save/RestoreV2_16&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_17/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights/Adam_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_17/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_17&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_17/tensor_names&quot;\\n  input: &quot;save/RestoreV2_17/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_17&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/attn_output_projection/weights/Adam_1&quot;\\n  input: &quot;save/RestoreV2_17&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_18/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/biases&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_18/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_18&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_18/tensor_names&quot;\\n  input: &quot;save/RestoreV2_18/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_18&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/biases&quot;\\n  input: &quot;save/RestoreV2_18&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/basic_lstm_cell/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_19/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_19/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_19&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_19/tensor_names&quot;\\n  input: &quot;save/RestoreV2_19/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_19&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam&quot;\\n  input: &quot;save/RestoreV2_19&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/basic_lstm_cell/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_20/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_20/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_20&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_20/tensor_names&quot;\\n  input: &quot;save/RestoreV2_20/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_20&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam_1&quot;\\n  input: &quot;save/RestoreV2_20&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/basic_lstm_cell/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_21/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_21/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_21&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_21/tensor_names&quot;\\n  input: &quot;save/RestoreV2_21/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_21&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n  input: &quot;save/RestoreV2_21&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_22/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_22/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_22&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_22/tensor_names&quot;\\n  input: &quot;save/RestoreV2_22/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_22&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam&quot;\\n  input: &quot;save/RestoreV2_22&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_23/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_23/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_23&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_23/tensor_names&quot;\\n  input: &quot;save/RestoreV2_23/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_23&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam_1&quot;\\n  input: &quot;save/RestoreV2_23&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_24/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Decoder/attention_cell_wrapper/biases&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_24/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_24&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_24/tensor_names&quot;\\n  input: &quot;save/RestoreV2_24/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_24&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/biases&quot;\\n  input: &quot;save/RestoreV2_24&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_25/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Decoder/attention_cell_wrapper/biases/Adam&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_25/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_25&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_25/tensor_names&quot;\\n  input: &quot;save/RestoreV2_25/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_25&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/biases/Adam&quot;\\n  input: &quot;save/RestoreV2_25&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_26/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Decoder/attention_cell_wrapper/biases/Adam_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_26/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_26&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_26/tensor_names&quot;\\n  input: &quot;save/RestoreV2_26/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_26&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/biases/Adam_1&quot;\\n  input: &quot;save/RestoreV2_26&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_27/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Decoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_27/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_27&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_27/tensor_names&quot;\\n  input: &quot;save/RestoreV2_27/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_27&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/weights&quot;\\n  input: &quot;save/RestoreV2_27&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_28/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Decoder/attention_cell_wrapper/weights/Adam&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_28/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_28&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_28/tensor_names&quot;\\n  input: &quot;save/RestoreV2_28/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_28&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/weights/Adam&quot;\\n  input: &quot;save/RestoreV2_28&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_29/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Decoder/attention_cell_wrapper/weights/Adam_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_29/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_29&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_29/tensor_names&quot;\\n  input: &quot;save/RestoreV2_29/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_29&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Decoder/attention_cell_wrapper/weights/Adam_1&quot;\\n  input: &quot;save/RestoreV2_29&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Decoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_30/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Encoder/attention_cell_wrapper/attention/attn_v&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_30/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_30&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_30/tensor_names&quot;\\n  input: &quot;save/RestoreV2_30/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_30&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_v&quot;\\n  input: &quot;save/RestoreV2_30&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/attn_v&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_31/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Encoder/attention_cell_wrapper/attention/attn_v/Adam&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_31/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_31&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_31/tensor_names&quot;\\n  input: &quot;save/RestoreV2_31/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_31&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_v/Adam&quot;\\n  input: &quot;save/RestoreV2_31&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/attn_v&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_32/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Encoder/attention_cell_wrapper/attention/attn_v/Adam_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_32/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_32&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_32/tensor_names&quot;\\n  input: &quot;save/RestoreV2_32/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_32&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_v/Adam_1&quot;\\n  input: &quot;save/RestoreV2_32&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/attn_v&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_33/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Encoder/attention_cell_wrapper/attention/attn_w&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_33/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_33&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_33/tensor_names&quot;\\n  input: &quot;save/RestoreV2_33/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_33&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_w&quot;\\n  input: &quot;save/RestoreV2_33&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/attn_w&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_34/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Encoder/attention_cell_wrapper/attention/attn_w/Adam&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_34/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_34&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_34/tensor_names&quot;\\n  input: &quot;save/RestoreV2_34/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_34&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_w/Adam&quot;\\n  input: &quot;save/RestoreV2_34&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/attn_w&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_35/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Encoder/attention_cell_wrapper/attention/attn_w/Adam_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_35/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_35&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_35/tensor_names&quot;\\n  input: &quot;save/RestoreV2_35/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_35&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/attn_w/Adam_1&quot;\\n  input: &quot;save/RestoreV2_35&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/attn_w&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_36/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Encoder/attention_cell_wrapper/attention/biases&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_36/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_36&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_36/tensor_names&quot;\\n  input: &quot;save/RestoreV2_36/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_36&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/biases&quot;\\n  input: &quot;save/RestoreV2_36&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_37/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Encoder/attention_cell_wrapper/attention/biases/Adam&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_37/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_37&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_37/tensor_names&quot;\\n  input: &quot;save/RestoreV2_37/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_37&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/biases/Adam&quot;\\n  input: &quot;save/RestoreV2_37&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_38/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Encoder/attention_cell_wrapper/attention/biases/Adam_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_38/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_38&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_38/tensor_names&quot;\\n  input: &quot;save/RestoreV2_38/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_38&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/biases/Adam_1&quot;\\n  input: &quot;save/RestoreV2_38&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_39/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Encoder/attention_cell_wrapper/attention/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_39/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_39&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_39/tensor_names&quot;\\n  input: &quot;save/RestoreV2_39/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_39&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/weights&quot;\\n  input: &quot;save/RestoreV2_39&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_40/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Encoder/attention_cell_wrapper/attention/weights/Adam&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_40/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_40&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_40/tensor_names&quot;\\n  input: &quot;save/RestoreV2_40/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_40&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/weights/Adam&quot;\\n  input: &quot;save/RestoreV2_40&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_41/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Encoder/attention_cell_wrapper/attention/weights/Adam_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_41/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_41&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_41/tensor_names&quot;\\n  input: &quot;save/RestoreV2_41/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_41&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attention/weights/Adam_1&quot;\\n  input: &quot;save/RestoreV2_41&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attention/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_42/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Encoder/attention_cell_wrapper/attn_output_projection/biases&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_42/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_42&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_42/tensor_names&quot;\\n  input: &quot;save/RestoreV2_42/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_42&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/biases&quot;\\n  input: &quot;save/RestoreV2_42&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attn_output_projection/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_43/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Encoder/attention_cell_wrapper/attn_output_projection/biases/Adam&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_43/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_43&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_43/tensor_names&quot;\\n  input: &quot;save/RestoreV2_43/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_43&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/biases/Adam&quot;\\n  input: &quot;save/RestoreV2_43&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attn_output_projection/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_44/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Encoder/attention_cell_wrapper/attn_output_projection/biases/Adam_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_44/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_44&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_44/tensor_names&quot;\\n  input: &quot;save/RestoreV2_44/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_44&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/biases/Adam_1&quot;\\n  input: &quot;save/RestoreV2_44&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attn_output_projection/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_45/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_45/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_45&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_45/tensor_names&quot;\\n  input: &quot;save/RestoreV2_45/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_45&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n  input: &quot;save/RestoreV2_45&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_46/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/Adam&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_46/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_46&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_46/tensor_names&quot;\\n  input: &quot;save/RestoreV2_46/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_46&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/Adam&quot;\\n  input: &quot;save/RestoreV2_46&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_47/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/Adam_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_47/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_47&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_47/tensor_names&quot;\\n  input: &quot;save/RestoreV2_47/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_47&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/attn_output_projection/weights/Adam_1&quot;\\n  input: &quot;save/RestoreV2_47&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/attn_output_projection/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_48/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/biases&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_48/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_48&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_48/tensor_names&quot;\\n  input: &quot;save/RestoreV2_48/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_48&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/biases&quot;\\n  input: &quot;save/RestoreV2_48&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/basic_lstm_cell/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_49/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_49/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_49&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_49/tensor_names&quot;\\n  input: &quot;save/RestoreV2_49/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_49&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam&quot;\\n  input: &quot;save/RestoreV2_49&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/basic_lstm_cell/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_50/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_50/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_50&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_50/tensor_names&quot;\\n  input: &quot;save/RestoreV2_50/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_50&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam_1&quot;\\n  input: &quot;save/RestoreV2_50&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/basic_lstm_cell/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_51/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_51/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_51&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_51/tensor_names&quot;\\n  input: &quot;save/RestoreV2_51/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_51&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n  input: &quot;save/RestoreV2_51&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_52/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_52/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_52&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_52/tensor_names&quot;\\n  input: &quot;save/RestoreV2_52/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_52&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam&quot;\\n  input: &quot;save/RestoreV2_52&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_53/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_53/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_53&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_53/tensor_names&quot;\\n  input: &quot;save/RestoreV2_53/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_53&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam_1&quot;\\n  input: &quot;save/RestoreV2_53&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/basic_lstm_cell/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_54/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Encoder/attention_cell_wrapper/biases&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_54/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_54&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_54/tensor_names&quot;\\n  input: &quot;save/RestoreV2_54/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_54&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/biases&quot;\\n  input: &quot;save/RestoreV2_54&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_55/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Encoder/attention_cell_wrapper/biases/Adam&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_55/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_55&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_55/tensor_names&quot;\\n  input: &quot;save/RestoreV2_55/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_55&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/biases/Adam&quot;\\n  input: &quot;save/RestoreV2_55&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_56/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Encoder/attention_cell_wrapper/biases/Adam_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_56/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_56&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_56/tensor_names&quot;\\n  input: &quot;save/RestoreV2_56/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_56&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/biases/Adam_1&quot;\\n  input: &quot;save/RestoreV2_56&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/biases&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_57/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Encoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_57/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_57&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_57/tensor_names&quot;\\n  input: &quot;save/RestoreV2_57/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_57&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/weights&quot;\\n  input: &quot;save/RestoreV2_57&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_58/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Encoder/attention_cell_wrapper/weights/Adam&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_58/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_58&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_58/tensor_names&quot;\\n  input: &quot;save/RestoreV2_58/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_58&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/weights/Adam&quot;\\n  input: &quot;save/RestoreV2_58&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_59/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Encoder/attention_cell_wrapper/weights/Adam_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_59/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_59&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_59/tensor_names&quot;\\n  input: &quot;save/RestoreV2_59/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_59&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Encoder/attention_cell_wrapper/weights/Adam_1&quot;\\n  input: &quot;save/RestoreV2_59&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_60/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Variable&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_60/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_60&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_60/tensor_names&quot;\\n  input: &quot;save/RestoreV2_60/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_60&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Variable&quot;\\n  input: &quot;save/RestoreV2_60&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Variable&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_61/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Variable/Adam&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_61/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_61&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_61/tensor_names&quot;\\n  input: &quot;save/RestoreV2_61/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_61&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Variable/Adam&quot;\\n  input: &quot;save/RestoreV2_61&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Variable&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_62/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Variable/Adam_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_62/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_62&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_62/tensor_names&quot;\\n  input: &quot;save/RestoreV2_62/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_62&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Variable/Adam_1&quot;\\n  input: &quot;save/RestoreV2_62&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Variable&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_63/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Variable_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_63/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_63&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_63/tensor_names&quot;\\n  input: &quot;save/RestoreV2_63/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_63&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Variable_1&quot;\\n  input: &quot;save/RestoreV2_63&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Variable_1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_64/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Variable_1/Adam&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_64/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_64&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_64/tensor_names&quot;\\n  input: &quot;save/RestoreV2_64/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_64&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Variable_1/Adam&quot;\\n  input: &quot;save/RestoreV2_64&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Variable_1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_65/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;Variable_1/Adam_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_65/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_65&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_65/tensor_names&quot;\\n  input: &quot;save/RestoreV2_65/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_65&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Variable_1/Adam_1&quot;\\n  input: &quot;save/RestoreV2_65&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Variable_1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_66/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;beta1_power&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_66/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_66&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_66/tensor_names&quot;\\n  input: &quot;save/RestoreV2_66/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_66&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;beta1_power&quot;\\n  input: &quot;save/RestoreV2_66&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_67/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;beta2_power&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_67/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_67&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_67/tensor_names&quot;\\n  input: &quot;save/RestoreV2_67/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_67&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;beta2_power&quot;\\n  input: &quot;save/RestoreV2_67&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Encoder/attention_cell_wrapper/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/restore_all&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^save/Assign&quot;\\n  input: &quot;^save/Assign_1&quot;\\n  input: &quot;^save/Assign_2&quot;\\n  input: &quot;^save/Assign_3&quot;\\n  input: &quot;^save/Assign_4&quot;\\n  input: &quot;^save/Assign_5&quot;\\n  input: &quot;^save/Assign_6&quot;\\n  input: &quot;^save/Assign_7&quot;\\n  input: &quot;^save/Assign_8&quot;\\n  input: &quot;^save/Assign_9&quot;\\n  input: &quot;^save/Assign_10&quot;\\n  input: &quot;^save/Assign_11&quot;\\n  input: &quot;^save/Assign_12&quot;\\n  input: &quot;^save/Assign_13&quot;\\n  input: &quot;^save/Assign_14&quot;\\n  input: &quot;^save/Assign_15&quot;\\n  input: &quot;^save/Assign_16&quot;\\n  input: &quot;^save/Assign_17&quot;\\n  input: &quot;^save/Assign_18&quot;\\n  input: &quot;^save/Assign_19&quot;\\n  input: &quot;^save/Assign_20&quot;\\n  input: &quot;^save/Assign_21&quot;\\n  input: &quot;^save/Assign_22&quot;\\n  input: &quot;^save/Assign_23&quot;\\n  input: &quot;^save/Assign_24&quot;\\n  input: &quot;^save/Assign_25&quot;\\n  input: &quot;^save/Assign_26&quot;\\n  input: &quot;^save/Assign_27&quot;\\n  input: &quot;^save/Assign_28&quot;\\n  input: &quot;^save/Assign_29&quot;\\n  input: &quot;^save/Assign_30&quot;\\n  input: &quot;^save/Assign_31&quot;\\n  input: &quot;^save/Assign_32&quot;\\n  input: &quot;^save/Assign_33&quot;\\n  input: &quot;^save/Assign_34&quot;\\n  input: &quot;^save/Assign_35&quot;\\n  input: &quot;^save/Assign_36&quot;\\n  input: &quot;^save/Assign_37&quot;\\n  input: &quot;^save/Assign_38&quot;\\n  input: &quot;^save/Assign_39&quot;\\n  input: &quot;^save/Assign_40&quot;\\n  input: &quot;^save/Assign_41&quot;\\n  input: &quot;^save/Assign_42&quot;\\n  input: &quot;^save/Assign_43&quot;\\n  input: &quot;^save/Assign_44&quot;\\n  input: &quot;^save/Assign_45&quot;\\n  input: &quot;^save/Assign_46&quot;\\n  input: &quot;^save/Assign_47&quot;\\n  input: &quot;^save/Assign_48&quot;\\n  input: &quot;^save/Assign_49&quot;\\n  input: &quot;^save/Assign_50&quot;\\n  input: &quot;^save/Assign_51&quot;\\n  input: &quot;^save/Assign_52&quot;\\n  input: &quot;^save/Assign_53&quot;\\n  input: &quot;^save/Assign_54&quot;\\n  input: &quot;^save/Assign_55&quot;\\n  input: &quot;^save/Assign_56&quot;\\n  input: &quot;^save/Assign_57&quot;\\n  input: &quot;^save/Assign_58&quot;\\n  input: &quot;^save/Assign_59&quot;\\n  input: &quot;^save/Assign_60&quot;\\n  input: &quot;^save/Assign_61&quot;\\n  input: &quot;^save/Assign_62&quot;\\n  input: &quot;^save/Assign_63&quot;\\n  input: &quot;^save/Assign_64&quot;\\n  input: &quot;^save/Assign_65&quot;\\n  input: &quot;^save/Assign_66&quot;\\n  input: &quot;^save/Assign_67&quot;\\n}\\nnode {\\n  name: &quot;init_3&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/weights/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/biases/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/basic_lstm_cell/weights/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/basic_lstm_cell/biases/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attention/attn_w/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attention/attn_v/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attention/weights/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attention/biases/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attn_output_projection/weights/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attn_output_projection/biases/Assign&quot;\\n  input: &quot;^Variable/Assign&quot;\\n  input: &quot;^Variable_1/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/weights/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/biases/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/basic_lstm_cell/weights/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/basic_lstm_cell/biases/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attention/attn_w/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attention/attn_v/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attention/weights/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attention/biases/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attn_output_projection/weights/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attn_output_projection/biases/Assign&quot;\\n  input: &quot;^beta1_power/Assign&quot;\\n  input: &quot;^beta2_power/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/weights/Adam/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/weights/Adam_1/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/biases/Adam/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/biases/Adam_1/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam_1/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam_1/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attention/attn_w/Adam/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attention/attn_w/Adam_1/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attention/attn_v/Adam/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attention/attn_v/Adam_1/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attention/weights/Adam/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attention/weights/Adam_1/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attention/biases/Adam/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attention/biases/Adam_1/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attn_output_projection/weights/Adam/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attn_output_projection/weights/Adam_1/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attn_output_projection/biases/Adam/Assign&quot;\\n  input: &quot;^Encoder/attention_cell_wrapper/attn_output_projection/biases/Adam_1/Assign&quot;\\n  input: &quot;^Variable/Adam/Assign&quot;\\n  input: &quot;^Variable/Adam_1/Assign&quot;\\n  input: &quot;^Variable_1/Adam/Assign&quot;\\n  input: &quot;^Variable_1/Adam_1/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/weights/Adam/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/weights/Adam_1/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/biases/Adam/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/biases/Adam_1/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/basic_lstm_cell/weights/Adam_1/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/basic_lstm_cell/biases/Adam_1/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attention/attn_w/Adam/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attention/attn_w/Adam_1/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attention/attn_v/Adam/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attention/attn_v/Adam_1/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attention/weights/Adam/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attention/weights/Adam_1/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attention/biases/Adam/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attention/biases/Adam_1/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attn_output_projection/weights/Adam/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attn_output_projection/weights/Adam_1/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attn_output_projection/biases/Adam/Assign&quot;\\n  input: &quot;^Decoder/attention_cell_wrapper/attn_output_projection/biases/Adam_1/Assign&quot;\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.5488135039273248&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tb.show_graph(tf.get_default_graph().as_graph_def())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
